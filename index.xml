<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>おおたの物置</title>
    <link>http://ota42y.com/</link>
    <description>Recent content on おおたの物置</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <lastBuildDate>Tue, 10 Apr 2018 04:21:50 +0000</lastBuildDate>
    
	<atom:link href="http://ota42y.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>技術書典4でMicroservices architectureよろず本を出します</title>
      <link>http://ota42y.com/blog/2018/04/10/microservices_yorozu_book/</link>
      <pubDate>Tue, 10 Apr 2018 04:21:50 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2018/04/10/microservices_yorozu_book/</guid>
      <description>技術書典4の&amp;rdquo;か13&amp;rdquo;、「すべてがM(icro)になる」で、 Microservices architecture よろず本 を出します。
https://techbookfest.org/event/tbf04/circle/16650003
マイクロサービスアーキテクチャに関する様々な事を書いた本になります。
著者は私(@ota42y)と、@hyata、@qsona、@shinofumijp、@nobuhikosawai、@gemaの6人です。
人体ってマイクロサービスだよね、的な発言が著者人の1人からあったのでこんな表紙になっています。

目次は以下で、全77ページになります。
第 1 章 マイクロサービスとは何か？ 1.1 マイクロサービスどこから来たの？？ 1.2 マイクロサービスの特徴とは？ 1.3 マイクロサービスのメリットとは？ 1.4 マイクロサービスにデメリットはあるのか？ 1.5 おわりに 第 2 章 MS 将棋道場サービスの挑戦とマイクロサービス 2.1 プロローグ 2.2 将棋⼤会機能 - はじめてのマイクロサービス 2.3 指導サービス - 新しいサービス連携の形 2.4 MS 将棋道場本体サービスの分割 2.5 エピローグ 第 3 章 組織構造に基づいたサービス分割と⾮同期アーキテクチャ 3.1 はじめに 3.2 マイクロサービスの境界 3.3 実際の例 3.4 まとめ 第 4 章 マイクロサービスの育成⽅法 4.1 はじめに 4.2 マイクロサービス全体像 4.3 サービスの所有権を持つチームは 1 つにすべき 4.4 チームとサービスは完全に⼀致させなくても良い 4.</description>
    </item>
    
    <item>
      <title>gemのバージョンアップで依存関係が壊れるのを調べやすくした</title>
      <link>http://ota42y.com/blog/2018/04/04/virtual_gem/</link>
      <pubDate>Wed, 04 Apr 2018 00:10:37 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2018/04/04/virtual_gem/</guid>
      <description>要約  gemのdependencyに &amp;lt; 5.2.0と書いてあると、 5.2.0.rc2までインストールできる  5.2.0がリリースするまでインストールできないと気がつきにくい  任意のgem依存関係を上書きして、bundle install/updateを可能にするgemを作った  https://github.com/ota42y/virtual_gem 5.2.0.rc2を元に5.2.0のgemを作れる 任意のgemの依存関係を書き換えられる 事前に依存関係が壊れるのを調べやすくなる   gemの依存関係によるインストール失敗 現在、Rails 5.2.0.rc2がリリースされています。
ですが、仮に現状で上手く動いていたとしても5.2.0のリリース時に一部のgemの依存関係が5.2.0.rc2までしか受け入れておらず、インストールできなくなる可能性があります。
gemには依存関係を設定できますが、&#39;&amp;lt; 5.2.0&#39; と書いた場合は5.2.0未満のgemを指定したこととなるため、5.2.0.rc2は対象となります。
ですが、5.2.0は範囲外となるため、5.2.0をインストールすることはできません。
そのため、rc版で問題なく動いていたとしても正式リリースされると依存関係を解決できずにアップデートできず、依存するgemの更新を待たないといけません。
依存するgemが依存しているgemが同じような問題を含んでいた場合は、それらが全て解決するのを待つ必要があります。
事前に依存関係を修正しておくことでこの問題を回避できますが、事前に全てのgemの依存関係を調べることはかなり難しいです。 かつ正式リリースされるまで問題は起こらないため先に対策するのは困難です。
また、gemによっては正式リリースまで依存関係を修正しないポリシーもあり得るため、複雑な依存関係の場合には調べるのがいっそう困難になります。
この問題は2つの原因から成り立っています。
 gemが正式リリースされるまでbundle installで新しいバージョンを試せない bundle install時の依存関係の解消にはgemの設定を直す必要がある  問題のあるgemを全てforkして、バージョンを書き換えた上でそれを使うことでこれらの問題は解決できます。
ですが、継続的にmasterを追従していくことや、変更をするgemが大量にあった場合はそれを続けるのは困難です。
virtual gemによる解決 この問題に対して、bundle install/update時にgemのバージョンと依存関係を書き換え、問題のある設定になっていないかを調べられるgemを作りました。 https://github.com/ota42y/virtual_gem
仮想的なgemを作る 以下のようにGemfileで設定を行うと、特定バージョンのgemとして振る舞う仮想的なgemを作成して、依存関係の解決を行えます。
require &#39;virtual_gem&#39; ::VirtualGem.register_virtual_gem(name: &#39;ota42y_test_gem&#39;,new_version: &#39;0.3.0&#39;, original_version: &#39;0.2.0&#39;) source &amp;quot;https://rubygems.org&amp;quot; gem &#39;ota42y_test_gem&#39;, &#39;0.3.0&#39;  この例では、&amp;rsquo;ota42y_test_gem&amp;rsquo;のバージョン0.2.0を元に、バージョン0.3.0として振る舞う仮想的なgemを作成しています。
0.3.0はこの世に存在していないので、通常ではbundlerは見つけられずに失敗します。
ですが、virtual_gemによって仮想的なgemがインストールされている状態になるため、bundle installは成功します。
これにより、rails 5.2.0.rc2をrails 5.2.0として扱うことができるため、依存関係の解決を試せます。
依存関係を書き換える virtual_gemではまだ存在しないgemで依存関係の解決を試して見ることができますが、依存関係が衝突する可能性は十分にあります。 例えばrails 5.</description>
    </item>
    
    <item>
      <title>Globalizeを使ったRailsを5系対応する際にはまった</title>
      <link>http://ota42y.com/blog/2018/03/28/globalize_5/</link>
      <pubDate>Wed, 28 Mar 2018 09:10:37 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2018/03/28/globalize_5/</guid>
      <description>要約  columns_hash等でI18n対応したカラムと同名のカラムを参照してると落ちる だいたいGlobalizeのtranslateしたときの消し忘れなのでつらい チェックして消す&amp;amp;今後増えないようにするgemを作った  問題  GlobalizeをRails5対応版(5.1.0.beta2)にあげる 一部の処理が落ちる(´・_・`)  原因 News.columns_hash.keysは、そのモデルのカラム名を配列で返します。
GlobalizeのRails 5未対応版の5.0.1が入っているとDBのカラムをそのまま配列にして返します。
しかし、Rails 5対応版の5.1.0.beta2では、I18n対応したカラム名と同名のカラムを除いたものを返します。
そのため返ってくる値がバージョンによって代わり、それに依存している処理がおかしくなります。
ただし、この影響を受けるカラムは多くの場合参照できないため、通常は仕様ミスや顕在化してないバグの可能性が高いです。
解説 Globalize gemでは、あるモデルのカラムに対してtranslate指定をすることで、translate用のテーブルの同名カラムを参照するようになります。
例えば以下のようなNewsモデルがある場合を考えます。
# == Schema Information # # Table name: news # # id :integer not null, primary key # title :string(255) # description :string(255) class News translate :title, :description end  このモデルのtitleとdescriptionはtranslateされているので、newsテーブルの同名のカラムは使われません。 Globalizeによって、I18n対応した別テーブル(news_translates)の該当するlocaleのデータを取ってくるように変更されます。 そのため、translateによって処理が上書きされたカラムと同じ名前のカラムが元のテーブルにあっても、アクセスすることは出来ません。
ですが、Globalize 5.0系ではcolumns_hash等で全てのカラムの一覧を取得した場合、このアクセス不能なカラムが一覧に入っています。 そのため、上書き前のカラムがあるという前提のコードを書けてしまいます。
News.columns_hash.keys =&amp;gt; [&amp;quot;id&amp;quot;, &amp;quot;title&amp;quot;, &amp;quot;description&amp;quot;, &amp;quot;created_at&amp;quot;, &amp;quot;updated_at&amp;quot;]  Globalizeの5.1系では、内部的に以下の方法でtranslateに設定されたカラム名が除外されるため、columns_hash等を使っても上書き前のカラムにアクセスできなくなります。
News.columns_hash.keys =&amp;gt; [&amp;quot;id&amp;quot;, &amp;quot;created_at&amp;quot;, &amp;quot;updated_at&amp;quot;]   Rails4系の場合  もとのcolumns_hashからtranslateしたカラムを消している。 Globalize::ActiveRecord::ClassMethods#columns_hash  Rails5系の場合  ignored_columnsでActiveRecord的にカラムを無効化 Rails5から追加されたメソッド Globalize::ActiveRecord::ActMacro#allow_translation_of_attributes   そのため、 columns_hash を利用して上書き前のカラムがあることを前提としたコードを書いている場合、処理が失敗します。</description>
    </item>
    
    <item>
      <title>一部のSAMLライブラリの脆弱性の詳細</title>
      <link>http://ota42y.com/blog/2018/03/01/saml_vulnerability/</link>
      <pubDate>Thu, 01 Mar 2018 15:17:35 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2018/03/01/saml_vulnerability/</guid>
      <description>複数のSAMLライブラリに認証を回避できる脆弱性があると明らかになりました。
JVNVU#98536678: 複数の SAML ライブラリに認証回避の脆弱性
どういう脆弱性なのかが書いてあり、かつ簡単に理解可能だったので詳細を書いていきます。
Duo Finds SAML Vulnerabilities Affecting Multiple Implementations | Duo Security
要約 簡単に説明すると、認証した結果のデータを改ざんすることで別のユーザになりすませてしまう問題です。
本来なら署名が存在するため改ざんしても検出されてしまいます。
ただし、XMLの署名には意味的に同じであれば表現方法が違うデータであっても同じとして見なす仕様があります。
そのため、問題のあるライブラリでただしく取り出せないような表現方法にデータを改ざんすることで、別のデータをライブラリに読み込ませることができてしまうと言う問題です。
SAMLの署名 SAMLの認証の流れで、認証した結果誰なのかを認証する側のサービスが認証をお願いした側のサービスに送ってきます。
この中には以下のように認証したユーザの情報と、改ざん防止のために署名が含まれています。
この例ではNameIDの値に入っているota42y@ota42y.caが認証に成功したというデータになります。
&amp;lt;!-- これは階層構造とかを大幅に省略しています --&amp;gt; &amp;lt;SAMLResponse&amp;gt; &amp;lt;NameID&amp;gt;ota42y@ota42y.ca&amp;lt;/NameID&amp;gt; &amp;lt;SignatureValue&amp;gt;BASE64_TEXT&amp;lt;/SignatureValue&amp;gt; &amp;lt;/SAMLResponse&amp;gt;  ここでNameIDの値を書き換えたとしても、署名した結果がSignatureValueにあるため、それを検証すれば書き換えられたとわかります。
そのため、別のユーザへのなりすましを回避できています。
XMLの正規化 XMLは以下のように、意味を変えずに属性の順番を変えたり空白やコメントを入れられます。
&amp;lt;A C=&amp;quot;2&amp;quot; B=&amp;quot;1&amp;quot;&amp;gt;text&amp;lt;/A&amp;gt; &amp;lt;A B=&amp;quot;1&amp;quot; C=&amp;quot;2&amp;quot; &amp;gt;te&amp;lt;!-- comment--&amp;gt;xt&amp;lt;/ A &amp;gt;  この2つのXMLはハッシュ値が違いますが実際には同じ内容であり、これで署名が一致しなくなるのは困る場合があります。
そのため、XMLの署名では正規化(canonicalization)を定義しています。
正規化によって形式をそろえた状態で署名を検証することで、予期せぬ表現の違いで署名が一致しなくなるという事を防いでいます。
一般的には以下のURLの方法で正規化が行われるそうです。
この正規化ではコメントを削除しない仕様であり、例に挙げたXMLは同一とはみなされません。
しかし、一般的にはだいたいの場合コメントも削除するようです。
（今回の問題はこの違いも影響している）
Exclusive XML Canonicalization Version 1.0
問題となるライブラリの挙動 例えば以下のNameIDのユーザAとユーザBがいたとします
ユーザA &amp;lt;NameID&amp;gt;ota42y@ota42y.ca&amp;lt;/NameID&amp;gt; ユーザB(.calはgoogleが持っているトップレベルドメインで実在します) &amp;lt;NameID&amp;gt;ota42y@ota42y.cal&amp;lt;/NameID&amp;gt;  このとき、ユーザBが以下のようにNameIDを書き換えます
&amp;lt;NameID&amp;gt;ota42y@ota42y.co&amp;lt;!-- comment --&amp;gt;l&amp;lt;/NameID&amp;gt;</description>
    </item>
    
    <item>
      <title>ゼロから始めるgem生活</title>
      <link>http://ota42y.com/blog/2017/08/20/rubygem-tutorial/</link>
      <pubDate>Sun, 20 Aug 2017 22:44:10 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2017/08/20/rubygem-tutorial/</guid>
      <description>背景 周りでgemを作りたいという人は多いのですが、なんか大変そうといった印象で実際に手を動かすのに躊躇する人が多かったので、心理的ハードルを下げるために30分程度で0からgemを作るハンズオンをやりました。
これはそのときにやったことのメモで、現状これに沿って行っていけば簡単なgemを公開できるようになります。
ゴール  簡単なgemを作成してrubygemsで公開するまで  事前準備  githubにアカウント作る git pushで自分のリポジトリにpushできる状態 ruby 2.0以上 gem install bundler 済み gem update &amp;ndash;system 済み  名前決め まずgemの名前を決めます。
他のgemと名前が被ってはいけないので、
https://rubygems.org/ で検索して無い名前をつける必要があります。
面倒な場合は、 nickname-hellor-world とかなら被りません。
今回は ota42y_test_gem で作るので、後は適度に読み替えてください。
namespaceの階層を合わせるため、区切り文字は_ではなく-を使ってください。
ひな形作成 bundle gem ota42y_test_gem -t Creating gem &#39;ota42y_test_gem&#39;... Code of conduct enabled in config create ota42y_test_gem/Gemfile create ota42y_test_gem/lib/ota42y_test_gem.rb create ota42y_test_gem/lib/ota42y_test_gem/version.rb create ota42y_test_gem/ota42y_test_gem.gemspec create ota42y_test_gem/Rakefile create ota42y_test_gem/README.md create ota42y_test_gem/bin/console create ota42y_test_gem/bin/setup create ota42y_test_gem/.gitignore create ota42y_test_gem/.travis.yml create ota42y_test_gem/.</description>
    </item>
    
    <item>
      <title>committee &#43; prmd でJSON Schemaをいい感じに運用する</title>
      <link>http://ota42y.com/blog/2016/12/19/finc-advent/</link>
      <pubDate>Mon, 19 Dec 2016 22:37:37 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2016/12/19/finc-advent/</guid>
      <description>FiNC Developer Advent Calendar 2016の19日目の記事です。
要約  リクエスト・レスポンス形式のドキュメントはメンテが大変 メンテされていなくても気がつかないため放置され気味 committeeでJSON Schemaに実装が沿っているかを確認可能 prmdでJSON Schemaを楽に書ける＋人が読めるドキュメントが生成できる メンテされていない事が検出できるので続く（はず  API作成時のリクエスト・レスポンス形式問題 サーバとクライアントで並行開発しながらAPIを作成する場合、以下のようにリクエスト・レスポンス形式を整えにくいという問題があります。
 リクエストの形式が想定と違う  想定外のキーを送ってくる 構造が想定していない構造  レスポンスに知らないキーが含まれている 同じデータなのにエンドポイントによって形式が違う  enumの数値だったり、対応する文字列だったり カテゴリ名を返していたりカテゴリIDを返していたり   コミュニケーションや、APIに関するドキュメントをきちんとメンテしていれば回避可能な問題ですが、
以下の2つを保証する必要があり、何のサポートなしに維持するのはとても大変です。
 形式が全てドキュメント化されている ドキュメントと実装が揃っている事を保証する 同じデータは同じ形式になる事を保証する  そこで、今回話題にするJSON Schema+committee+prmdを導入することで、この問題を大幅に解決できます。
JSON Schemaで仕様を定義する JSON SchemaとはJSONを用いてデータを記述する際の形式の1つで、主にアノテーションやバリデーションを目的としているものらしいです。
詳しくはこちら
http://json-schema.org/
例えば以下のように定義すると、GET /v1/friendsが、access_tokenをパラメータとして必ず取り、戻り値には必ずfriendsとreccommendsというキーが含まれているという事を定義できます。 また、friendsとreccomendsの具体的な内容については別に定義して読み込みますが、同じ景色であるということも表現しています。 （なお、実際はもっと情報が多く、この部分だけでは動きません）
{ &amp;quot;title&amp;quot;: &amp;quot;show self friends with recommends&amp;quot;, &amp;quot;href&amp;quot;: &amp;quot;/v1/friends&amp;quot;, &amp;quot;method&amp;quot;: &amp;quot;GET&amp;quot;, &amp;quot;schema&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;, &amp;quot;required&amp;quot;: [ &amp;quot;access_token&amp;quot; ], &amp;quot;properties&amp;quot;: { &amp;quot;access_token&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot; } } }, &amp;quot;targetSchema&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;, &amp;quot;required&amp;quot;: [ &amp;quot;friends&amp;quot;, &amp;quot;recommends&amp;quot; ], &amp;quot;properties&amp;quot;: { &amp;quot;friends&amp;quot;: { &amp;quot;$ref&amp;quot;: &amp;quot;#/definitions/user/definitions/friends&amp;quot; }, &amp;quot;recommends&amp;quot;: { &amp;quot;$ref&amp;quot;: &amp;quot;#/definitions/user/definitions/friends&amp;quot; } } } }  JSON Schemaにそって定義を書くことで、形式が統一されプログラムから処理しやすくなるのと、他の場所の定義を共有できるため、</description>
    </item>
    
    <item>
      <title>Shinjuku.rb #42でRubocopについて話した</title>
      <link>http://ota42y.com/blog/2016/10/27/shinjuku-rb-42/</link>
      <pubDate>Thu, 27 Oct 2016 15:37:37 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2016/10/27/shinjuku-rb-42/</guid>
      <description> Shinjuku.rb #42に行ってきました。
http://shinjukurb.connpass.com/event/42245/
今回はコード分析・解析系のgemがテーマで、私はRubocopについて発表しました。
発表内容   Shinjukurb 42  from ota42y 
Rubocopはソースコードをパーサgemを使ってASTに変換し、ちゃんとどういう構造なのかを見ていて凄くちゃんと解析していたので凄いなーと思う反面、 自分でルールを追加するのもの凄く大変そうでした。
また、こういったコードの整形ツールは自転車小屋の議論になって導入するの凄く大変だよね…という闇の深いディスカッションが行われました… やはり何処も苦労するところは同じようです…(´・_・`)
その他 リファクタリングを支援してくれるgemが紹介されていましたが、 基本的に既存部分を切り出す→新しいロジックと同時に実行して実行結果をチェック→結果をDBとかに保存する、 といった手順を踏んでおり、どれもやることは変わらず、rubyのリファクタリング手法としては一般的な方法なのかな…？と思いました。 (稀にしか起きない問題や、副作用をどう消すかといった部分は別途考える必要ありそう)
Rubocopがやってくれる一般的なものではなく、もっと自分たち向けのチェック項目が欲しいけど、 ルール追加がめちゃくちゃ大変だなーと思っていたら、凄くいいgemの発表がありました。
soutaro/querly: Query Method Calls from Ruby Programs
メソッドが呼ばれているかをちゃんと解析してチェックできるので、
ヤバそうなメソッドや使い方が怪しいやつとかを抽出できるらしく、凄く良さそうでした。
他の方々の資料  reading suture scientist.md  </description>
    </item>
    
    <item>
      <title>Ginza.rb 第40回でprmdについて話した</title>
      <link>http://ota42y.com/blog/2016/10/22/ginza-rb-40/</link>
      <pubDate>Sat, 22 Oct 2016 22:37:37 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2016/10/22/ginza-rb-40/</guid>
      <description> Ginza.rb 第40回に行ってきました。
https://ginzarb.doorkeeper.jp/events/52895
今回は参加者がLTをする形式で、14人ぐらい？LTがありました。
発表内容   prmdのドキュメントが読みやすくなる話  from ota42y 
JSON Schema+prmd+committeeは、ドキュメントと実態を揃えられる＆揃える利点を増やしてくれるので、とてもいい構成だと思います。
また、prmdが出力するドキュメントにはcurlのコマンドやレスポンスのJSONが整った形式でついているため、 クライアント側としても手軽にAPIをテストすることができてとても有用なので、 最悪JSON Schema+prmdだけでも導入すると良いと思います。 (元クライアントからの意見)
その他 ruby c extensionが凄い面白かったのでちょっと触ってみようと思います(｀・ω・´)
他の方々の資料 #ginzarbのハッシュタグを見て見つかったものだけ上げています
 Ginza.rb 第40回 - koicの日記 Ginza.rb で &amp;ldquo;grifork&amp;rdquo; について発表してきた - weblog of key_amb 自分の使ったものをみせてみよう // Speaker Deck 勉強会/ginza.rb #40 - esa-pages.io Rails Authorization // Speaker Deck buoys gem の紹介  </description>
    </item>
    
    <item>
      <title>前職を退職しました</title>
      <link>http://ota42y.com/blog/2016/09/15/sea-bream-shock/</link>
      <pubDate>Thu, 15 Sep 2016 19:45:49 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2016/09/15/sea-bream-shock/</guid>
      <description>6月末に前職のソーシャルゲーム会社を退職しました。
今はRoRエンジニアとして、ヘルスケアベンチャーで働いてます(｀・∀・´≡｀・∀・´)
欲しい物リストは以下になります。
https://www.amazon.co.jp/registry/wishlist/11D376T6XISM6/ref=cm_sw_r_tw_ws_HMYoxbW6MSKSA</description>
    </item>
    
    <item>
      <title>恋になりたいAQUARIUMの聖地、伊豆・三津シーパラダイスに行ってきた</title>
      <link>http://ota42y.com/blog/2016/04/28/lovelive-sun-aquarium/</link>
      <pubDate>Thu, 28 Apr 2016 00:21:46 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2016/04/28/lovelive-sun-aquarium/</guid>
      <description>恋になりたいAQUARIUMの聖地、伊豆・三津シーパラダイスに行って来ました。
発売日当日に行きましたが、平日なので他に同じ目的で来ている人は見かけませんでした。

聖地の場所 このPVのほとんどは伊豆・三津シーパラダイスがモデルになっています。
みとしー自体も特に隠しておらず、公式サイトにモデルになったと書いてあります。
（2016/04/27現在）
行き方は三島から伊豆箱根鉄道の伊豆長岡駅で下車し、
伊豆・三津シーパラダイス行きのバスで２０分程度です。
シーンごとの場所 地図 公式ページの地図にプロットするとだいたい以下のようになります。
実際に行く人は参考にしてください(クリックで拡大)

また以下では視聴動画にある部分はその画像も乗っけておきます
入り口 見たとおり入り口です。


バスは何種類かあるらしく、行きも帰りもPVの色ではありませんでした。
もしかしたら沼津行きのバスがこの色なのかもしれません。
ショースタジアム付近 PVの中で１番で多く使われている場所です。
ただし、当然ながらステージ側は入れないため、構図が似ている写真は撮れませんでした。

皆が掃除？してるシーンや曜ちゃんが遅れて登場するシーンなどは全部このステージ前です。 

イルカジャンプ ステージ左側の階段がいるかが飛ぶシーンのやつです。
（この日は階段の上ではなくステージ上で同じパフォーマンスをしていました）


水槽前ステージ 手前の水槽前ステージの部分が、ヨハネ、ルビィ、梨子がイルカに水をかけられるシーンと、ルビィがトドと戯れるシーンになります。
なお、トドは実際にいてステージ部分まで降りてきてくれます。



花丸のお茶 ステージ中央から右を向くと花丸がお茶を飲んでいるシーンになります。


曜ちゃん捜索 ちょうどショースタジアムの真後ろになります。


皆で探そうとしてると位置的にぴったりだったり、光るクラゲの場所からは全然遠かったりと、
結構場所も考慮した上でPVが作られているのがわかります
ヨハネとイルカ像 ヨハネがポーズしてるシーンです。


曇りなのでわかりにくいですが肉眼だとうっすらと富士山が見えたので、多分晴れればあの構図で写真を撮ることができます。
一年集合 建物内部の展示コーナーです。


クラゲ万華鏡水槽 本当にいろんな色に光ってて凄かったです。


３年生集合 上記のクラゲの所に加えて以下の２カ所です。


ショーステージ うちっちーが出てくるところです。
なお、実際にうちっちーはいませんでした。</description>
    </item>
    
    <item>
      <title>ラブライブ！ありがとうトラックの軌跡をまとめた</title>
      <link>http://ota42y.com/blog/2016/03/16/lovelive-truck/</link>
      <pubDate>Wed, 16 Mar 2016 00:33:01 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2016/03/16/lovelive-truck/</guid>
      <description>ラブライブ！μ’sありがとうProjectの一環として、ありがとうトラックが日本全国を巡っているそうです。
http://news.lovelive-anime.jp/app-def/S-102/news/?p=11201
公式Twitterで毎日どこにいるかがツイートされており、3/14に折り返しに入ったとのことですが、 実際どのくらい回ったのかが細切れになっていてわからなかったので、地図上にまとめました。
ありがとうトラックの軌跡 写真付きツイートで位置が判明したものを表示しています。
オレンジ色がありがとうトラック１号の位置、白色がありがとうトラック２号の位置になります。
クリックすると大きい画面になるため、そちらの方が見やすいと思います。
ありがとうの軌跡
 3/15日の状況 １号は北海道まで太平洋側を北上していき、その後日本海側を南に進んでいるようです。 ２号は名古屋~奈良~四国から九州へと、太平洋側を南に進んでいます。
この後予定では１号は京都から岡山の方へ行き、２号はなんと沖縄に渡り、その後九州の西側へ行くようです。 １号は北海道から岡山までとかなり移動していますが、２号も沖縄に行って帰ってくるため、かなりの距離を移動するようです。 日本横断は相当大変ですね…
3/24の状況 沖縄へフェリーで渡るのはまる２日以上かかるようです。遠いですね…
また、残り一週間となりましたが残すは１号が広島と岡山、２号が福岡のみになりました。 残り日数的に東京に戻ってもライブまで時間が余りそうな感じなので、3/28か29あたりから東京も回りそうですね。
3/30の状況 ついに日本全国を回り終わり、東京に戻ってきています。
私も初めて実物を見ることができました。

4/1の状況 全ての日程が終了し、無事車庫に戻ったようです。
最後のツイートによると走行距離は2台合わせて14530Kmとのことです。
東京から南極点までの距離が14Kmなので南極点まで余裕で行くぐらいの距離を走ってたようです…
FAQ  どうやってまとめたの？
ツイートから気合いで探しています。 Google Mapsからそれらしい場所を探し、ストリートビューで写真と一致している場所を探しています。駅前やランドマークとの写真が多いため、思ったよりも簡単に場所が見つかります。
 若干ずれている気がするけど？
Google Mapsの地図編集画面から直接ストリートビューを見られないため、数百メートルのずれはどうしても出てきてしまいます。ご了承ください。
 いくつか無い写真があるけど？
地理にはかなり疎いため、いくつかわからない場所があります。もしその場所を知っている方は情報を頂けると幸いです。
 実物を見たいんだけどどこに行けばいい？
実際にどこにいるかの予測はできませんが、多くの場合県名になるようなかなり大きめの駅周辺にいるようです。 とはいえ公式Twitterが前日にどこに行くかつぶやいてくれるので、それを頼りに探すのが良いと思います。 見損ねてもたぶん最後にドームに来ると思われるので、ライブ参加者はもう一度見るチャンスがあります。
  場所が不明なもの 可能な限り位置を調べてはいますが、以下のツイートはどうしても位置がわからなかったものになります。 ご存じの方がおりましたらTwitter等でご連絡ください。
なお、社内が撮影されているもの等、どうやっても場所の判別が不可能そうなものは除いています。
https://twitter.com/LoveLive_staff/status/704889601014505472 https://twitter.com/LoveLive_staff/status/705003241365491713 https://twitter.com/LoveLive_staff/status/705288053242384384 https://twitter.com/LoveLive_staff/status/705740788551049216 https://twitter.com/LoveLive_staff/status/706777941347328000 https://twitter.com/LoveLive_staff/status/706834522441170945 https://twitter.com/LoveLive_staff/status/709003462529445888
https://twitter.com/LoveLive_staff/status/709762320650661890
(鬼の洗濯板のように見えるが、google mapsでは画像のように写真が撮れる位置が見つからず)
https://twitter.com/LoveLive_staff/status/710105060500439044 https://twitter.com/LoveLive_staff/status/710465459578146816
https://twitter.com/LoveLive_staff/status/711158993406857216 (できる人なら反射してる建物から場所がわかりそう…)
https://twitter.com/LoveLive_staff/status/711555116457992192 https://twitter.com/LoveLive_staff/status/711875536033878016 https://twitter.com/LoveLive_staff/status/713711952380473344 https://twitter.</description>
    </item>
    
    <item>
      <title>うるう日に1番なりやすい曜日は？</title>
      <link>http://ota42y.com/blog/2016/03/01/leap-day/</link>
      <pubDate>Tue, 01 Mar 2016 22:43:14 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2016/03/01/leap-day/</guid>
      <description>今年はうるう年のため、昨日の2016-02-29はうるう日でした。 うるう年は4年に一回かつ干支は12種類のため、うるう年になる干支は子と辰と申しかありません。
さて、うるう年にならない干支が存在しますが、うるう日にならない曜日というのは存在するのでしょうか？ 存在しないなら、うるう日になりやすい曜日というのは存在するのでしょうか？
気になったので調べました。
うるう年のルール うるう年は4年に一回、ただし100で割り切れて400で割り切れない年は来ないというルールになっており、400年間で97回うるう年がおきて一周します。 そのため、一周する400年間の傾向を調べれば、後はその繰り返しなので変化がなく、うるう日の傾向をすべて調べたと言えます。 今回は2000年から2399年までのうるう年を調べました。
うるう日になりやすい曜日は何曜日か 97は7で割り切れず、6余るため、全ての曜日が均等にうるう日になることはありません。 数学が得意な人ならば数式でぱっと曜日計算ができそうですが、私は得意ではないのでプログラムで全列挙して集計します。
うるう日カウントプログラム 以下のrubyスクリプトを実行する2ことで、2000年から2399年までのうるう日の曜日をカウントして集計します。
require &#39;date&#39; w = Hash.new(0) 2000.step(2399, 4) do |year| if Date::valid_date?(year, 2, 29) d = Date.new(year, 2, 29) w[d.wday] += 1 end end 7.times {|wday| puts &amp;quot;#{Date::DAYNAMES[wday]}:#{w[wday]}&amp;quot;}  結果 結果は以下のようになりました。
Sunday:13 Monday:15 Tuesday:13 Wednesday:15 Thursday:13 Friday:14 Saturday:14  月曜日と水曜日がうるう日になりやすく、日曜と火曜と木曜がうるう日になりにくい結果になりました。 もっと差が出るかと思いましたが、均等に起きた場合に比べて一回少ない程度に収まり、思った以上に偏りはないようです。
うるう日の偏りの原因 どうしてこのような偏りができるのでしょうか。 どうやら100年に一度のうるう年にならない年が影響していそうです。
うるう日は基本的に以下の順に曜日が変わっていきます。
火→日→金→水→月→土→木→火（2000年をスタートとした場合)
ですが、100で割り切れて400で割り切れない年はうるう年では無いため、曜日がずれてしまいます。 このとき、次のうるう日まで一日少なくなるため、次のうるう日の曜日は一日前の曜日になります（土曜日なら金曜日になる）
例えば、100年に一度のスキップが無い場合、2104年のうるう日は土曜日になりますが、2100年がスキップされたため一日前の金曜日になります。 これにより、火→日→金→水→金→水→月→土→木→火と、うるう日のループでは3つ前の曜日に戻ります。
これにより、2092年、2096年に金曜日と水曜日がうるう日になりましたが、2104年と2108年もまた金曜日と水曜日がうるう日になり、他の曜日より2回多くなります。同じように2204年は木曜日になるはずですが2200年の影響により水曜日になり、水曜日と月曜日が、2304年は木曜日になるはずが月曜日になり、月曜日と土曜日が多くうるう日になります。
まとめると、2100年の影響で金曜日と水曜日が、2200年の影響で水曜日と月曜日が、2300年の影響で月曜日と土曜日が多くうるう日になります。 そのため、400年間で多くならなかった日火木が13回うるう日になり、1回多くなる金土が14回、2回多くなる月水が15回うるう日になるようです。
次にうるう日が月曜日になるのはいつか？ 今年と同じく、月曜日がうるう日になるのは最速で28年後の2044年です。 例外でスキップされる場合を除けば28年後になります。</description>
    </item>
    
    <item>
      <title>2016年02月まとめ</title>
      <link>http://ota42y.com/pages/summary/2016/2016-02/</link>
      <pubDate>Sun, 28 Feb 2016 22:40:59 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2016/2016-02/</guid>
      <description>まとめ。
ぎりぎりに一気に書くスタイル…
 2015年のpixiv内ラブライブイラストの分析  去年との違いを意識したまとめ サンシャインのこれからには期待  dump/restoreコマンドでデータのバックアップ・リストアをする  思った以上に簡単で便利  開発・実行環境をDockerで整える  開発環境構築に時間かけるの人生の無駄なので凄くいい 遅いのは何とかしたい  Jenkinsのノード選択をNode and Label parameter pluginでやる  Pipeline Pluginは自分で道を開かないといけないので大変 こっちはできる事に限界があるが標準的なジョブ構成で使える  MacのDocker上で動くMongoDBのデータを永続化するの大変そう  凄くいいと思ったら意外な落とし穴が とりあえずMongoDBは外部に立てて凌いでます   </description>
    </item>
    
    <item>
      <title>Jenkinsのノード選択をNode and Label parameter pluginでやる</title>
      <link>http://ota42y.com/blog/2016/02/28/jenkins_node_select/</link>
      <pubDate>Sun, 28 Feb 2016 19:37:37 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2016/02/28/jenkins_node_select/</guid>
      <description> 概要 Jenkinsのノード制御はあまりカスタマイズが効きません。そのため、ノードを細かく制御したい場合はジョブを分ける必要があり、ジョブ数が増えて管理コストが上がります。このプラグインはパラメーターでノードを選択できるので、一つのジョブでノードごとにジョブを動かせるようになります。
Jenkinsのノード選択は柔軟性に欠ける Jenkinsのノード選択がパラメータでできるプラグインです。 Jenkinsでは基本機能として、実行するノードを制限して特定ノードでだけジョブを実行できます。 ですが、この設定項目は柔軟性に欠け、パラメータでノードを切り替えたり、該当するノード全てでビルドするといったことができません。 そのため、例えばgoでWindowsとMac両方でテストを実行したい場合、windows用とmac用というノードごとの個別のジョブを作る必要があります。 しかし、それを続けるとJenkinsのジョブの数が際限なく増えていき、ジョブのメンテが大変になるという問題が出ます。
Node and Label parameter plugin https://wiki.jenkins-ci.org/display/JENKINS/NodeLabel+Parameter+Plugin
Node and Label parameter pluginは、以上の問題点を解決できるプラグインです。このプラグインを入れると、ビルド時のパラメータにNodeとLabelを追加することができるようになります。

Nodeの場合は文字通り任意のノード名でビルドノードを選択できます。 また、Labelの場合はノードにつけたLabelでビルドするノードを選択できます。 そのため、ノードにwindowsやmac等の適切なラベルをつけることで、任意のノードでビルドすることができるようになります。 また、全てのノードに共通のラベルを一つ定義しておくことで、どれでもいいからとりあえずビルドするといったこともできます。
該当するノード全てでビルドする このプラグインは、特に設定がない場合は該当するノードのどれか一つでjobが実行されますが、以下のオプションをつけることで該当するノード全てで実行されるようになります。 そのため、複数環境でテストしたい場合も１回ビルドを実行するだけで後は自動で全てのノードでビルドが実行されるようになります。 なお、全ノードで実行される一つのビルドではなく、ノードごとに個別のビルドが実行されるようです。

Pipeline Pluginとの違い Pipeline Pluginでは、Jenkinsのジョブを丸ごとgroovyで書けるため、このプラグインよりもさらに柔軟に選択できます。 ただ、こちらは本当にジョブの内容を全てgroovyで書く必要があり、既存のJenkinsの知見を活用できなかったり、他のプラグインとの相性はあまり良くないです。 Node and Label parameter pluginはパラメータのみ変更されるため、標準的なジョブ構成であればとても簡単に導入できます。 そのため、どうしても複雑な制御が必要な時にのみPipeline Pluginを使うべきだと思います。
まとめ  通常の機能だとノードをあまり制御するのは難しい プラグインで任意のノードをパラメーターで選択できる  選択方法はノード名とラベル ラベルは複数に同じラベルをつけたりといろいろ便利なのでオススメ  ノード選択部分をさらに柔軟にしたいならPipeline Plugin  より複雑な事ができるが、導入の敷居も高い Jenkins Workflow Pluginで複数slaveを扱うのが楽になる   </description>
    </item>
    
    <item>
      <title>開発・実行環境をDockerで整える</title>
      <link>http://ota42y.com/blog/2016/02/25/docker_develop/</link>
      <pubDate>Thu, 25 Feb 2016 22:44:10 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2016/02/25/docker_develop/</guid>
      <description>概要 開発環境構築はだいたい辛い作業ですが、Dockerを使うことで構築がとても楽になります。もちろん銀の弾丸ではないので別のつらさは存在します。 しかし一歩も進まずに時間だけが過ぎることはなくなるため精神衛生上良く、開発を始める段階においてはとても効果的です。
開発環境構築が辛い 開発環境構築はプログラミングをする上で必須の作業ですが、依存関係など非常に多くの落とし穴があることが多いです。 また、環境構築そのものが目的ということは少なく、その先に別にやりたい事が控えていることが多いと思います。 そのため、とりあえずやってみようと思った際に、その試してみるべき部分にたどり着けずに時間を浪費していくのはかなり辛いです…
設定自動化スクリプトでは足りない 環境構築の手順が煩雑な問題に対しては、chefやAnsible等、自動で環境を構築する方法はいくつかあります。 これらは一度作れば同じ環境を何度でも作ることが出来ますが、2回目以降を楽にするものであるため初回はそれほど楽になりません。 また、すでに入っているものとの依存関係の問題などはこれらのツールでは解決しないため、 複雑な環境になればなるほど環境構築の難易度は上がります。
開発環境にDockerを使う このような問題に対応するために、Dockerを開発環境として使うことを考えます。 Dockerでは毎回クリーンな環境から必要な分だけインストールするため、 依存関係地獄にはまりにくいです。 さらに、Dockerfileで構築手順を保存できるため、 複数のマシンで同じ環境を揃えるのが簡単で、かつ不要になったらコンテナを消すことで容量の削減にもなります。
また、Dockerfileを本番環境に持って行き、そのままデプロイすることも可能です。 そのため、本番でだけバグが起きる…といったことも回避出来ます。
Dockerで開発環境を整える 細かい部分はDocker入門を読むのが早いと思います。 そのため、割とさくっとしか説明しません。
Dockerコンテナの作成 Dockerコンテナを構築するのに便利なDockerfileは、 ベースとなるコンテナをもとに、指定されたコマンドを実行してコンテナを作ってくれます。 例えば、以下のDockerfileはffmpegがインストール済みのコンテナに対して、 ニコニコ静画を使ったChainer用の学習済みモデルファイルを利用出来る環境を整えています。 ffmpeg自体はベースのコンテナに入っているため、その後にpythonの実行環境を入れ、様々なライブラリを入れているだけです。 コンテナの作成はこのDockerfileが置いてあるディレクトリまで移動し、 docker build -t ffmpeg_test . で作れます。
FROM cellofellow/ffmpeg:latest RUN apt-get -y update &amp;amp;&amp;amp; apt-get -y upgrade RUN apt-get install -y ccache curl g++ gfortran git libhdf5-dev RUN apt-get install -y python-pip python-dev RUN pip install numpy==1.10.2 # scipy RUN apt-get install -y libblas-dev liblapack-dev libatlas-base-dev gfortran RUN pip install scipy RUN pip install Pillow RUN pip install scikit-image RUN pip install chainer==1.</description>
    </item>
    
    <item>
      <title>dump/restoreコマンドでデータのバックアップ・リストアをする</title>
      <link>http://ota42y.com/blog/2016/02/07/dump_restore_command/</link>
      <pubDate>Sun, 07 Feb 2016 12:27:38 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2016/02/07/dump_restore_command/</guid>
      <description>まとめ  dump/restoreコマンドでパーティションごとバックアップ/リストアができる SDカードの中身を移行するのに便利  もちろんHDDでもいける Windowsは基本的にSDカードの第一パーティションしか認識できない   SDカードのデータ移行がしたい Androidに使っているMicroSDが速度面で不満が出てきたため、より早い物に入れ替えを行いました。
ですが、アプリのデータを入れていたりと、通常のMicroSDと使い方がだいぶ違うため、普通にコピペでバックアップ・リストアすると、権限周りやシンボリックリンク周りで問題が起きそうでした。
ASUS MeMO Pad 7 ME572CでLink2SDを動かす
そこで、dump/restoreコマンドを使い、ファイルシステムを丸ごとバックアップ・リストアしました。今回の対象はSDカードですが、対象のデバイスは関係ないため、 HDDを対象としたバックアップも同じ手順で可能です。
SDカードの第二パーティションのマウント Link2SDを使っているため、SDカードの第二パーティションにバックアップ対象が保存されています。このパーティションをマウントし、dumpコマンドを利用してバックアップを行います。
なお、Windowsでは基本的にSDカードの第二パーティションはマウントできないため、Linux上で作業を行いました。
# 接続された位置をfdiskで調べる sudo fdisk -l # 以下のようにSDカードのパーティションの位置が表示される # /dev/sdc1 42 42424242 4242424242 42 HPFS/NTFS/exFAT # /dev/sdc2 4242 42424242 42424242 42 Linux # ext3でフォーマットしているのでマウントする # (どうフォーマットしているかはその人次第です) sudo mount -t ext3 /dev/sdc2 /mnt/sd  dumpコマンドでバックアップ dumpコマンドを使うことで、ファイルシステムがext2,ext3,ext4のものをバックアップできます。 差分バックアップもサポートしていますが、今回はデータ移行が目的なのでフルバックアップを行います。
# mount先の/mnt/sd以下をバックアップ sudo dump -0 -f /data/sd.dump /mnt/sd # -0でダンプレベル0(フルバックアップ) # -fでバックアップしたデータのファイル名を指定  restoreコマンドでリストア restoreコマンドを使うことで、dumpコマンドでバックアップしたデータをリストアできます。 バックアップが終わった時点でSDカードをフォーマット済みの新しい物に差し替え、 上記の手順で同じ場所にマウントしてあります。</description>
    </item>
    
    <item>
      <title>2015年のpixiv内ラブライブイラストの分析</title>
      <link>http://ota42y.com/blog/2016/02/04/lovelive-pixiv-character-2016/</link>
      <pubDate>Thu, 04 Feb 2016 21:27:18 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2016/02/04/lovelive-pixiv-character-2016/</guid>
      <description>HP先行の結果が出ましたね。
…………ご察しください。
さて、前回(2015年のpixiv内ラブライブイラストの推移 )に引き続き、pixiv内のラブライブイラストについて見ていきます。
本来はもう少し内容を書く予定でしたが、落選のダメージがでかいので少なめです…＿(:3 」∠)＿
キャラごとの投稿数増加率 映画で人気が出たキャラがいないかと思い、2014年の12月から2015年の12月までに総投稿数がどれくらい増えたかを調べました。
特に増加率が大きい場合、映画によって人気が上がった可能性が考えられます。

小泉花陽と園田海未がかなり伸びていますが、この二人は去年12月時点では二期が始まってからの聖誕祭が行われていなかったので、前回が少なすぎただけの可能性があります。 一応投稿数の推移を調べてみたところ、他のキャラと同じく全体的に上がっており、劇場版で爆発的に人気が出たとかそういうのではないようです。
一方で、星空凛、高坂穂乃果、東條希の3人は2期の恩恵を受けた前回からも伸びているため、全体的に人気が上がったor劇場版で大きく伸びたといえそうです。 とはいえ、ほぼ全員が2倍ぐらい伸びているため、劇場版で人気が伸びたと言うほど差があるとは言いづらい結果になっています。 なお、にこは割合ではちょっと少なめですが、去年の投稿数、および総投稿数では一位であり、増加率では劣ってますが増加数ではものすごい量でした。
カップリング人気か、単体人気か イラストは必ずしもカップリングだけとは限らず、単体でも投稿されています。 前回の結果からわかるとおり、にこまき等のカップリングイラストはとても多いですが、キャラ単体とカップリングとでどちらの方が人気なのでしょうか？ そこで、キャラごとにカップリングとキャラ単体、どっちが人気なのかを集計しました。
なお、カップリングタグがついたものにはキャラの名前のタグがつけられる傾向にあるため、 キャラ単体でカップリングタグがないものと、カップリングタグがあるもの（キャラ単体のタグもあっていい）で集計しています。 また、投稿数だとキャラ別に比較しにくいので、そのキャラの全イラストとの割合で集計しています。
（全イラスト＝キャラ単体＋カップリングイラスト）

多くのキャラはキャラ:カップリングイラストが7:3になっていますが、南ことりはキャラ単体の人気が高く、80％以上が単体のイラストになっています。
一方で西木野真姫と絢瀬絵里はカップリング人気が特に高く、4割を超えています。 昨日の結果からわかるように、この二人はそれぞれ3種類のカップリングがランクインしており（「にこまき」「りんまき」「ほのまき」、「のぞえり」「うみえり」「ほのえり」）、やはりカップリング人気が高いのもうなずけます。
サンシャイン サンシャインが発表されておよそ一年たちましたが、pixiv上の反応はどうだったのでしょうか。 というわけで調べてみました。
サンシャインイラスト全体 サンシャインイラスト全体の投稿数の推移をまとめました。

CD発売で一気に盛り上がり、12月の段階では一段落した感じです。 μ&amp;rsquo;sの初期も同じく発売後2ndまでは落ちていったので、およそ同じ傾向にあるようです。
(参考: pixivのタグから読み解くラブライブイラストの歩み)
ただ、1月にAqoursのイベントが行われたり、アニメ化が発表されたりと大きく動いたため、今月にかけてまた一気に盛り返すと思われます。
Aqoursキャラごとの投稿数推移 Aqoursメンバーのキャラごとに投稿数推移を見てみました。
なお、月ごとの投稿数をグラフ化すると上昇下降が激しすぎたので、累計で集計しています。

センター総選挙では渡辺曜、黒澤ルビィ、桜内梨子の順でしたが、イラストではルビィがずば抜けて多く、次にヨハネ〜1年2年組と固まっています。
総選挙でも3年生は下にいましたが、イラストでも同じ結果になっています… とはいえ絶対数ではそもそもそんなに差は無く、μ&amp;rsquo;sのころに比べれば全然多いため、アニメ化や2ndでこれから先どう変化していくのが楽しみです。
まとめ  劇場版で爆発的に人気が出たキャラはいなさそう  うみぱなが伸びているが、2期以降初めての聖誕祭が今年だったためと思われる 他の伸びが大きいキャラもそこまでかけ離れてはいない にこは伸び率は悪いが投稿数は一番多い 伸びてないというより、元から凄い人気だっただけかと  キャラごとにカップリングか単体かはかなり好みが違う  西木野真姫と絢瀬絵里はカップリング相手としてかなり人気 南ことりはカップリングより一人の方が人気 他のキャラはだいたい単体7:カップリング3ぐらい  サンシャインはいったん落ち着いた  μ&amp;rsquo;sの1st以降と同じ傾向 ただし数自体は全然多い 総選挙とpixivの人気は違うみたい とはいえ、そこまで差は無い 2016/01で大きく動いたので今後の動きに期待   サンシャインに関しては先月にかなり大きく動いたため、ここ最近の変化も調べたいと思います。</description>
    </item>
    
    <item>
      <title>2016年01月まとめ</title>
      <link>http://ota42y.com/pages/summary/2016/2016-01/</link>
      <pubDate>Sun, 31 Jan 2016 16:54:04 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2016/2016-01/</guid>
      <description>まとめ。
Qiitaを再開した。
 Centos7でsquidを立てる  必要なときにサクっと作れるの便利 実際はプロキシサーバはVPNでつないで内部ネットワークにしているので、外部からのアクセスを想定した制限は不要  2015年のpixiv内ラブライブイラストの推移   全落ちつらい… サンシャインに関して取り直してるので次はもうちょっと時間かかる  tmux-change-paneというコマンドを作った  便利 CLIでもUIは大事  The innodb_system data file~でmacのmysqlが動かない場合の対処法  どのタイミングから動かなくなったのかは不明 無駄に時間を浪費した…  オーバーライドとオーバーロードは全く別物  そもそもオーバーロードは必要なのか？という疑問もある。 動的に切り替わるわけではないので、メソッド名が統一されるという利点があるけど… 暗黙的にキャストされるので、ビルドは通るけど意図した動作でない事が起きそう 内容が同じメソッドならいいけど、処理を振り分けるなら危なそう   </description>
    </item>
    
    <item>
      <title>2015年12月まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/2015-12/</link>
      <pubDate>Sun, 31 Jan 2016 16:36:57 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/2015-12/</guid>
      <description>まとめ忘れていたけどまとめ。
月5本以上何らかのアウトプットするのは達成。
 MongoDB の update は部分 update ではない  基本部分updateでいい気がするけど、なんでキーを置き換えるようにしているんだろう…？ MySQLと違ってどんな列があるか明確じゃないし、データを消すのが難しいからとか？  Wunderlistでサマリー出力をする  まだ使ってる。便利。  GithubのContributionsを使ってアウトプットを500日続ける  どうも表示される最大長しかカウントされないらしく、今見たらLongestが500より減っていた 常に手を動かし続ける習慣はできたし、良かったと思う  論文紹介「Unravel: Rapid Web Application Reverse Engineering via Interaction Recording, Source Tracing, and Library Detection」  めっちゃ便利。早く標準搭載されて欲しい。 普通のデバッガはわかっているコードに対してバグ調査に使えるけど、コード理解のお供としては使えないよね。  論文紹介「Corona: Positioning Adjacent Device with Asymmetric Bluetooth Low Energy RSSI Distributions」  Bluetoothだけで距離計測ができるとか凄い 実際適当に追実装してもいい精度になるので、ちゃんと作れば凄い良さそう。 ボードゲームの一部として使えそう   </description>
    </item>
    
    <item>
      <title>オーバーライドとオーバーロードは全く別物</title>
      <link>http://ota42y.com/blog/2016/01/31/override-overload/</link>
      <pubDate>Sun, 31 Jan 2016 14:45:49 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2016/01/31/override-overload/</guid>
      <description>EffectiveJavaを読んでて、C++でも当てはまるのかな？と思って試してみたら当てはまったので…
まとめ  オーバーライドされたvirtualなメソッド呼び出しは実行時に決定  実態が子クラスなら、親クラスとして扱っても子クラスのメソッドが実行される 実態が孫クラスの場合は孫クラスのメソッドが呼ばれる 親から子、孫へと呼び出される関数が探されていく感じ  オーバーロードされたメソッド呼び出しはコンパイル時に決定  virtual関数のように実態で切り替えが出来ない 実態が子クラスでも、親クラスとして扱うなら親クラスのメソッドが実行される 一致するものが無い場合は親クラスにキャストして一致するものを探す 子から親へと一致するものを探していく感じ   オーバーライドとは メソッドのオーバーライドとは、親クラスのメソッドを子クラスで再定義することです。 C++では親クラスでvirtualにされているメソッドをオーバーライドすると、 子クラスのオブジェクトを親クラスにキャストしてメソッドを呼び出しても、再定義された子クラスのメソッドが呼ばれます。 (virtualをつけないと親クラスのメソッドが呼ばれます)
たとえば以下のコードでは、オーバーライドされているためExtのshowメソッドが実行され、「show ext」が表示されます。
#include &amp;lt;cstdio&amp;gt; class Base { public: virtual void show() { printf(&amp;quot;show base\n&amp;quot;); } }; class Ext: public Base { public: virtual void show() { printf(&amp;quot;show ext\n&amp;quot;); } }; int main() { Ext *e = new Ext(); Base *b = static_cast&amp;lt;Base*&amp;gt;(e); b-&amp;gt;show(); return 0; }  このように、オーバーライドされたメソッドは実行時にどのメソッドが呼ばれるか決定されます。</description>
    </item>
    
    <item>
      <title>tmux-change-paneというコマンドを作った</title>
      <link>http://ota42y.com/blog/2016/01/26/tmux-change-pane/</link>
      <pubDate>Tue, 26 Jan 2016 22:24:44 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2016/01/26/tmux-change-pane/</guid>
      <description>tmux-change-paneというコマンドを作りました。
https://github.com/ota42y/tmux-change-pane
tmuxで現在開いているウィンドウの一覧を表示し、peco(pecol)で絞り込んで任意のタブに切り替えるアプリです。
似たようなものでswap-paneというものがすでにあります。
https://github.com/abicky/swap-pane
swap-paneでは複数画面に分割している際に、全ての分割をいったん消して選択画面を表示します。そのため、切り替えのたびに画面が一度フラッシュし、かつ必ず左上に作業領域が移ります。そのため、右下で作業してるような場合にかなり視線の操作が強制的に発生してとても面倒でした。
tmux-change-paneでは今いる分割領域しか使わないため、作業する位置が大きく変わることがなく、また切り替え中も他の領域の情報を参考にすることができます。</description>
    </item>
    
    <item>
      <title>2015年のpixiv内ラブライブイラストの推移</title>
      <link>http://ota42y.com/blog/2016/01/22/lovelive-pixiv-illust-2015/</link>
      <pubDate>Fri, 22 Jan 2016 14:14:08 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2016/01/22/lovelive-pixiv-illust-2015/</guid>
      <description>「はぁい！楽しい人生送ってる？」
私はファイナルライブ落ちたので楽しい人生は送れてません…
さてーーー
去年「pixivのタグから読み解くラブライブイラストの歩み」というのを書きましたが、それ以降である2015/12月までの変化をまとめました。
去年との違いに絞っているため、量は少なめですが、それでも多すぎるので分割して更新しています。 今回はカップリングイラストについての変化、次回はキャラごとのイラストの変化とサンシャインの動向についてを予定しています。
(次: 2015年のpixiv内ラブライブイラストの分析)
イラスト投稿数推移 こちらが2013年のアニメ一期以降のイラスト投稿数の推移を月ごとにまとめたものになります。

アニメ2期で一気に広まり、いったん落ち着きを見せましたが、映画が始まってから投稿数は一気に伸び、夏コミのある8月には月8000枚を超しています。ここ3ヶ月ほどは横ばいのため、いったん落ち着いたようです。
カップリング閲覧数 収集方法 前回と同じく、pixivのタグから各カップリングタグ（にこまき、のぞえり等）と、ラブライブもしくはloveliveが両方つけられているイラストの閲覧数の合計を集計しています。 また、「まきりんぱな」や「ことほのうみ」等の3人タグは除外し、完全一致する場合のみ集計しています。 (ラブライブ、loveliveについては!ありなしどちらでも可)
順序を気にしない場合の結果 「にこまき」と「まきにこ」を区別しない場合の2015年12月までの表は以下のようになります。

1,000,00以上のものは黒色、2,000,000以上のものは青色、10,000,000以上のものは赤色で表示しています。順序を考慮していないため対角線を挟んで値は同じです。
「のぞえり」「えりのぞ」と「うみえり」「えりうみ」を除くと、ほとんどが固定の順序になっており、順番を考慮してもしなくてもだいたい同じ結果になりそうです。 そのため、以降は集計を簡単にするために特筆しない限り順序を考慮して見ていきます。
順序を気にした場合の結果 2015年12月までの閲覧数の表 順序を気にする場合（「にこまき」と「まきにこ」は別物として扱う）、それぞれの結果は以下の通りになります。 なお、方向無しの場合とは違い、500,00以上のものは黒色、1,000,000以上のものは青色、2,000,000以上のものを赤色で表示しています。
前回と同じく「にこまき」と「まきにこ」の両方がつけられたようなイラストが存在し、こちらの集計では別々にカウントされるため、順番を気にしない場合のグラフとは結果が異なります。

2014年12月までの閲覧数の表 また、企画開始から2014/12月までに投稿されたイラストの閲覧数は以下になります。 この1年で閲覧数は変化しているため実際に去年の同じ時期とは値がだいぶ異なりますが、 このグラフと2015年12月までの表を見比べることで、一年間の変化を調べるのの参考にはなります。 比較する場合は、去年時点で閲覧数はこれより少なかったということを考慮に入れる必要があります。

カップリングイラストのまとめ 「にこまき」が圧倒的人気なのは今まで通りですが、去年は倍以上の差があった「のぞえり」が大きく増加し、差が縮まっています。 また、去年に比べて閲覧数1,000,000以上のカップリングが11組から19組と、かなり増加しています。なかでも「ゆきあり」はμ&amp;rsquo;sメンバー以外で唯一1,000,000を超えを達成しています。
色つきのものはほとんどが1.5~1.9倍ぐらいに伸びていますが、「ことぱな」が3倍近く、「まきぱな」「りんまき」が2倍以上と一年生が大きく伸びている印象です。 この中でも「りんまき」は去年の時点でもかなり高く、今年はもっと高くなっていることからかなり人気が上がっていると考えられます。
「ことうみ」や「りんぱな」はほかのカップリングを引き離し、メジャーカップリングとしての地位が確立しつつあるようです。 一方で「まきぱな」や「ことぱな」「ことえり」等、去年はそれほど閲覧数が多くなかったカップリングが今年になって2倍程度、「ことぱな」に関しては3倍近く伸びており、ここ一年でいろんなカップリングにも注目が集まっているようです。
なお、今回比較しているのは去年12月までに投稿されたイラストの現在の閲覧数であり、実際に去年の12月時点では現在よりも閲覧数は少ないため、実際の倍率はより大きいです。
カップリングイラストの推移 2015年度カップリングイラスト推移 では、実際今のトレンドと去年のトレンドはどう変化していったのでしょうか。 残念ながら閲覧数では時系列で推移を見ることはできませんが、投稿数は日時と紐付いているため、変化を見ることができます。
そこで、2015年の順序ありカップリングで赤文字になっている、閲覧数2,000,000以上の12カップリングと、それと同じくらい投稿数の多い三人組について、去年からの投稿数の推移をまとめました。 右側の凡例は、2015/12月現在で投稿数の多いもの順に並べています。

2015年度カップリング推移 上記のグラフは「にこまき」、「のぞえり」のぶっちぎり感がよく出ていますが、ほかのカップリングについては見にくいので、最大値を「ことうみ」に合わせて調節したグラフが以下になります。

「ほのうみ」と「りんまき」はほぼ同数のため被ってしまって見えていませんが、同じ線と考えて問題ありません。
カップリング増加率 また、各カップリングの増加率は以下の通りです。
200%以上のものに関しては赤字にしてあります。

カップリングイラストの推移まとめ 「ことうみ」「ほのうみ」「ことほのうみ」と園田海未を含むカップリングが大きく増えています。
園田海未は2014年の聖誕祭が2期前であり、2015年でその効果が含まれている事を考慮すると、ようやく本来の人気に追いついた感じがします。 また、「りんまき」も増加しており、かつ閲覧数もかなり増加していたことから、「りんまき」は2015年でかなり人気になったと思われます。
去年もそうでしたが、閲覧数では「ことうみ」の方が「りんぱな」よりも多いのに対し、投稿数では「りんぱな」の方が「ことうみ」よりも多い状態になっています。 この現象は「ことほの」と「ほのえり」にも見られているため、勘違いではなさそうですが、いまいち原因がわからないので、もう少し調査しようと思います…
ほのキチの人気が分散している問題 去年もそうでしたが、カップリング投稿者数では穂乃果を含むカップリングは上位にはいませんが、 代わりに4人との別々のカップリングと、3人組がランクインしており、ランキングに一番多く載っているキャラになっています。（ことほの、ほのえり、ほのうみ、ほのまき、ことほのうみ） さらにこれらのカップリングはだいたい似たような順位についているため、穂乃果はこれといったカップリングがなく、人気が複数のカップリングに分散していそうです。
(絢瀬絵里もカップリングだけなら4つ載っていますが、穂乃果ほど分散してるとは考えにくい)</description>
    </item>
    
    <item>
      <title>論文紹介「Corona: Positioning Adjacent Device with Asymmetric Bluetooth Low Energy RSSI Distributions」</title>
      <link>http://ota42y.com/blog/2015/12/25/hci_advent_25/</link>
      <pubDate>Fri, 25 Dec 2015 00:39:52 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/12/25/hci_advent_25/</guid>
      <description>まとめ  BluetoothのRSSIだけで対象のデバイスがどこにいるかわかる  動画を見るのが手っ取り早い https://www.youtube.com/watch?v=VqAoxVhO4xA  BluetoothのRSSIが位置によって違うことを利用している  RSSIを学習すれば、どこにあるか予測可能  オリジナルより精度が悪いて実装をしてみたが、それでも結構ちゃんと動く  https://github.com/ota42y/corona-clone https://youtu.be/wsglHEhtYCM   はじめに このエントリは「ヒューマンコンピュータインタラクション論文紹介 Advent Calendar 2015」の25日目のものです。
http://qiita.com/advent-calendar/2015/hci
紹介する論文 今回もUIST2015から、Corona: Positioning Adjacent Device with Asymmetric Bluetooth Low Energy RSSI Distributionsという論文を紹介します。
これはBluetoothのRSSIだけを利用し、デバイスの相対的な位置を検出する手法についての論文です。
動画はこちらになります。

手法について Bluetoothのチップの位置はデバイスの中心からずれたところに置かれているため、 各辺からチップまでの距離はそれぞれ異なり、結果としてRSSIが異なってきます。この距離の違いから来るRSSIの違いを学習しておくことで、 現在のRSSIからどの位置に置かれているかが識別できるというのがこの論文で提案している手法です。
わかりにくいと思うので図にしました。灰色の長方形内にあり、中心線上にない任意の青色の点から、 オレンジで表された各辺までの距離が全て異なるということがわかると思います。

実際、私の手元のタブレットでは左上にチップがあるため、右辺や下辺は遠く、上辺や左辺は近くなっており、RSSIも-40~-20までかなりの差が出ます。 値は常に若干上下しますが直近数秒でまとめるとだいたい安定しているので、確かに識別に使えそうです。
やってみた RSSIだけで位置が測定できる！と聞いても半信半疑だったので、実際にやってみました。
(15秒あたりから開始)
https://github.com/ota42y/corona-clone
実際に動いている様子は以下の通りです。

オリジナルの手法をそのまま実装するのが大変だったので、学習量を少なめに、かつ適当なベイズ識別期に投げるだけとかなり簡素化しています。
動画の通り、かなり簡素化した状態でもわりといい精度でどの位置にあるかを識別できていますので、 ちゃんとオリジナルのを実装すれば相当な精度になることが予想できます。
その他 Bluetoothチップが中心からずれていることに依存しているので、今後中心や中心線上に乗ったデバイスに対しては識別できるのか心配です。 たとえば横方向の中心線上（上辺と下辺から等距離）の場合、左右では距離が違いますが上下では距離が同じになります。 もちろん片方のデバイスが等距離でももう片方が等距離でないなら、上に置いたときと下に置いたときで距離が変わるため識別可能ですが、どちらも同じ距離になるようにおいた場合は判別ができなさそうです…
とはいえ、動画のサムネイルや論文の図にあるRSSIの等高線が円ではなく多角形の形をしていることから、 内部の基板やデバイスの材質によって大きく変動するため、完全にRSSIが同じ2点が存在するというのはほぼあり得ないと考えて良さそうです。</description>
    </item>
    
    <item>
      <title>論文紹介「Unravel: Rapid Web Application Reverse Engineering via Interaction Recording, Source Tracing, and Library Detection」</title>
      <link>http://ota42y.com/blog/2015/12/13/hci_advent_13/</link>
      <pubDate>Sun, 13 Dec 2015 07:01:33 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/12/13/hci_advent_13/</guid>
      <description>まとめ  Webページのリバースエンジニアリングの支援をする研究 コール情報やDOMの変化を記録、閲覧可能に 該当のソースを見つけるまでの時間が早くなる chrome拡張が公開されているので実際に試せる  はじめに このエントリは「ヒューマンコンピュータインタラクション論文紹介 Advent Calendar 2015」の13日目のものです。
今回はUIST20015からWeb系の論文でいいのがあったので紹介します。
実は昨日のUIST勉強会で読んだので、参加してる人にとってはまたかよ…ってなりますが、
去年を見る限りUIST勉強会は資料を公開しないようなので、参加しなかった人にとってはお得なはずです…
論文の内容 今回紹介する論文は「Unravel: Rapid Web Application Reverse Engineering via Interaction Recording, Source Tracing, and Library Detection」です。 この論文は、UnravelというWebページのリバースエンジニアリングを支援するツールについての論文です。
このツールはまず、Webページ上で動いているDOMの変化やJSのコール情報を記録していきます、 そして、Chromeのデベロッパーツールにタブを追加し、そこに変更回数や呼び出し回数でソートして情報を表示します。 これにより、Webページ上での動きが、どのようなJSとDOMの変化によって実現されているのかを簡単に調べることができます。
動画はこちらになります。

先行研究との比較 論文中でいくつかあげられていますが、先行研究は条件が限定的だったり、巨大なサイトを対象にしたときに動作が遅くなってしまうのに対し、 このツールは任意のサイトに適応可能で、巨大なサイトでも遅くならないと述べられています。
実装方法 documentへの書き込みを全てフックして情報を保存してるらしいです。 そのため、他の研究に比べて取れる情報が限定的になりますが、任意のサイトに適応可能になっているとのことです。 また、後述するようにソースが公開されているので、詳しく知りたい方はコードを読むのがいいと思います。
実験 このツールのありなしで、TumblerやApple等のサイトのアニメーションを再現するタスクを行い、途中のチェックポイントごとに比較をしたそうです。 結果として、システムアリの方が関連するソースの最初に見つけるまでの時間が50%短縮され、全体としても30%短縮されたそうです。 ただし、関連するコードを見つけてからタスク終了までの時間に差は無いらしく、動きからコードの位置を調べるのには役に立っているが、 ソースコードが理解しやすくなっているわけではなさそうです。 なお、上級者と初級者で特に差は無かったそうです。
残念なことに、論文ではこのツールのありなしでしか比較しておらず、先行研究との比較がされていませんでした。 このツールの押しの1つであるポータブル製（環境が特殊で無くてもどこでも使える）は比較が難しいですが、 もう一つの巨大なサイトでも遅くならないという速度に関しては比較できそうなので、そっちは既存のやつとの差を調べてほしかったと思いました。
その他 このツールは公開されているので普通に使う事が出来ます。
https://github.com/NUDelta/Unravel
ただし、minifyされているとJSのコール情報部分がとてもわかりにくいので、事前にunminifyしておく必要があります。
（実験ではunminify済みの状態で行った）
知っているコードならともかく、知らないコードに対しては動きと対応する部分を見つけるのはとても大変なので、 もの凄く便利そうでした。自動minifyとかが出来ると、普通に実用的なツールになると思います。
なお、今日が「ヒューマンコンピュータインタラクション論文紹介 Advent Calendar 2015」の13日目ですが、14日目はあいているのでおすすめな論文がある人や面白い論文を読んだことがある人は是非！</description>
    </item>
    
    <item>
      <title>GithubのContributionsを使ってアウトプットを500日続ける</title>
      <link>http://ota42y.com/blog/2015/12/09/500_output/</link>
      <pubDate>Wed, 09 Dec 2015 07:00:01 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/12/09/500_output/</guid>
      <description>前からGithubのContributionsを使ってアウトプットを150日続けるというものを続けていましたが、ついに500日たちました。ちょうどいい区切りなので振り返ってみようと思います。

良かったこと  常にプログラムを書いたり、物事を調べてまとめる習慣が出来た 「いつかやろう」を放置せずに取りかかるようになった  およそ前に考えたとおり、アウトプット量を確保するために色々なことに手を出し、良い習慣になっていたと思います。
問題点  一つ一つのアウトプットの質が安定しない 読書など長いインプットをしなくなった github以外のアウトプットができなくなった  特に質が悪い部分に関しては前から予想できており、 そのときはまずは質より量の方が重要と思い、特に対策はしませんでした. ただ、最近は量は担保できるようになったので、次は質かな…と思っています。
また、本を読むなど量の少ないアウトプットしか出来ない事に対して消極的になってしまいました。 これは質の高い1つのアウトプットより、コードを書くなど試行錯誤した安定しないアウトプットの方が量が稼げるため、 最適戦略をとるとどうしてもいいアウトプットを作る方向に行かないのが原因です。
また、計測をGithubでやっている関係上、QiitaなどGithub以外のアウトプットは加算されないため、ほとんどしなくなっていました。
これから 利点もいくつかありましたが、同時に欠点も見えてきました。 特にgithubに固定化されてしまうことや、毎日更新という量に縛られる事が問題となっている気がします。
そのため、次からは月に5回以上の文章によるアウトプットをする方向で行こうと思います。 これにより、github以外のアウトプットも換算できますし、コードを書いてそれを文章でまとめればコーディングも換算できます。 また、回数が減ったことにより量を確保することに縛られなくなるため、時間がかかるような事も出来るようになります。
とりあえず今月から初めて見ようと思います。 毎月前月の分をまとめることで（まとめエントリはカウント外）、継続しているかのチェックにもなりそうです。
しばらくやってみて、続かなそうならまた一日一コミットに戻そうと思います。</description>
    </item>
    
    <item>
      <title>Wunderlistでサマリー出力をする</title>
      <link>http://ota42y.com/blog/2015/12/08/wl_report/</link>
      <pubDate>Tue, 08 Dec 2015 07:58:08 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/12/08/wl_report/</guid>
      <description> 最近知ったWunderlistが、全プラットフォーム使えたり、 モバイルアプリが使いやすかったり、タスクがショートカットで追加できるデスクトップアプリがあったりと、なかなか便利そうなので今試しに使っています。
ただ、終了したタスク一覧をさっと見られないため、あれやったっけ？とか、今週疲れ気味かも…とか、がんばり具合とかがわかりません。 たぶんあった方が良さそうな気がしたので、過去一週間に完了したタスクを日ごとに分けてレポート出力するやつを作りました。
すでにCLIから一覧を取ってJSONで書き出せるものがあるので、これの出力を利用します。
https://github.com/robdimsdale/wl
使い方 以下のように利用します。
 wl tasks --completed true -j | go run report.go 
こんな感じに日ごとに終了したタスクがリスト化されていきます。
 2015-11-30 10 tasks completed
07:37:51 mail check
07:59:23 review PR
22:06:09 create blog entry
 コード コードはこれです。 
その他  全部のタスクを並べるのはつらいので、3日以前は終了タスク数のみとかにしたい
 コマンドが面倒なので、単体で完結したい  単体で完結するようになったらgithubのリポジトリ作るかも  もうちょっと見やすくしたい  </description>
    </item>
    
    <item>
      <title>MongoDB の update は部分 update ではない</title>
      <link>http://ota42y.com/blog/2015/12/02/mongodb_update/</link>
      <pubDate>Wed, 02 Dec 2015 07:25:25 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/12/02/mongodb_update/</guid>
      <description>まとめ  MongoDB の update はレコードのID以外の要素を全て置き換える 指定したカラムだけ置き換えてはくれない 部分 update したい場合は専用の方法でクエリを作る  MongoDB の update はレコードをほぼ全て置き換える 以下のように insert して update を実行します
db.test.insert({a: 1}) db.test.update({a:1}, {b: 1})  # ruby版 mongo = Mongo::Client.new(&amp;quot;mongodb://localhost&amp;quot;) col = mongo[:test] col.insert_one({ a: 1 }) col.find(a: 1).replace_one({b:1})  insert と update とでキーが違うため、指定した部分だけが書き換わり
{a:1, b:1}
となりそうですが、実際は
{b:1}
と、後のデータで完全上書きされてしまいます。
このように、MongoDB の update はデータの全置き換えを実行します。
なお、ObjectId は変わらないため、消して再度 insert しているのではなく、ObjectId 以外のデータを全て変更しています。
回避方法 もちろん毎度上書きだととても不便なため、回避方法が存在します。
以下のように、update する際に $set のキーとして変更するデータのみを渡します。
これにより、指定したものだけ書き換えられるようになります。
db.test.update({a:1}, {$set:{b: 1}})  # ruby版 col.</description>
    </item>
    
    <item>
      <title>バッカソン（おばかハック）! #MA11 に参加してきた</title>
      <link>http://ota42y.com/blog/2015/10/18/ma11/</link>
      <pubDate>Sun, 18 Oct 2015 22:23:57 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/10/18/ma11/</guid>
      <description>http://hacklog.jp/events/view/61
こんなの(バッカライザー)を作ったところ、Microsoft賞を頂きましたヽ(´ー｀)ノ

以下反省も含めたメモメモ
 Cloud.9便利  いい点 オンラインIDE+公開サーバ 複数人で同時編集可能 作ってその場で動かせて共有可能なのでもの凄い便利 sudoも自由にできる はまりどころ 謎の挙動が多い  URLの末尾にいつの間にか-1がついてたり プログラムから自分のURLを取ると:80がついていたり(URLには無い) コンソールが8行ぐらいしか出なかったり(同時に開いている一番小さいサイズになる?)   Microsoft Project Oxford便利  精度のいい文字認識 顔認識も Cloud9の公開ディレクトリに保存してURLを渡すだけで終わる  チーム分担  全員エンジニアなだけあって、役割分担は凄く上手くいった 特にタスク切らなくても勝手に動いていく感じ 実装、コンテンツ作成、調査と分担もいい感じ 結果としては後述するように着地点の設定が甘かった  アイデアだし  そもそも技術的に難しそうなチャレンジにフォーカスして中身に集中できなかった 認識部分をどう作るか？にフォーカスしすぎた 今回のハッカソンの趣向的に、モノの仕上がり具合やプレゼンに集中すべきだった チャレンジよりコンテンツ作りに振るべき 思しくすることに時間を割けなかった 発表プレゼンとかも  技術力不足  画像認識によるシール判別をしたかったが時間が足りず断念 事前にある程度作れるものをまとめておいた方が絶対にいい 何を作るかだけにフォーカスする 登録された画像パターンの検索は何かと便利そうだし作っておいていいかも   色々と問題点が明らかになりましたが、やっていてとても楽しかったです。
今回のMAはもう終わりますが、次のMAは色々なハッカソンに参加してみようと思います。</description>
    </item>
    
    <item>
      <title>Debian7にapt-getでmongodb3.0が入れられない</title>
      <link>http://ota42y.com/blog/2015/10/14/mongodb3-debian7-32bit/</link>
      <pubDate>Wed, 14 Oct 2015 23:24:16 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/10/14/mongodb3-debian7-32bit/</guid>
      <description>まとめ  Debian 7 32bitではmongodb3.0はapt-getで入れられない tarballで入れられるが、前のmongodbとファイル位置が違うので注意 面倒ならdbファイルを/data/dbに移動してしまうのが楽そう  Debian 7 32bitのmongodb Debian 7 のmongodbを以下の公式サイトの手順で入れようとしても、
3.0が見つからず最高でも2.6までしか入れる事ができません(2015/10/12現在)
Install MongoDB on Debian
mongodbのaptのリポジトリを見ると、amd64版にはたくさんありますが、
i386版のリポジトリは空です。
このため、apt-getでmongodb3.0系を入れるのはできないらしく、
tarballからインストールする必要があります。
ただし、aptで入れた2.6からアップデートし、既存のDBファイルを使う場合は若干面倒になります。
aptからのアップグレード時の注意点 新規に入れる場合は公式サイトの手順通りですが、aptからのアップグレードの場合は問題が起きます。
公式サイトの手順では、ログインユーザで/data/dbをDB書き込み先として使いますが、
aptで入れた2.6系統は特に設定しない場合、mongodbユーザで/var/lib/mongodbにdbファイルを書き込んでいるため、
そのままですと前のDBが読み込めません。
そのため、/usr/bin以下にmongodbを展開し、mongodbユーザで実行可能にするか、
mongodbユーザがtarballの解凍と実行をする必要があります。
今回は後者の方式をとります。
インストール手順 公式サイトの手順からインストール先を変え、実行時にオプションを指定しているだけです。
Install MongoDB From Tarbal
wget http://downloads.mongodb.org/linux/mongodb-linux-i686-3.0.6.tgz tar zxvf mongodb-linux-i686-3.0.6.tgz sudo cp mongodb-linux-i686-3.0.6/bin/* /usr/bin  起動は以下のようにユーザとdb位置を指定します
sudo -u mongodb mongod --dbpath /var/lib/mongodb  まとめ 起動が面倒なら、/var/lib/mongodbを/data/dbに移動して所有権を書き換えるのもいいと思います。</description>
    </item>
    
    <item>
      <title>IBM BluemixのAlchemyAPIで超簡単に画像認識する</title>
      <link>http://ota42y.com/blog/2015/10/11/alchemyapi/</link>
      <pubDate>Sun, 11 Oct 2015 22:39:33 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/10/11/alchemyapi/</guid>
      <description>まとめ  AlchemyAPIに画像をアップロードすると識別結果を返してくれる 無料で使える ライブラリも豊富 画像認識がAPIで呼び出せる世界の可能性凄い  画像認識ライブラリすら必要ない世界…   AlchemyAPIで画像認識が凄い簡単にできる 先日のABPro2015でこんな発表をしました。
  身近なサイバー攻撃から身を守る  from ota42y 
ここで画像の判定にAlchemyAPIを利用しています。
Twitterの画像URLをAPIに投げるだけでかなり精度のいい識別結果を返してくれるため、とても便利です。
AlchemyAPIとは？ http://www.alchemyapi.com/
IBMが買収した機械学習によるデータ分析会社です。
画像認識や自然言語処理に強い会社だったらしく、その一部をIBM Bluemixで提供しています。
IBM Bluemixは最小構成なら無料で使え、
(おそらく)AlchemyAPIの使用権はアカウントを持っていれば1日1000件まで無料らしいので、
回数は限られますが無料で画像認識ができます。
どうやらIBM Bluemixは単なるクラウドサービスではなく、
音声認識や機械翻訳、データ処理等複雑な処理をサービスとして簡単にセットアップできるようになっているらしく、
その中の1つにAlchemyAPIが含まれているようです。
なお、IBMが用意した画像認識サービスもあるらしく、そちらの方はBetaですが制限はないようです。
使い方 IBM Bluemixのアカウントを作り、適当なアプリケーションを1つ作成します(仮想マシンを一個作る感じ)。
その後、サービスからAlchemyAPIを選んで作ったアプリケーションに登録するだけです。
登録が終わると作ったアプリケーションの環境変数としてapikeyが設定されるので、
それを利用して画像のURLやデータをAPIに投げれば認識結果が帰ってきます。
ライブラリ 公式がいくつかの言語のライブラリを提供しています。
https://github.com/AlchemyAPI
golangによる画像認識APIは無かったため、ライブラリを作りました。
使いたい方はどうぞ。
https://github.com/ota42y/go-alchemyapi
サンプルコード package main import ( &amp;quot;os&amp;quot; &amp;quot;fmt&amp;quot; alchemyapi &amp;quot;github.com/ota42y/go-alchemyapi&amp;quot; ) func main() { token := os.Getenv(&amp;quot;ALCHEMYAPI_TOKEN&amp;quot;) if token == &amp;quot;&amp;quot; { fmt.Println(&amp;quot;skip this test because no token&amp;quot;) return } client := alchemyapi.</description>
    </item>
    
    <item>
      <title>ABPro2015で発表してきた</title>
      <link>http://ota42y.com/blog/2015/10/10/abpro2015/</link>
      <pubDate>Sat, 10 Oct 2015 23:58:16 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/10/10/abpro2015/</guid>
      <description>togetterまとめ
  身近なサイバー攻撃から身を守る  from ota42y 
ディープラーニングによる画像認識でTL上の飯テロ画像を判定しています。
事前に発表タイミングをお昼直前にお願いしており、
その通りにしていただけたため大好評だったと思います。
なお、画像認識自体はAlchemyAPIに投げているだけです.</description>
    </item>
    
    <item>
      <title>第48回情報科学若手の会2015に参加してきた</title>
      <link>http://ota42y.com/blog/2015/09/22/wakate2015/</link>
      <pubDate>Tue, 22 Sep 2015 00:45:03 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/09/22/wakate2015/</guid>
      <description>2015年9月19日(土)〜21日(月)に静岡県伊東市の山喜旅館で開催された、
第48回情報科学若手の会2015に参加してきました。
四回目の参加で、ショートで発表を行いました。
第48回情報科学若手の会まとめ #wakate2015
1日目  はじめてでもわかる!IoTの過去・現在・未来 (特にホームネットワーク)
 http://www.slideshare.net/TsubasaYumura/iot-52973276 TRON電脳住宅  1988年にIoTに言及していた電脳社会論  BLEとか今使われている技術の特徴とか 最新研究とかも(IllumiRoomとかSquamaとか) IoTは最近いろんな製品が出てきてますし、有名なのだとIFTTTや国内だとmyThingsみたいに、インターネット側も実世界の機器との連動ができるサービスが出てきていて、数年でアーリーマジョリティぐらいまでは到達しそうかなーって気がします。  IoTで進化するミツバチとの交流
 https://speakerdeck.com/decobisu/iotdejin-hua-surumitubatitofalsejiao-liu 知られざるミツバチの実態  ミツバチは刺激しない限り刺さない ハチミツがとれるぐらい蜜を集めるのはミツバチだけ 養蜂は意外とブームらしい(観測範囲は狭い)  巣箱の温度をセンサとRasPiで調べてMackerelに飛ばしてSlackに通知 めっちゃ簡単に作れそうな構成だけど、凄く役立ちそうでテンション上がる 動物とのインタラクション研究とかわりとあるので、ミツバチとのインタラクションも普通に研究になりそう… TLで言及されていたハチミツを集めやすい巣箱が凄い  https://www.indiegogo.com/projects/flow-hive-honey-on-tap-directly-from-your-beehive#/story https://twitter.com/kyoro353/status/645125093425676288 ハンドルを回すと六角形が崩れて一直線上になるので、ハチミツが下に流れていく機構   プログラムを書くってどういうこと？を哲学的に考える話
 話は凄い上手いし言ってることも解るのに理解が追いつかない…(´･_･`) プログラムは人が計算機で計算をしているor計算のさせ方を教示しているという仮定の下では、プログラムを書く行為はプログラムが表現できる世界への対応付けを行っている…という話だった気がします。
理解が追いついていないのでスライドがほしい…  群ロボット
 複数のロボットを使って1つのタスクを実行する  ベイマックスのマイクロロボットみたいなもの？  複数のロボットを強調させるために動物の動きを模したアルゴリズムを使っているらしい  実際の動物とは全く関係ないアルゴリズムとかあるのだろうか？  既存の探索を行うアルゴリズムは通信回数や移動で性能が良くない  それを改善するアルゴリズムを作った ただしコストが安い代わりに探索時間はかかる  質疑応答で無線通信ならブロードキャストできるので、既存アルゴリズムも通信コストは安いのでは？という質問  無線ってブロードキャストできるんだ…(よく知らない   Alloyで学ぶ形式手法
 https://speakerdeck.com/marin72_com/alloydexue-buxing-shi-shou-fa 形式手法言語  設計時に使うらしい 仕様バグを調べるみたい？ AlloyはLL形式手法言語みたいなもの  仕様を定義して、ちょっとの記述をするだけで反例があるかを返してくれる  自動でテスト条件を検証してくれるみたい？ めっっっっっっちゃ便利っぽい  スライド中のサンプルコード</description>
    </item>
    
    <item>
      <title>第48回情報科学若手の会2015で発表してきた</title>
      <link>http://ota42y.com/blog/2015/09/20/wakate2015-hci/</link>
      <pubDate>Sun, 20 Sep 2015 17:10:23 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/09/20/wakate2015-hci/</guid>
      <description>というわけで発表しました。
発表資料はこちら。
実際は動画で紹介していますが、めっちゃ重くなったのでそれは外した軽量版になります。
代わりにリンクをスライドに書いてあるので、そちらで見てください。
また、実際は使わなかった非表示スライドも表示していますが、ちゃんと作っていないのでご注意ください。
  HCI分野の紹介と最新研究  from ota42y 
発表内容について 4回目ぐらいの参加ですが、ほぼ毎年CHIの締め切りと重なったり、
UISTの数週間前みたいな時期なのであまりこの分野からの参加者が少なく、
需要がありそうとういうのを去年から考えていました。(去年のエントリ)
何故か今年に限って似た分野の研究が多かったですがw
新しいインタフェース系は結構話題になりますが、改善とか調査系とかはあまりHCI分野の人以外からは話題にされないので、
意図的に新しいインタフェース系は抑えてそれ以外のやつを入れています。(非表示にたくさんあるけど)
実際、その辺についてはあまり聞かないのでためになったという声を頂いたので、
いい案配だったとおもいます。
反省点 質疑応答で盛り上がるだろう場所は予測できていたので、
そこに関しての補足スライドを用意するべきでした…
事前に作ったスライドを、前日にダメダメに見えたので作り直してしまったので、
結果としては良くなったけどもう少し早くに完成させて修正したかったです。
ただ、調査を1ヶ月ぐらい前から始められたので、その点に関してはわりとよかった。
追加情報 今回のはほぼ英語論文から取ってきましたが、日本語での発表を捜すならインタラクションがオススメです。
また、質疑応答時やTwitterで追加情報を流していただきました。感謝です。
HCI分野でも（調査系と比べて）とくに新技術の発表が多いトップカンファレンスACM UISTは、来年アジア初開催、「日本で」開催ですよ！みんなで行こー #wakate2015 / 今年は http://t.co/GY2XiamnkI
&amp;mdash; arc@dmz (@arcatdmz) 2015, 9月 20 
HCI系のトップカンファレンスACM CHIの自主勉強会が毎年東京と北海道で開催され、過去の資料が公開されているので、HCIに興味持った方はまずこの資料見てみるといいと思います！ http://t.co/WTIFCbO04m #wakate2015
&amp;mdash; 湯村 翼＠10/3おうちハックナイト (@yumu19) 2015, 9月 20 
調査系（エスノグラフィー）なども多くユーザビリティを仕事にしてる方も参考になりそうなトップカンファACM CHIは、今年アジア初開催、韓国でした。 http://t.co/7iFp4VFQPG ←6月の勉強会 #wakate2015 https://t.co/VsVFP9y5Ov
&amp;mdash; arc@dmz (@arcatdmz) 2015, 9月 20 
#wakate2015 HRI http://t.</description>
    </item>
    
    <item>
      <title>freeコマンドに新しく追加されたavailableについて</title>
      <link>http://ota42y.com/blog/2015/09/09/free-command/</link>
      <pubDate>Wed, 09 Sep 2015 21:53:07 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/09/09/free-command/</guid>
      <description>freeコマンドに追加されたavailable 3.14および2.6.27から、freeコマンドの書式が変わり、
-/+ buffers/cacheが消えてavailableという項目が増えました。
availableは/proc/meminfoの中にあるMemAvailablの値を参照しています。
この値は、新しいアプリがスワップせずに使える容量はどれくらいか。といった値を示しているようです
available Estimation of how much memory is available for starting new applications, without swapping. Unlike the data provided by the cache or free fields, this field takes into account page cache and also that not all reclaimable memory slabs will be reclaimed due to items being in use (MemAvailable in /proc/meminfo, avail‐ able on kernels 3.14, emulated on kernels 2.6.27+, otherwise the same as free)  コード /proc/meminfoは以下のコードによって作成されるようです。</description>
    </item>
    
    <item>
      <title>CEDEC2015のCoverityクイズをどう直すか</title>
      <link>http://ota42y.com/blog/2015/08/29/cedec2015-coverity/</link>
      <pubDate>Sat, 29 Aug 2015 17:35:03 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/08/29/cedec2015-coverity/</guid>
      <description>CEDEC2015では、Coverity社のブースでバグのあるコードが掲示されていました。
バグが入ったC++コードらしい。#CEDEC2015 pic.twitter.com/ca9vb0emVT
&amp;mdash; alwei (@aizen76) 2015, 8月 26 
バグがあるかどうかはCoverityが検出してくれるとして、じゃあどう解決すれば良いのか？を考えました。
コードを見る限り、
 配列の添え字と同じ値を入れたい 処理は2つのクラスで共通化したい  という要求があると考えます。
(zに値を入れないが元コードもそうなのでよしとする)
また、簡単化のために与えられるデータは全て正しいと仮定します。
(実際は適切な箇所でエラーチェックが必要)
テンプレートでキャストを回避する 今回のバグは暗黙的キャストが行われ、かつキャストした状態でアドレス計算をするのが問題のため、
キャストされないように、テンプレートを使って両方のクラス用の関数を用意する事で回避する方法です。
#include &amp;lt;stdio.h&amp;gt; class base_class { public: base_class() { x = 0; y = 0; } public: int x ; int y; }; class derived_class : public base_class { public: derived_class() { z = 0; } public: int z; }; template &amp;lt;typename T&amp;gt; void calc_class_members(T b, int array_size) { for(int i = 0; i &amp;lt; array_size; i++) { b[i].</description>
    </item>
    
    <item>
      <title>2015年34週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-34/</link>
      <pubDate>Mon, 24 Aug 2015 07:34:15 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-34/</guid>
      <description>毎週まとめはおしまい そもそも自作のTODOアプリで、やったことを見返す習慣が付いたので、やったことを各週ごとにまとめるのはおしまい。
rsyncで特定フォルダをバックアップする rsync -a --delete origin/ backup
で、backupフォルダの中身をoriginと一緒になるように同期してくれるため、フォルダごとバックアップすることが出来ます。
rsync -a --delete origin backup
と、スラッシュを外すと、backupフォルダ内にoriginフォルダを作って同期します。</description>
    </item>
    
    <item>
      <title>構造体のサイズはsizeofをちゃんと使うべき</title>
      <link>http://ota42y.com/blog/2015/08/20/c-struct/</link>
      <pubDate>Thu, 20 Aug 2015 07:43:49 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/08/20/c-struct/</guid>
      <description>以下のコードはApple LLVM version 6.1.0 (clang-602.0.53)で8と12を出力します。
ですが、intは4byte、boolは1byteなのでStは5byte、StStは6byteしか使わないため、サイズが増えてしまっています。
#include &amp;quot;stdio.h&amp;quot; struct St{ int a; bool flag; }; struct StSt{ S s; bool f; }; int main(void) { printf(&amp;quot;%lu\n&amp;quot;, sizeof(St)); printf(&amp;quot;%lu\n&amp;quot;, sizeof(StSt)); return 0; }  原因 Cでは構造体の後ろにパディングを追加できるようになっています。
6.7.2.1 Structure and union specifiers
13 Within a structure object, the non-bit-field members and the units in which bit-fields reside have addresses that increase in the order in which they are declared. A pointer to a structure object, suitably converted, points to its initial member (or if that member is a bit-field, then to the unit in which it resides), and vice versa.</description>
    </item>
    
    <item>
      <title>2015年33週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-33/</link>
      <pubDate>Mon, 17 Aug 2015 22:08:50 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-33/</guid>
      <description> tmux-change-pane作った swap-paneは機能としてはとても便利なのですが、
ショートカットからの切り替え時に画面全てを切り替えてしまい、
とても使いづらかったので現在のパネル上で切り替える版を作りました。
tmux-change-pane
いずれちゃんと記事を書く予定です…
C++03だとmapのatが無い std::mapの[]は値が無い場合、その値を作ってしまうためconstではありません。
http://www.cplusplus.com/reference/map/map/operator[]/
そのため、以下のように配列内の要素を参照する場合、たとえ100％あることがわかっていても、constにすることは出来ません。
void printName(int unique_id, const std::map&amp;lt;int, std::string&amp;gt; &amp;amp;uniqueIdToName) { printf(&amp;quot;%s\n&amp;quot;, uniqueIdToName[unique_id].c_str()); }  そのため、以下のようにfindして要素を取り出す必要があります。
void printName(int unique_id, const std::map&amp;lt;int, std::string&amp;gt; &amp;amp;uniqueIdToName) { std::map&amp;lt;int, std::string&amp;gt;::const_iterator it = uniqueIdToName.find(unique_id); printf(&amp;quot;%s\n&amp;quot;, it-&amp;gt;second.c_str()); }  C++11からだとconstでも使えるatメソッドが用意されているため、これを利用することで、以下のようにかなりすっきりと書くことができます。
void printName(int unique_id, const std::map&amp;lt;int, std::string&amp;gt; &amp;amp;uniqueIdToName) { printf(&amp;quot;%s\n&amp;quot;, uniqueIdToName.at(unique_id).c_str()); }  </description>
    </item>
    
    <item>
      <title>2015年32週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-32/</link>
      <pubDate>Mon, 10 Aug 2015 21:50:30 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-32/</guid>
      <description>最近のHCI研究を調べてる とりあえずざっとアブストを見て良さそうなものをピックアップする作業。
いくつかは昔読んだ奴をもう一度読んでいたりします。
jamSheets http://tangible.media.mit.edu/project/jamsheets/
一枚のシートを、人が座れるぐらい堅くしたり、毛布みたいにまで柔らかくしたり、
自由自在に過多さを変えられるシートを作ったみたい。
どうなっているのかすごい気になる…
Inverse-Foley Animation http://www.cs.cornell.edu/projects/Sound/ifa/
事前に音と3Dモデルを学習しておき、入力された音にマッチする映像を作成する。
映像にマッチする音じゃなくて、音にマッチする映像を作るのがすごい…
afordance++ http://hpi.de/baudisch/projects/affordance.html
ものを持ったときや触れる前に、そのものの情報にあった動きを腕に提示することで、
触れずとも熱いものがわかったり、どう扱えば良いかがわかるようになる。
大体視覚と過去の経験に頼ってるアフォーダンスを大幅に拡張できて凄い。
3D Object Manipulation in a Single Photograph using Stock 3D Models http://www.cs.cmu.edu/~om3d/
一枚の画像から3Dオブジェクトを取り出し、写真と上手く合成してくれるっぽい。
光の加減とかも調節してくれるため、凄く上手く合成出来ていて凄かった。
FluxPaper: Reinventing Paper With Dynamic Actuation Powered by Magnetic Flux http://masaogata.com/projects/fluxpaper/
紙の裏に磁力を持つ物体を付け、磁界を制御することで紙の移動や検索などを可能にしたみたい。
紙に付ける方のパターンを工夫してるっぽい。
Scene Completion Using Millions of Photographs http://graphics.cs.cmu.edu/projects/scene-completion/
従来の写真の中の一部分を削除する技術に加え、
大量の画像の中から似た画像を探し出してくることで、
消した部分を正しく補完できるみたい。</description>
    </item>
    
    <item>
      <title>2015年31週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-31/</link>
      <pubDate>Mon, 03 Aug 2015 22:21:40 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-31/</guid>
      <description>エンジニアボドゲ会に参加しました 不思議な力によってエンジニアが多く集まる（エンジニア以外も集まる）ボードゲーム会に参加しました。
想像以上にボードゲームがあって凄かったです。
プレイしたゲームは
 王たちの同人誌  同人誌即売会をネタにしたゲーム カードの引きにかなり左右される イベントあるあるネタが多数  枯山水  徳を高めて庭を造る 中々思い通りにならなくて面白い  Splendor  拡大再生産を繰り返していく 素材を元に素材を生み出すカードを買うのでインフレしていく インフレさせすぎると他の人にゲームクリアされるので、バランス大事  エセ芸術家ニューヨークへ行く  お題の絵を一筆ずつ書いていく 一人だけいる、お題を知らない人（エセ芸術家）を見つける  ピクテル  出されたお題をピクトグラムで表現してあててもらうゲーム 限られたピクトグラムを組み合わせてお題を表現するのがとても面白い  init  変数の初期化を元にしたゲーム ルールがよくわからず、おかしい状態に陥った…  CV 履歴書  自分の履歴書を作っていくゲーム さいころ運が全てを左右する 勝敗もそうだけど、どんな履歴書になったかを見るのも面白い  羊と狼のピースフルワールド  カードを自分で見えないように掲げ、自分以外の番号から自分のの番号を推理するゲーム 頭の体操に良さそう 番号が書かれていないカードもある  お蜜柑様  指定された動きを取り入れた踊りをするゲーム 他のチームの動きが何かを推理して、それも踊りに入れる 1度やってみたいけど2回目以降はやってるのを見ていたいゲーム  お邪魔無視  お宝まで通路を延ばしていくゲーム チームに分かれてたり、お宝を取らせなかったら勝つ人がいたり お互い仲間なのか敵なのかわからずに進む感じが面白い  なつのたからもの  なつのたからものを集めるゲーム 金魚すくい、かき氷、花火大会、etc&amp;hellip; カードを集めて合計得点を競う 欲張りすぎると何も手に入らない等バランス大事 精神的ダメージが大きい あの頃のあの夏は2度と来ない…(´・_・`)  ディクシット  出されたお題に最も近いものを手札のカードから選ぶ かなり抽象的なカードばかりで、ぴったりのを考えるのが面白い   情報科学若手の会2015に参加します 今回はショート発表でHCIについて話す予定です。</description>
    </item>
    
    <item>
      <title>Hugoのブログテーマを作った</title>
      <link>http://ota42y.com/blog/2015/07/29/blog_theme/</link>
      <pubDate>Wed, 29 Jul 2015 23:25:20 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/07/29/blog_theme/</guid>
      <description>ブログテーマ更新を更新しました。
今回は、Bootstrapを元にしたHonokaを利用して作りました。
リポジトリはこちら
https://github.com/ota42y/honokichi
Bootstrapの作りにあったHTMLを出力するようにテンプレートを作ると、
ほぼそれっぽいデザインになったためとても楽でした。
一応、ブログでよく使う見出し間の間隔や、記事一覧画面でのマージンをちょっと変更しています。</description>
    </item>
    
    <item>
      <title>2015年30週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-30/</link>
      <pubDate>Mon, 27 Jul 2015 23:09:42 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-30/</guid>
      <description>ブログ修正中 Bootstrapを使って構築し直してます。
ついでにHugoをv0.14に上げたところ、いくつかdeprecatedになっていたので修正してます。
freeコマンドの出力が変わってる freeするとavailableなる見慣れない出力が出ます…
おそらく空きメモリを示しているようですが、ちょっと調べる必要がありそうです。
Evernoteの代わりを探し中 昨日ちょうどEvernoteプレミアムが切れてしまい、慌てて延長しました。
このとき、一時的に容量超過状態になったため、説明には「コンテンツのアップロードが出来ません」と出ていましたが、
ローカルのEvernoteクライアント上での編集や新規ノートの作成など、アップロード以外の作業が全て出来なくなっていました。
また、その際の挙動も編集できないのでは無く、従来通り編集可能ですが保存されないため、
別の操作をしたタイミングでデータが消滅するというかなり危険な挙動をしていました。
今回はたまたまオンラインに繋げる環境にいたので大事には至りませんでしたが、
もしオフライン環境だったらと思うと一大事です…
Evernoteはオフラインでもそこに保存しておけば次にオンラインになった時に同期されるという仕様のため、
オフライン環境で特によく使い、その上プレミアムを戻しても非プレミアム中にやった変更は消滅するため、かなり被害が大きいです。
というわけで、Evernoteは参照オンリーの情報だけ保存するようにし、
変更が必要なノートなどは別の方式で管理しようと思います。
元々ノートの容量増加に伴って、Evernoteクライアントの反応速度が低下しつつありましたし、 良い頃合いかと思います。
おそらくDropbox内に置いたファイルを上手く閲覧するような感じになると思います。
此方の場合は最悪でもローカルファイルになるだけなので、Evernoteのように変更不可能になることはないため、何かあった場合でも大丈夫です。</description>
    </item>
    
    <item>
      <title>2015年29週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-29/</link>
      <pubDate>Tue, 21 Jul 2015 17:56:49 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-29/</guid>
      <description>CPUの特権レベルについて調べた CPUモード
リングプロテクション
CPUには複数のモードが存在し、特定の機能やメモリ領域にアクセスできるかどうかがこのモードで制御されるらしいです。
通常、ユーザのプログラムはレベル３、カーネルがレベル０で動いており、
ユーザのプログラムはレベル０で動いています。
そのため、 レベル０用のリソースを使うためにはカーネルに処理をお願いする必要があるそうです。
goのビルドツールについて調べる go buildを実行することで、コンパイルとリンクをまとめて行うことが出来ます。
ですが、goではそれを別々に実行することも出来ます。
go tool 8gもしくはgo tool 6gを使うことで、goのソースコードからオブジェクトファイルを作成できます。
(どちらかは使用しているマシンのアーキテクチャによります)
生成されるオブジェクトファイルは.8もしくは.6の拡張子になります。
このファイルをgo tool 8lもしくはgo tool 6lを使ってリンクすることで、実行ファイルを作成できます。
実行ファイルはファイル名を指定しない場合、8.outもしくは6.outになります。
レーベンシュタイン距離計算を実装した https://gist.github.com/ota42y/29dc71841f4388957020
二つの文字列のうち、片方をどれくらい編集したらもう片方の文字列に出来るかを求めるやつです。
アルゴリズム的には凄い簡単ですね。</description>
    </item>
    
    <item>
      <title>2015年28週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-28/</link>
      <pubDate>Mon, 13 Jul 2015 23:25:27 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-28/</guid>
      <description>シェル環境を改善中 tmux plugin managerを導入しました。
https://github.com/tmux-plugins/tpm
tmuxのconfファイルでやる設定をある程度共通化し、 プラグインのインストールだけで使えるようにするものです。
公式でいくつか便利なものが提供されているので入れてみました。
プラグインは自分で作れるので、swap-paneもこれで管理したいな…と思います。
pecoの導入 入力からインクリメンタルに行を検索できるツールです。
goなのでバイナリ単体で簡単に動き、インストールが簡単なのが特徴です。
https://github.com/peco/peco
前にいたディレクトリをたどる設定や、 git statusから指定したファイルをinsertする設定をしました。
markdown-modeが便利 http://jblevins.org/projects/markdown-mode/
emacs上でmarkdownのハイライトやスニペット入力が出来る便利モードです。
個人的には見出しをorg-modeのように折りたたみが出来るので、
アウトラインエディタとしてものすごく便利です。
mecab-ipadicをいじっていた 形態素解析をいじってみたくて、igoとmecab-ipadicについて調べていました。
igoとは igoとはjava製の形態素解析です。 http://igo.osdn.jp/
javaとantがあれば直ぐに使えるため、インストールがとても楽です。
なお、辞書は同梱されていないため、mecabの辞書を取ってくる必要があります。
http://taku910.github.io/mecab/
辞書が組み込みではないので普通に使う分にはちょっと面倒ですが、 今回のように辞書をいじって動きを観察する用途にとても便利でした。
ipadicを変更してみる igoの動かし方はサイトの通りなので省略します。
ipadicから解析用のバイナリ辞書を生成する手順がありますが、
このときipadicのフォルダに辞書ファイルを追加する事で、新たな辞書を追加する事が出来ます。
また、既存のファイルを削除することで単語の登録をさせないことが出来るため、
自分が作った辞書だけにすることで、正しく使えるかどうかを調べることが出来ます。
なお、バイナリ辞書作成時はフォルダの中のcsvを全て読み込むらしく、
追加時に必要な設定などは無いようです。</description>
    </item>
    
    <item>
      <title>2015年27週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-27/</link>
      <pubDate>Mon, 06 Jul 2015 22:42:53 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-27/</guid>
      <description> tmux環境を整えた 画像のように、tmux内でemacsを開きつつ、zshを表示するようにしました。
ただペインを分けているだけでなく、ペインの中身はウィンドウなので、
ウィンドウを作って入れ換えたり、左右の中身を入れ換えたり出来ます。
具体的にはこんな感じで、分割を維持したまま各シェルを入れ換え可能です。 入れ替えはswap-paneを利用しています。
https://github.com/abicky/swap-pane
gemのAnnotateが便利 https://github.com/ctran/annotate_models
DBからSchemaを読み取り、対応するモデルが書かれたファイルにその情報を書き込むgemです。
このモデルにどんなカラムがあったっけ？って時に、schemeファイルから探し出す必要がなくなるので大変便利です。
また、情報はコメントに追加されるため、既存のコードの邪魔になったり勝手にアクセサが追加されたりしないのも便利です。
emacsのパッケージ管理 現在私のemacsはpackage.el、el-get、init-loaderの3つを使ってパッケージ管理をしています。
package.el、el-getがパッケージのDLや読み込み、init-loaderが設定ファイルのロードをしています。
具体的な使い方はググれば解るので省略するとして、使い分けとこの構成になった理由は以下の通りです。
なお、以下のリポジトリに設定ファイルが置いてあります。 https://github.com/ota42y/dotfiles
パッケージ本体の管理 まず、package.elはMELPA等専用なため、そこにあるパッケージはこちらで管理します。
一方で、githubや個別のgitリポジトリで管理されているものはel-getでrecipeを書いて管理しています。
機能的にはel-getだけで完結しますが、package.elの方が楽なので基本はpackage.el、
そこでカバーできないものをel-getで管理させています。
また、これらで管理されているものは自動でロードされるようになっているため、
設定ファイルに追加するだけでインストールが完了します。
パッケージ設定の管理 init-loderを使うことで、特定フォルダ内の設定ファイルを一気に読み込むことが出来ます。
このため、気軽に設定ファイルを分割できます。
ファイル名に数字を入れる事で読み込む順番の設定も出来るため、
package.elがダウンロードし、読み込みが終了してからパッケージの設定ファイルを読み込むように設定しています。
また、私の環境では、el-getでinit-loaderを管理しているため、
el-get&amp;gt;init-loder&amp;gt;package.el&amp;gt;その他の設定ファイルの順で実行をしています。
el-getも自動インストールするようにしているため、設定ファイルを持って行くだけで全て自動でインストールできます。
まとめ  recipeに追加（必要なら） 使用するパッケージを追加する  package.elの場合 init-loderから読まれるpackage.el用の設定ファイル 無ければ自動で読み込む el-getの場合 emacs.dのinit.elに名前を書く recipeを元にDLする 自動で読み込みも行われる  設定をconf下に書く  init-loderが読み込む カスタマイズが必要ならば   </description>
    </item>
    
    <item>
      <title>dinoでrubyからArduinoを扱う</title>
      <link>http://ota42y.com/blog/2015/07/05/dino/</link>
      <pubDate>Sun, 05 Jul 2015 14:15:20 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/07/05/dino/</guid>
      <description>まとめ  PCからArduinoを制御するのは面倒  作業量が多い 電子工作 Arduinoプログラミング PCからArduinoを制御するプログラミング Arduinoプログラミングは専用言語 学習コストがある  dinoならrubyだけで全て完結できる  rubyからArduinoの全機能を制御するプログラムが付随 rubyのオブジェクト操作でArduinoを制御出来る   PCからArduinoを制御するのは面倒 Arduinoのおかげで、電子部品をプログラムから制御する際の敷居は大幅に下がりました。
ですが、Arduinoを制御するためにC/C++をベースとした専用の言語を覚える必要があります。
PCとからArduinoを制御する場合、
 電子工作をしてArduinoと部品をつなげる Arduinoを制御したり、PCに情報を送るプログラムを書く PC側でArduinoからの情報を受け取って制御するプログラムを書く  の3種類の別々の作業が必要になります。
Arduino抜きでやるよりかは簡単になりましたが、これもまだまだ面倒です。
ここで、rubyのdinoというgemを使うと、Arduino本体のプログラミングを省略し、
rubyプログラムを書くだけでArduinoの制御が全てできるようになります。
これにより、PC側のプログラムを書くだけでArduinoを制御出来ます。
dino Arduinoをrubyから扱うライブラリです。
https://github.com/austinbv/dino
Arduinoの全機能を外部から制御可能にするプログラムが付随しており、
これを書き込むことで、arduinoをruby上のオブジェクトとして扱うことができます。
サンプルコード A0ポートに対する入力を拾うプログラムは以下の通りです。
board = Dino::Board.new(Dino::TxRx::Serial.new) sensor = Dino::Components::Sensor.new(pin: &#39;A0&#39;, board: board) sensor.when_data_received do |data| puts &amp;quot;data=#{data.to_i}&amp;quot; end sleep  ポートに対する入力があるたびにブロックが実行されます。
このように、arduinoをほぼrubyのオブジェクトのように扱えるため、
PC上でrubyプログラムを書くだけで、電子部品を制御することができるようになります。
ただし、当然ながらPCと接続して制御するものであり、
Arduino単体で動作させたい場合は今まで通りArduinoプログラムを書く必要があります。</description>
    </item>
    
    <item>
      <title>2015年26週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-26/</link>
      <pubDate>Mon, 29 Jun 2015 22:17:29 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-26/</guid>
      <description>fluentd-forwarderのビルドの仕組み 基本はREADMEの通りにすればビルド出来ますが、やっていることがちょっと複雑なのでメモ。
https://github.com/fluent/fluentd-forwarder
READMEに書いてある一つ目の手順の、
go get github.com/fluent/fluentd-forwarder/entrypoints/build_fluentd_forwarder
で、GOPATH以下にリポジトリがcloneし、
entrypoints/build_fluentd_forwarder内のビルド作業用バイナリが作られます。
次に、bin/build_fluentd_forwarder fluentd_forwarderを実行することで、
さっき作ったbuild_fluentd_forwarderがgo get -uを実行し、
リポジトリをcloneして(既にしているけど)、entrypoints/fluentd_forwarder下をビルドします。
というわけで、
 go getでbuild_fluentd_forwarderをビルドする build_fluentd_forwarderがgo getでfluentd_forwarderをビルドする。  という流れになっています。
fluentd-forwarderに手を加えてみる ローカルで変更する場合 fluentd-forwarderに手を加えたい場合、GOPATH下のfluent/fluentd-forwarderを書き換えてビルドするのが最も早い方法です。
build_fluentd_forwarderがcloneしますが、変更はそのままでビルド出来ます。
Forkしたリポジトリに変更を加える場合 Forkしてそこに対して変更をした場合、若干やっかいな事が起きます。
前述したように、build_fluentd_forwarderはgo getを使い、リポジトリをクローンしてビルドします。
そのため、build_fluentd_forwarderの中に書いてあるリポジトリを書き換え、自分のリポジトリに変更する必要があります。
また、entrypoints/fluentd_forwarderはリポジトリルートのファイルを外部ライブラリとしてインポートしています。
そのため、main.goでfluent/fluentd_folwarderをインポートしている部分も、自分のリポジトリに変更する必要があります。
以上の変更を加えることで、Forkした自分のリポジトリからビルドすることが出来ます。
まとめると以下のの2ステップになります。
 build_fluentd_forwarder内のリポジトリを書き換える  https://github.com/fluent/fluentd-forwarder/blob/master/entrypoints/build_fluentd_forwarder/main.go ImportPathBaseを書き換える  fluent/fluentd_forwarderをインポートしている部分を書き換える  entrypoints/fluentd_forwarder以下の2ファイル共   前者はbueild_fluentd_forwarderを変えれば何とかなりそうですが、
後者はgoのimportの仕組みがリポジトリ名まで指定する仕組みのため、書き換える以外に良い解決方法が思いつきません。</description>
    </item>
    
    <item>
      <title>2015年25週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-25/</link>
      <pubDate>Mon, 22 Jun 2015 22:09:20 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-25/</guid>
      <description>go conに参加していた 去年の秋と比べて、実際に使っている例が多くて、
もうプロダクション投入されてるなーという印象でした。 参加記録 Go Conference 2015 summer
atom.ioは辞めた Atom.ioの現状の仕様は、プロジェクトごとにウィンドウを割り当てて使うようになっています。
ですが、MacではWindowsに比べて同じアプリ内のウィンドウを切り替えるのがとても大変であり、
このようにウィンドウを細かく分けられると著しく不便です。
Macのひどい仕様に負けるのはあまり良い気分ではないですが、
このあたりの仕様が変わるまでは私の使い方では使い物になりません…
また、立ち上がりが重いのも大きな問題の1つです。
ビルド中はCPUをものすごく使っているおり、その状態でAtom.ioを立ち上げると起動まで十数秒かかります。
ただこの点に関しては、ネィティブではなくwebエンジニアであれば問題にならないとは思います。
gosweepが便利そう https://github.com/hailiang/gosweep
gofmtやgolint等のgoの便利ツールを全て実行してくれる奴です。
便利＆go vetやgoimports等知らないのもあったので有用そうです。</description>
    </item>
    
    <item>
      <title>参加記録 Go Conference 2015 summer</title>
      <link>http://ota42y.com/blog/2015/06/21/gocon-2015-summer/</link>
      <pubDate>Sun, 21 Jun 2015 23:43:02 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/06/21/gocon-2015-summer/</guid>
      <description>今回は日経さんとCAさんがスポンサーとのことですが、
かなりの人がいるのにwifiが普通に繋がっていて凄かったです…
なんか機械も見たことも無い凄い奴でしたし。
実際に運用してる上での話が多く、goの時代が来ている…！
という感じを受けました。
お仕事でほぼ使ってないのでちゃんと使いたいですねー
また、みんなcode generateに走っているのはおもしろかったです。
コピペになるようなコードはgenerateするってのはほぼ統一見解ですね。
キーノート  go1.5凄そうです go oracleのすごさ  関数がどこで使われているかが調べられる 関数ポインタで呼ぶ場合も大丈夫  gomobile、早くiOSに対応してC++を駆逐してほしいです… 公式の依存管理ツールも楽しみです  発表まとめ 発表順は覚えてないので、タイムテーブルに順不同で並べられている順です。
Gaurun〜A general push notification server in Go〜  https://speakerdeck.com/cubicdaiya/a-general-push-notification-server-in-go go製のpushサーバ モニタリング用のAPIを用意してるの、凄い良いなーと思いました。  Debugging Go Code with GDB  http://kaneshin.hateblo.jp/entry/2015/06/21/202850 GDB、あんまり使いこなしてないのでもっとちゃんと使わないと…と思った。 Goはコンパイル早いし、落ちたときにちゃんとどの行で落ちたか出してくれるし、無くても何とかなる感はあります  使った方が何倍も楽なのでちゃんと使った方が良いかと  go 1.5からコンパイラがgoで実装されるけど、吐き出されるバイナリにDWARFが入ってれば関係ないよね？  リリース直後はバグったDWARFになる可能性はありそう…  フォントが$300ってwwwwww  Generative programming in Go.  https://speakerdeck.com/monochromegane/generative-programming-in-go reflection凄く遅いです…(直接呼ぶのと1000倍違う) 金槌を持つと何でも釘に見える病…  gore - Go で REPL のはなし  irbみたいにgoを対話的に実行できるやつの内部事情 go runしてるだけ go runに失敗しないように色々前処理をがんばってるみたいです。  Goと電子工作とロボット  goでロボット制御できて、しかもスタンドアロンで動いていて凄いです  GoのASTをいじくって新しいツールを作る  http://www.</description>
    </item>
    
    <item>
      <title>C言語でのスレッド処理</title>
      <link>http://ota42y.com/blog/2015/06/18/c-thread/</link>
      <pubDate>Thu, 18 Jun 2015 07:47:43 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/06/18/c-thread/</guid>
      <description>C言語でのスレッド処理と、ロックの仕方をまとめました。
他の言語のようにスレッド用のクラスを継承するのでは無く、
別スレッドで実行する関数のポインタと、
その関数に渡すデータのポインタを指定して実行するようです。
スレッドによる並行処理 スレッドの作成(pthread_create) Cではpthread_createを利用することで、別スレッドで任意の関数を実行できます。
int pthread_create(pthread_t * thread, pthread_attr_t * attr, void * (*start_routine)(void *), void * arg);   thread  スレッド管理用のpthread_t型の変数  attr  スレッドの属性を指定する。 NULLの場合はデフォルトが使われる  (*start_routine)(void *)  別スレッドから呼び出される関数へのポインタ  arg  start_routineの引数として渡すデータのポインタ 元のスレッドからデータを送るのに使う   スレッドの終了を待つ(pthread_join) pthread_joinで、指定したスレッドが終了するまで待機することができます。
int pthread_join(pthread_t th, void **thread_return);   th  待機するスレッドをpthread_t型の変数で指定する  **thread_return  スレッドの戻り値を格納する領域   サンプルコード 以下の例はグローバルな値にメインとサブの2つのスレッドから加算処理を行っています。
排他制御をしていないため、スレッドによる並行処理が行われると、値がおかしくなる可能性があります。
実際、何度か実行すると値がおかしくなり、並行処理が行われていることが確認できます。
なお、コンパイルする際はは-pthreadオプションを指定する必要があります。
#include &amp;quot;stdio.h&amp;quot; #include &amp;quot;pthread.</description>
    </item>
    
    <item>
      <title>2015年24週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-24/</link>
      <pubDate>Mon, 15 Jun 2015 22:37:01 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-24/</guid>
      <description> C++入出力の書式を制御する c++でstreamを使って出力する場合、iomanipに用意されているものを利用することで、
入出力の書式を制御することが出来ます。
設定はstreamに対して設定用のオブジェクトを送るように設定するみたいです。
なお、以下は出力の例です。
#include &amp;lt;iostream&amp;gt; #include &amp;lt;iomanip&amp;gt; int main(void) { double a = 25252; int p = 2; std::cout &amp;lt;&amp;lt; a &amp;lt;&amp;lt; std::endl; // 25252 std::cout &amp;lt;&amp;lt; std::setprecision(p); //浮動小数点の有効桁数を指定する std::cout &amp;lt;&amp;lt; a &amp;lt;&amp;lt; std::endl; // 2.5e+04 std::cout &amp;lt;&amp;lt; std::oct; // 8進数表示 std::cout &amp;lt;&amp;lt; 874 &amp;lt;&amp;lt; std::endl; // 1552 std::cout &amp;lt;&amp;lt; std::showbase; // 基数を表示する std::cout &amp;lt;&amp;lt; std::hex; // 16進数表示 std::cout &amp;lt;&amp;lt; 874 &amp;lt;&amp;lt; std::endl; // 0x36a return 0; }  </description>
    </item>
    
    <item>
      <title>2015年23週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-23/</link>
      <pubDate>Mon, 08 Jun 2015 23:41:38 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-23/</guid>
      <description>Lock Freeって何 複数スレッドや複数コアから同じデータを更新する場合、
通常はlockをかけて排他制御を行いますが、lockをかけずに行う方法があるらしいです。
lockをかける場合、その間そのデータにはアクセスできず、並列処理を止めてしまうため、
並列数が大きくなるに従って性能が劣化していくそうです。
lockなしで排他処理を行う場合、並列数を増やしても性能の劣化が無くなるそうです。
やっていることとしては、値をコピーして変更を加え、
「コピー前の値と現在の値が一緒かどうかをチェック」「変更した値を書き込む」を一括にやる命令を利用し、
比較と更新を同時にするというものだそうです。
(Compare-and-Swap、CAS命令)
この比較して更新する命令があることで、
ロックせずに複数から値を変更しても一貫性が保たれるらしいです。
また、この命令が無いとlock freeが実現できない事が証明されているらしいです。
コンペア・アンド・スワップ
i386ではCMPXCHG〜命令でこれが出来るらしく、
goのatomicパッケージでも利用されているようです。
https://github.com/golang/go/blob/master/src/sync/atomic/asm_386.s#L34</description>
    </item>
    
    <item>
      <title>Linuxのlocaleがおかしくなっていた</title>
      <link>http://ota42y.com/blog/2015/06/03/linux_local/</link>
      <pubDate>Wed, 03 Jun 2015 07:39:36 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/06/03/linux_local/</guid>
      <description>突然プログラムが起動しなくなったため、
原因を探ったところ、localesが壊れていました。
そのため、再インストールすることで直りました。
言語取得部分は動くけど、おかしい結果を返す壊れ方のため、
発見にわりと手間取りました。
まとめ  プログラム上でlocaleを参照する部分がおかしい結果を返す  常にANSI_X3.4-1968 Debian 7.8  dpkg-reconfigure localesが何か壊れているメッセージを出す  localeをアップデートすると直る apt-get install locales   localeの取得がおかしい pythonではgetpreferredencoding()で設定されている言語情報を取ってこれます。 ですが、
LANG=&#39;ja_JP.UTF-8&#39; echo &#39;import locale; print locale.getpreferredencoding()&#39; | python
を実行しても、ANSI_X3.4-1968が返ってきてしまい、日本語処理の部分でおかしくなっていました。
(前にそのプログラムは動いていたので、気がついたらおかしくなっていました)
localeを実行してみたところ、以下のように表示されました。
locale -a locale: Cannot set LC_CTYPE to default locale: No such file or directory locale: Cannot set LC_MESSAGES to default locale: No such file or directory locale: Cannot set LC_COLLATE to default locale: No such file or directory C C.</description>
    </item>
    
    <item>
      <title>2015年22週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-22/</link>
      <pubDate>Mon, 01 Jun 2015 22:00:39 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-22/</guid>
      <description>今週まとめ
Cでのマルチスレッド処理を調べてる Cにはクラスが無いので、Threadクラスを継承し、
runメソッドをオーバーライドするといった、いろんな言語でよくあるパターンは使えません。
代わりに、pthread_create関数にコールバック関数を渡すと、
別スレッドから呼び出してくれるため、それを利用して別スレッドを作るようです。
Cでのロック処理を調べている ロック用変数を作り、pthread_mutex_lock関数を呼ぶことで、ロックをかけられます。
他のスレッドから起こされるまで待つには、
mutexとcond変数を定義して、mutexのロックを取得して、
pthread_cond_waitを呼びます。
一定時間たったら呼ばれて無くても起きて欲しい場合は、
pthread_cond_timedwaitを呼びます。
なお、C++11からmutexオブジェクトといった、
ロックや排他制御が簡単にできる仕組みが導入されています。
C++ではintに[]演算が出来る 元ネタ
http://twitter.com/mattn_jp/status/605245747429179392
#include &amp;lt;stdio.h&amp;gt; #include &amp;lt;stdlib.h&amp;gt; #include &amp;lt;memory.h&amp;gt; int main() { int v[] = {1, 2, 3}; int off = 2; printf(&amp;quot;%d\n&amp;quot;, off[v]); // 3 printf(&amp;quot;%d\n&amp;quot;, 1[v]); // 2 return 0; }  C++では、intやenum型に対して[]が実行された場合、暗黙的な方変換によりstd::ptrdiff_tに変換されます。
そのため、a[b]は*(a+b)と同義になり、上の例は*(off+v)や*(1+v)と解釈され、 正しく動くようです。
http://en.cppreference.com/w/cpp/language/operator_member_access</description>
    </item>
    
    <item>
      <title>2015年21週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-21/</link>
      <pubDate>Mon, 25 May 2015 22:09:32 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-21/</guid>
      <description> 体調を崩した 体調を崩して週の半分は倒れてた。
3日倒れるとやることリストが大変なことになるので、
体調がおかしい場合は速やかに医者に行って、薬をもらうべきだと再認識した。
C++03以前ではオーバーロードされたコンストラクタが呼び出せない 以下のように、コンストラクタから別のコンストラクタを呼び出すことで、
処理を共通化するのはわりと自然ではないかと思います。
これはdelegate constructorとうい名前がついており、
残念ながらC++11以降じゃないと使えません。
struct A { A() : A(0) {} A(int num) : a(num) {} int a; };  </description>
    </item>
    
    <item>
      <title>2015年20週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-20/</link>
      <pubDate>Mon, 18 May 2015 21:40:28 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-20/</guid>
      <description>一週間何もしていないことが可視化される 毎週先週やったことをまとめているので、
先週何もしていないのが可視化される…
PRマージされた fluentd-forwarderに送っていたPRがマージされた。
https://github.com/fluent/fluentd-forwarder/pull/10
が、テストのバグでテストが通らないとの指摘が。
ただし、手元のMacやLinuxでは通るので、Windowsだけの問題っぽいです。</description>
    </item>
    
    <item>
      <title>2015年19週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-19/</link>
      <pubDate>Mon, 11 May 2015 21:55:04 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-19/</guid>
      <description> atomicな処理でアライメントを揃える必要があるらしい goのatomic.AddInt64や、
WindowsのInterlockedExchangeでは、
32bit環境で64bitの値をアトミックに変更する場合、
その値が64bit境界に置かれている必要があるらしいです。
ですが、何故それが必要なのかを調べているのですが、それらしい理由が見つかりません…
一応、Intelのアーキテクチャには、ロックをせずに変更する場合、
64bitアライメントに乗っていない場合はアトミックな処理が保証されないと書いてありましたが、
goではロックしてからCMPXCHG8Bを実行しており、アトミックな処理になるはずです。
また、ロックやCMPXCHG8Bもメモリアライメントの制約は受けないため、特に問題は無さそうです。
一応、64bit境界に無い場合は性能が低下する可能性があるそうですが…
Android 触ってた  string.xmlは同じフォルダに別の名前のxmlを作っても、勝手にまとめてくれるので同じようにアクセスできるみたい。 Dropbox SDKを触ってみた  サンプルがかなり良い具合に出来ている  UIを変更できるのは描画用のスレッドだけ  doInBackgroundは別スレッドで処理を行う UIを変更しようとすると落ちる 終了時にonPostExecuteがUIスレッドで呼ばれるのでそこで処理する   fluentdを触ってる  自分用のアプリのログ処理を入れ換えた windowsとlinuxそれぞれあるけど、fluentd-forwarderを使ってlinuxのfluentdに集約 windowsはrubyで書いてる  ruby2.1じゃないと動かない  http://qiita.com/okahashi117/items/a0b55ea24a6ef7b6582b msgpackは未対応 オプションが渡せないため、bundlerも使えない  SSL接続に失敗する  https://gist.github.com/luislavena/f064211759ee0f806c88 ここのpemファイルの位置をSSL_CERT_FILE環境変数に設定する必要がある    fluentd-forwarder にPR送った  atomic処理のメモリアライメント的な問題でwindows 32bitだと動かなかったので直した  https://github.com/fluent/fluentd-forwarder/pull/10 まだマージされてないので、手元のブランチでビルドしたものを使ってる  goのパッケージシステムは手元で動かすのとものすごく相性が悪い  全てのパッケージバスを自分のリポジトリに書き換えないといけない しかもmasterブランチ限定   </description>
    </item>
    
    <item>
      <title>Go言語でメモリ上の大きさや配置を調べる</title>
      <link>http://ota42y.com/blog/2015/05/06/go-struct-offset/</link>
      <pubDate>Wed, 06 May 2015 20:16:57 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/05/06/go-struct-offset/</guid>
      <description>golangで構造体を定義した場合、 メモリ上にどのように配置されるのでしょうか。
通常意識する必要はありませんが、32bitと64bitで挙動がおかしい場合など、
ごく希に調べる必要に迫られる場合があります。
そのような場合、各要素のサイズや、構造体先頭からのオフセットを調べることで、
メモリ上に構造体がどう置かれるかを調べることができます。
C言語でのsizeofやoffsetofに対応する物が、golangのunsafeパッケージに用意されているため、
これを利用することで構造体の様子を調べることができます。
https://golang.org/pkg/unsafe/
今回は以下のようなテスト構造体を使い、メモリ上にどのように置かれるかを調べました。
テスト環境はwindows7(32bit)とmac(64bit)になります。
type A struct { flag bool num int64 ptr *int64 mini int32 str string nums []int64 nums5 [5]int64 strs []string }  要素のサイズ unsafe.Sizeof関数は、引数の要素のサイズを調べ、バイト数をint型で返してくれます。
a := A{} log.Println(unsafe.Sizeof(a)) // 92 (136) 括弧外は32bit環境、括弧内は64bit log.Println(unsafe.Sizeof(a.flag)) // 1 log.Println(unsafe.Sizeof(a.num)) // 8 log.Println(unsafe.Sizeof(a.ptr)) // 4 (8) log.Println(unsafe.Sizeof(a.mini)) // 4 log.Println(unsafe.Sizeof(a.str)) // 8 (16) log.Println(unsafe.Sizeof(a.nums)) // 12 (24) log.Println(unsafe.Sizeof(a.nums5)) // 40 log.Println(unsafe.Sizeof(a.strs)) // 12 (24) a.</description>
    </item>
    
    <item>
      <title>2015年18週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-18/</link>
      <pubDate>Mon, 04 May 2015 11:27:52 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-18/</guid>
      <description>goのテストはクラス設計がとても大事っぽい まだこうした方が良いんじゃないか？ぐらいなので、
ベストプラクティスは他にありそうです…
rubyで関数の差し替え Rubyだと以下のように、
すでに存在するクラスに対して関数を再定義したり、
継承して関数の差し替えをすることが簡単にできます。
class A def exec f() a() end def f print &amp;quot;f &amp;quot; end def a print &amp;quot;a\n&amp;quot; end end a = A.new a.exec # f a class A def f print &amp;quot;newf &amp;quot; end end a.exec #newf a class B &amp;lt; A def f print &amp;quot;newf &amp;quot; end end b = B.new b.exec #newf a  そのため、一部の関数をモックやスタブにし、正しく呼び出されているかを検証したり、
その機能以外のところがちゃんと動いているかを検証するのが簡単にできます。
goで関数の差し替え 埋め込みによる差し替え（できない） golangでは既存の構造体に関数を再定義することはできません。
また、以下のrubyコードのように埋め込んだ後に関数を再定義しても、</description>
    </item>
    
    <item>
      <title>参加記録 BPStudy#92</title>
      <link>http://ota42y.com/blog/2015/04/29/bpstudy92/</link>
      <pubDate>Wed, 29 Apr 2015 13:49:43 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/04/29/bpstudy92/</guid>
      <description>BPStudy#92に参加しました。
今回は経営やお金の話でした。
お金って資本主義社会のベースとなる概念ですし、
インターネットの仕組みがどうなってるのかとか、
コンピュータがプログラムをどう解釈しているのかと同じぐらい、
お金の事を知っておいた方がいい気がする…と思って参加しました。
エンジニアの経営学  http://www.slideshare.net/bejita/bpstudy92  http://www.slideshare.net/bejita/ss-45051035 一部詳しく書いている別資料  会社は環境の変数にめちゃくちゃ弱い 2014年の倒産件数  9731件 一日あたり26.6社が倒産している これでも前年比10%減  利益は車におけるガソリン  ガソリンが無いと走れない 補給は常に必要 ガソリンの供給源は売り上げのみ  利益＝売り上げーコスト  利益に対するコストの比率が低い、低コストの方が良い コストが低い方が安定する  @ota42y 必要な資金が少なければ少ないほど内部留保が溜まりやすく、変化に対応できるバッファが産まれるからです。ガソリン切れを起こすのが一番やってはいけないことなので、残るものが一緒なら使うカネが少ないほうが安定します。
&amp;mdash; やきう大好きござ先輩 (@gothedistance) 2015, 4月 28  詳しくは二つ目の資料   全ての企業活動はコスト  資料作成、メールを打つ、問い合わせ対応 それ自体は１円も生んでいない。 そのコストを回収するのは売り上げしか無い。 サービス運営で食べていくのは大変 オーダーメイドを作る方が簡単  お金も出してくれやすい  サービス開発は必ず投資が先行する 月額定額は青天井なので嫌われる  組織運営  一人で全ては出来ない 誰かにやってもらわないと組織運営は安定しない 高いスキルを持った人を集めても、ゴミしか生み出せない場合もある Good Player is not Good Manager  チームの将来は決定の速度で決まる  引き延ばしは金と時間の無駄 決定が遅いと間違いに気づくのも遅い 決定の妥当性はいま判断できない Done is Better Than Perfect.</description>
    </item>
    
    <item>
      <title>2015年17週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-17/</link>
      <pubDate>Mon, 27 Apr 2015 11:58:59 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-17/</guid>
      <description>cpplintが便利そう AtomLinter/linter-cpplint
Google Style Guideに沿っているかをチェックしてくれるcpplintを、
Atom.ioから使えるようにしているPluginです。
めっちゃ便利なのですが、標準だとcpplintのフィルターしか設定できず、
他のオプションを設定できないため、PRを送りました。
https://github.com/AtomLinter/linter-cpplint/pull/12
レビュー待ちです(o゜▽゜)
Apple Watch買った たまたま発売日に秋葉原を歩いていたら売っていたのでつい買ってしまいました。
が、iPhoneをメインで使っているわけではないので、いまいち使い道が良くわからないです。
しばらくは見せるようですかね…
fluentd-forwarderを触ってた fluent/fluentd-forwarder
Windowsのサーバで動かしているやつのログをとってくるの面倒だなー
と思っていたので、便利そうです(o゜▽゜)
とりあえず動かし方はわかったので、実際に入れてみようかと…</description>
    </item>
    
    <item>
      <title>参加記録 DroidKaigi</title>
      <link>http://ota42y.com/blog/2015/04/26/droidkaigi/</link>
      <pubDate>Sun, 26 Apr 2015 12:49:12 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/04/26/droidkaigi/</guid>
      <description>DroidKaigiに参加してきたのでメモ。
Twitterで流れてくる情報によると、あっちのセッションもおもしろそう…
みたいなのがあったので、ちゃんとセッションの概要も読んで天秤にかけるべきでした。
（エクセルシートから該当のセッションを探すのが大変だったので挫折した）
 Activity, Fragment, CustomView の使い分け - マッチョなActivityにさよならする方法 -
 http://www.slideshare.net/yanzm/droid-kaigi2015-yanzm ActivityはFragmentのブセット的な印象。 Fat Activityになりやすい場合はViewとか他の部分にコードを分割しましょうというお話っぽい。  RailsのFat Controller問題?  Fragmentにできる事はそっちにどんどん任せて、ActivityはActivityにしか出来ない部分に集中しようってこと？ Activityにしか出来ないことが多すぎるからFat Activity問題になってたので、 似たようなことが出来るけどライフサイクルをコントロールできるクラスの登場によって、 処理の分割ができるようになった感じ？  開発を効率的に進めるられるまでの道程
 http://www.slideshare.net/TakaoSumitomo/ss-47393000 コードを書くのとテストを書くのが一番開発で大変なところ どんな風にテストしているかの事例紹介  ダミーを作ってがんばる感じ 複数の間を取り持つクラスは疎通確認のみ   あるゲームアプリケーションの構成とアップデートサイクル
 http://www.slideshare.net/kentaroiizuka/droidkaigi 様々なリソースから、バイナリやアップデータを作るまでをすべてJenkinsがやっている  専用のパイプラインエンジニアがいるらしい  Playgroundつよい  Cocosはお察しなできなので… (´・ω・｀)  Luaでゲーム部分を書く  ゲーム部分以外はフレームワークが吸収  リソースの追加ダウンロードとかもフレームワーク側が持っているらしい  ゲーム側はゲームとしての実装に専念できるっぽい
   Android学ぶを君へ。生き抜くためのナレッジ共有
 https://github.com/operando/DroidKaigi 有用な情報がたくさん adbが思っていた以上に優秀だった  任意のアプリのViewTreeが見られるらしい  テスト書こう 発表と関連する資料が全部まとまっているの凄い便利  Kotlin</description>
    </item>
    
    <item>
      <title>LinuxとIntelliJを使ったWindowsでのプログラミング環境</title>
      <link>http://ota42y.com/blog/2015/04/24/go-windows-development/</link>
      <pubDate>Fri, 24 Apr 2015 10:00:26 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/04/24/go-windows-development/</guid>
      <description>取り扱っているのはGolangですが、特にプログラミング言語は問わないと思います。
 WindowsのGolang開発  IntelliJ便利  機能がとても豊富  Windows上ではつらい  開発ツールが少ない LinuxはGUI使いづらい  LinuxのファイルをWindowsから変更する  良いところ取り  開発はWindows 実行はLinux  ファイルはSambaで共有  ネットワークドライブをマウント  IDEデバッグは使えなくなる  そのときだけWindowsで動かすとか     Golang開発環境を整える IntelliJでのGo開発環境 IntelliJ IDEAとGolang pluginはとてもよく動くため、
Golang開発に関してはこれを使うのが最も簡単に快適な開発環境を整えられます。
Go の開発環境は IntelliJ IDEA + golang plugin がマトモだった
VMwareに開発環境を整える IntelliJもGolangはWindowsに対応しているため、比較的簡単に開発を行うことができます。
ですが、開発に便利なツールの多くはUnixの方が使いやすいことが多いため、
Windows上で開発するのは細かいところで面倒になることが多いです。
そのため、仮想マシンや別サーバにLinuxマシンを1台作り、
その中で開発をした方が何かと便利です。
ですが、LinuxのGUI環境は現状まともな環境がなく、とても使いづらいため、
開発以外の作業が発生する場合を考慮すると普段はできる限りWindowsを使用したくなります。
そこで、開発はWindows上のIntelliJ等で行い、実行環境や開発ツールはLinux上に整え、
それらをssh経由のCLIから操作するのが最も良い案になっています。
このような構成にすることで、Unixで動く便利なツールを利用しつつ、
Windowsの快適なGUI環境を利用することができます。
また、ファイルや実行環境と手元の環境とが切り離されるため、
複数の実行環境を切り替えたり、
マシンを入れ替える際に再設定する量を減らすことができるという利点もあります。
なお、私は手元のマシンのVMware上にLinuxを立てているため、転送速度はほぼ気になりません。
IntelliJで別サーバのファイルにアクセスする 残念ながらIntelliJはこのような用途を想定していないため、 別マシンの環境下で作業できません。
幸いなことに、Windowsがネットワークドライブとして別マシンのフォルダをマウントした場合、
IntelliJからは普通のドライブとして見えるため、別マシンのファイルにアクセスすることができます。
そこで、サーバ上の開発ディレクトリをSambaで共有し、
Windowsからネットワークドライブとしてそのフォルダをマウントしてあげることで、
IntelliJで開発を行うことができます。</description>
    </item>
    
    <item>
      <title>2015年16週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-16/</link>
      <pubDate>Mon, 20 Apr 2015 07:36:53 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-16/</guid>
      <description> goの開発環境充実中  go fmt  goのフォーマットをぱぱっと修正してくれます。  golint  動作に影響ないが推奨されないような部分を指摘し、何をすべきかを教えてくれます。 goのコーディング規約に沿っていないチェックが主です IntelliJから使えれば最高なんですが…   また、goの公式ではないですが、以下のテスティングツールが便利そうです。
 goconvey  テストの自動実行 go testの結果をブラウザで見やすくしてくれる 標準のテストを拡張したテストライブラリとしての機能もある  IntelliJ IDEA  go pluginを入れると凄い便利 Windowsの場合はVM上のLinuxに立てて、ネットワークドライブでアクセスするといい  goconveyをLinuxで動かしてブラウザで見る    勉強会参加 家にこもると良いことないので、わりと勉強会に出てました。
だいたい行く前はめんどくさいなーとか、行っても意味ないんじゃ…みたいになるけど、
いざ行くとかなり満足するし、有用な情報が手に入るので難しいです…
 参加記録 GolangNotHttpNight（Gunosy.go#12
 参加記録 第四回　ゲームサーバ勉強会  タスク管理アプリ作ってる  v0.0.1出した  https://github.com/ota42y/plaintodo/releases/tag/v0.0.1  タスク閲覧機能のみ 次はタスク追加機能とかつける
  </description>
    </item>
    
    <item>
      <title>参加記録 第四回　ゲームサーバ勉強会</title>
      <link>http://ota42y.com/blog/2015/04/19/game-server-4/</link>
      <pubDate>Sun, 19 Apr 2015 11:03:24 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/04/19/game-server-4/</guid>
      <description>第四回　ゲームサーバ勉強会に参加しました。
サーバにはあんまり詳しくないですが、そういう人向けの勉強会なので助かりました。
特に名前は聞いたことがあるけど使ったことないorちょっと触ったぐらいの物の理解がある程度深まりました。
IDC Frontierさんの話  IDCクラウドの話
 http://www.idcf.jp/cloud/ 最小プランが500円からとのこと…安い  ゲームでも採用事例あり。
 http://gihyo.jp/admin/column/01/vm/2014/nanohain02 http://ascii.jp/elem/000/000/952/952870/   ゲームサーバの作り方  http://www.slideshare.net/honyax/ss-47136625 簡単なチャットサーバの作り方 ゲームサーバも基本的なメッセージやりとり部分はチャットサーバみたいな物。  fluentdとembulkの話  http://www.slideshare.net/repeatedly/fluentd-and-embulk-game-server-4 外部からはjsonで入力するが、内部はMessagePackらしい tg-agentはfluentdにRubyインタプリタやプラグインをまとめてセットアップしやすくしたもの とりあえずfluentdに送って、そこから適切な場所に割り振る使い方みたい 組み込み環境向け  https://github.com/fluent/fluent-bit  Windows環境で動くやつ  https://github.com/fluent/fluentd-forwarder 名前の通り、ログをfluentdに転送するものっぽい？ これ本体で分類とかはせず、転送先のfluentdで転送する用途？  embulk  https://github.com/embulk/embulk ファイルから一気にデータを入れる用 失敗時に必要なファイルだけ再処理できる データから形式を推測する機能もある  fluentd-ui  https://github.com/fluent/fluentd-ui fluentdのWebUI データ処理のための正規表現をテストできる機能がある  データ欠損  ストリーム処理はデータ欠損が出てしまうもの 欠損が許されないものに使うのは危険  課金ログとか  欠損率0%の人もいるらしい  構成による？  通信失敗とかはfluentdが検知するのでそれで欠損はしない   負荷がたかいいんだから～♪（仮）  ゲームの負荷対策と負荷試験の話 memcached  全部メモリに乗ってるKVS  レプリケーション遅延  (たぶん)masterへの変更がslaveに変更される前に読み込んでデータがおかしくなった 水平分割して対処  特定のIDは特定のDBに書き込むようにする 1台あたりのリクエスト数を減らす？   NoSQL  Cassandra  リング型ノードになるため単一障害点がなくなるらしい DHT？  トランザクション処理は基本弱い  そういうところはMySQLで   負荷試験  リリース前に想定人数をちゃんと裁けるかのテスト  リリースしてから落ちにくい チューニングしやすい  テーブル構造の変更とかしやすい    JMeter  http://jmeter.</description>
    </item>
    
    <item>
      <title>golangのパッケージ管理</title>
      <link>http://ota42y.com/blog/2015/04/18/go-package-management/</link>
      <pubDate>Sat, 18 Apr 2015 10:55:37 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/04/18/go-package-management/</guid>
      <description>goでは標準でいろいろなツールが揃っていますが、
npmやbundlerのようなパッケージの依存管理をするツールはありません。
これは、goでは公開されている物は後方互換性を守り、
それを崩す場合は違うインポートパスにするべきだという思想によるものらしいです。
 Packages intended for public use should try to maintain backwards compatibility as they evolve. The Go 1 compatibility &amp;gt;guidelines are a good reference here: don&amp;rsquo;t remove exported names, encourage tagged composite literals, and so on. If different &amp;gt;functionality is required, add a new name instead of changing an old one. If a complete break is required, create a new package &amp;gt;with a new import path.</description>
    </item>
    
    <item>
      <title>参加記録 GolangNotHttpNight（Gunosy.go#12）</title>
      <link>http://ota42y.com/blog/2015/04/14/golang-no-http-night/</link>
      <pubDate>Tue, 14 Apr 2015 22:39:40 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/04/14/golang-no-http-night/</guid>
      <description>GolangNotHttpNight（Gunosy.go#12）
に参加しました。
ほぼ周りにgolangを使っている人がほとんどおらず、
情報がほぼ入ってこない環境にいるため、
ためになる話ばかりで、とても有意義な時間でした。
gomaについて  https://speakerdeck.com/kyokomi/gomanituite https://github.com/kyokomi/goma  Domaインスパイア  DBのテーブルから対応する構造体とCRUD用の関数を作ってくれるみたい DBへのアクセスは書き出されたSQLファイルを使う  SQLはgo-bindataでバイナリに入れ込む  いくつかライブラリを利用するが、作成されるコードには含まれないらしい  テーブル情報を取得するためにxormを利用 SQLファイル書き出しのためにegoを利用 CLI用にcliを利用   golintを使おう  いろいろ細かい所までチェックしてくれるgolintの話 pre-commitで実行するといい よくわかってなかったのでちゃんと使おう  go-timeout、もしくはUnixツールをgolangで書く話  http://songmu.github.io/slides/gunosygo-12/#0 mackerel-agentはgo製  https://github.com/mackerelio/mackerel-agent サーバ監視用の情報をmackarelに送る  https://mackerel.io/  pluginで拡張できる  外部コマンドとして呼んでるだけ   pluginが無限ループしたりするとブロックしてしまう  一定時間後にKILLするようにした  timeoutコマンドのようなものをgoで実装    Golang+Raspiで趣味的IoT入門的な話  https://speakerdeck.com/ymatsuwitter/golang-plus-raspidequ-wei-de-iotru-men-de-nahua Raspberry PiでIoT  armなのでgoでクロスコンパイル可能  Gobot  http://gobot.io/ いろんなデバイスをかなり抽象化された形で利用できる ロボットにパーツをつけていくメタファ 対応していないと使えないけど、対応しているととても便利っぽい  embed  https://github.</description>
    </item>
    
    <item>
      <title>2015年15週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-15/</link>
      <pubDate>Mon, 13 Apr 2015 11:53:02 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-15/</guid>
      <description>Atom.ioのstarでパッケージ管理 Atom.ioでは、パッケージのページ上でそのパッケージにスターをつけることができます。
https://atom.io/packages/project-manager
ここでスターをつけたパッケージはユーザのお気に入り扱いになり、
ユーザページ(例: https://atom.io/users/ota42y )上で確認できます。
さらに、Atom.ioにはapmというパッケージ管理システムがついており、
これにstarsオプションを与えることで一覧を見ることや、apm stars --installで、
ユーザのスターをつけたパッケージをすべてインストールできます。
https://github.com/atom/apm/blob/master/src/stars.coffee
プラグインのバックアップはもちろんのこと、複数環境でのプラグイン同期にとても便利なので重宝します。
タスク管理システム作ってる 最低限表示するだけのver. 0.0.1ができそうです。
https://github.com/ota42y/plaintodo
対話型インタフェースを作るのにlinerが凄い便利でした。
golangのラインエディタはlinerが便利
Travis.ciとか入れてCI回したいですが、テスト用コンテナでの依存パッケージの管理とか大変そうです…
godepやgomみたいなバージョン管理の導入を検討しますかね。
第6回ニコニコ学会β　データ研究会に参加した 参加記録 第6回ニコニコ学会β　データ研究会
今回の基調講演も発表もとてもおもしろかったです。
特に人狼はそこそこやっているため、とても参考になりました。
deviantARTは存在を知らなかったので、いろいろ調べてみたいですね…</description>
    </item>
    
    <item>
      <title>参加記録 第6回ニコニコ学会β　データ研究会</title>
      <link>http://ota42y.com/blog/2015/04/12/niconico_data_6/</link>
      <pubDate>Sun, 12 Apr 2015 21:03:35 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/04/12/niconico_data_6/</guid>
      <description>第6回ニコニコ学会β　データ研究会に参加しました。
基調講演  日立のH  スペシャリストのデータに基づいた施策より、超詳細なデータから計算した施策の方が有効  http://web-tan.forum.impressrd.jp/e/2015/03/06/19423  人間では見つけられない要因を特定できる  スキルより休憩中の雑談の盛り上がりが重要だったり 膨大なデータの因果関係を調べ上げるのは人間には無理   ウェアラブルハピネスメーター  人は動き続けているほど止まらなくなる  T時間動き続けると、止まる確率が1/T  実際のデータと1/Tとの乖離が高くなると、集団の幸福度が低下する  1/Tに近いほど集団が幸福度が高くなる  幸福度と業務の生産性は直結する  1/Tを上げることが生産性を上げることになる 生産性や幸福度に関連するデータを短いスパンで計測することができる  日々のKPIとして利用できそうなところが革新的っぽい 日立、集団の幸福感を測定する技術を開発 - PC Watch    参考  データの見えざる手: ウエアラブルセンサが明かす人間・組織・社会の法則  後で買う    人狼知能  脱初心者！経験は人狼力を向上させるのか？～データから見るベテランの実力～  http://www.slideshare.net/toritorix/ss-46899953 人狼BBSの分析 陣営ごとの平均プレイ回数が多い方が勝率が高い  運ゲーじゃなかった  経験によるスキル向上  占い師  的中率は変化なし  狩人  上級者ほど護衛成功しやすい  人狼への投票率  ちょっとだけ高い 上級者が複数人の陣営はわりと高くなる  個人の経験より集団の経験の方が強い  人狼知能プロジェクト  人狼BBSのデータセットとか公開するらしい http://www.</description>
    </item>
    
    <item>
      <title>golangのラインエディタはlinerが便利</title>
      <link>http://ota42y.com/blog/2015/04/11/go-liner/</link>
      <pubDate>Sat, 11 Apr 2015 14:05:04 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/04/11/go-liner/</guid>
      <description>peterh/liner
golangでCLIを作る際に活用できるラインエディタです。
使い方 liner.NewLinerで作成し、Prompt関数で入力を待機します。
入力があると関数が入力を返してくるため、それによって処理を分岐します。
なお、Ctrl+cの場合は普通に入力になりますが、Ctrl+dの場合はEOFとしてエラーを返してくるため、
エラー時に終了するようにしておくことでCtrl+dで終了できます。
package main import ( &amp;quot;fmt&amp;quot; &amp;quot;github.com/peterh/liner&amp;quot; ) func main() { line := liner.NewLiner() defer line.Close() for { l, err := line.Prompt(&amp;quot;Input: &amp;quot;) if err != nil { fmt.Println(&amp;quot;error: &amp;quot;, err) } else { fmt.Println(&amp;quot;get: &amp;quot;, l) if l == &amp;quot;exit&amp;quot; { break } } } }  入力履歴を使う AppendHistory関数に文字列を渡すことで、上下キーで入力履歴をたどれます。
State.AppendHistory
また、WriteHistory関数でファイルへの書き込みを、ReadHistory関数でファイルからの読み込みを行えます。
package main import ( &amp;quot;fmt&amp;quot; &amp;quot;github.com/peterh/liner&amp;quot; &amp;quot;os&amp;quot; ) func main() { line := liner.</description>
    </item>
    
    <item>
      <title>2015年14週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-14/</link>
      <pubDate>Mon, 06 Apr 2015 07:37:34 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-14/</guid>
      <description>pitが便利 cho45/pit
ユーザ名やキー等が必要な場合に、pitからロードするコードを書くことで、
保存されていればそれをロード、保存されていなければエディタを立ち上げて入力を促し、
結果をpitのディレクトリに保存してくれるものです。
これにより、アプリにパスワードを埋め込んだり、自分でいちいちファイルから読み込む必要がなくなり、
秘密情報の管理がとても楽になります。
golangにもpitがあるらしく、便利なのでgoで使ったやつにも入れようと思います。
https://github.com/typester/go-pit
changelogアプリに良いのがない githubの情報からchangelogを作ってくれるのはいくつかあります。
https://github.com/skywinder/Github-Changelog-Generator/wiki/Alternatives
ですが、どれも帯に短したすきに長しといった感じで、なかなか私が想定している使い方に合致した物がありません。
やっていることはそんなに難しくないはずなので、やはり自作するしか無いのでしょうか…
その場合、ある程度出力形式を変えられる形のがほしいですね。
一応golangでgithubのapiラッパーはあるようです。
google/go-github</description>
    </item>
    
    <item>
      <title>golangでIOへのテストを行う</title>
      <link>http://ota42y.com/blog/2015/04/01/go-io-test/</link>
      <pubDate>Wed, 01 Apr 2015 07:06:38 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/04/01/go-io-test/</guid>
      <description>まとめ  fmt.Print等にちゃんと出力されるかテストしたい  結論としては直接は無理  io.Writerを利用するように変えることで簡単にテスト可能  渡されたio.Writerに書き込むようにする 通常はos.Stdout、テストの時はbytes.Bufferを渡す どちらもio.Writerを実装している   標準出力への書き込みをテストしたい fmt.Print等で文字列を出力する場合、予期したものが出力されるかをテストしたい場合があります。
ですが、fmt.Printはそのまま出力まで行ってしまうらしく、こちら側で制御することは難しそうです。
このような場合、fmt.Printを使うのではなく、明示的に標準出力へ書き込むようにし、
テストの時は書き込み先を切り替えることで簡単にテストができるようになります。
fmt.Fprintで出力先を指定する golangでは任意の書き込み先に対して書き込むfmt.Fprint関数が用意されています。
この関数は、io.Writerに対してフォーマット指定した文字列を書き込めます。
https://golang.org/pkg/fmt/#Fprint
io.WriterはWrite(p []byte) (n int, err error)関数だけを持ったインターフェースです。
そのため、これを実装していればfmt.Fprintの書き込み先として使えます。
http://golang.org/pkg/io/#Writer
golangでは、io.Writerを実装した標準出力をos.Stdoutとして提供しています。
そのため、os.Stdoutにfmt.Fprintで書き込むことにより、
出力先を変更可能な状態で標準出力に出力できます。
メモリ上に出力する golangでは、byets.Bufferもio.Writerを実装しており、こちらは書き込まれた文字列をメモリ上に保持してくれます。
そして、String()関数により、書き込まれた文字列をstringとして取得できます。
http://golang.org/pkg/bytes/#Buffer
これを利用し、普段はos.Stdoutに書き込むようにし、テストの時に書き込み先をbyets.Bufferに変更することで、
標準出力に出力されたかどうかをテストすることができるようになります。
サンプルコード print.go
import ( &amp;quot;fmt&amp;quot; &amp;quot;io&amp;quot; &amp;quot;os&amp;quot; ) func testPrint(w io.Writer) { fmt.Fprint(w, &amp;quot;write test\n&amp;quot;) } func main() { testPrint(os.Stdout) }  print_test.go
package main import ( &amp;quot;bytes&amp;quot; &amp;quot;testing&amp;quot; ) func TestPrint(t *testing.</description>
    </item>
    
    <item>
      <title>2015年13週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-13/</link>
      <pubDate>Sun, 29 Mar 2015 10:37:34 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-13/</guid>
      <description> プロトタイプ展に行きました 東京大学山中研究室プロトタイプ展2015 &amp;ldquo;PLAYFUL&amp;rdquo;に行きました。
全く同じ素材でも構造を変えることで、握ったり持ったりしたときの感触が全く違うのはおもしろかったです。
現代アートっぽいやつはよくわからなかったですが…
Goについて理解が深まった Golangを多く触った一週間でした。
 GoのポインタはC++ポインタとは違う gxuiで簡単なGUIアプリを作りました  gorutineのすばらしさを改めて感じる 描画処理とは別に平行して処理をしたい場合にgorutineで簡単に書ける   </description>
    </item>
    
    <item>
      <title>GoのポインタはC&#43;&#43;ポインタとは違う</title>
      <link>http://ota42y.com/blog/2015/03/28/go_interface/</link>
      <pubDate>Sat, 28 Mar 2015 21:26:40 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/03/28/go_interface/</guid>
      <description>C++みたいなノリでGoのインターフェースとポインタを使ったところ、はまったのでメモ。
Goでインターフェースを実装したクラスのポインタを扱う Goで以下のように、インターフェースを実装したクラスを受けたい場合があります。
type Node interface{ ToString() string } func Output(l Node) { fmt.Println(l.ToString()) } type NodeTest struct{ } func (n NodeTest) ToString() string{ return &amp;quot;test&amp;quot; } func main(){ n := NodeTest{} Output(n) }  関数呼び出しのたびにオブジェクトがコピーされるのは無駄なので、 インターフェースのポインタを渡すように変更します。
func Output(l *Node) { fmt.Println((*l).ToString()) } func (n *NodeTest) ToString() string{ return &amp;quot;test&amp;quot; } func main(){ n := &amp;amp;NodeTest{} Output(n) }  この場合、インターフェースのポインタは、インターフェースを実装したstructのポインタとは違うため、 関数の引数として渡すことができず、コンパイルが通りません。
そのため、インターフェースを使う場合はオブジェクトをコピーせざるを得ないように思えますが、 ちゃんとこのような場合も解決方法は存在します。
ポインタにインターフェースを実装する 上記の2番目のコードでは、NodeTest型のポインタに対してインターフェースを実装しています。
そのため、以下のようにOutput関数の引数をNode型を受けるようにしておくのが正解になります。
func Output(l Node) { fmt.</description>
    </item>
    
    <item>
      <title>2015年12週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-12/</link>
      <pubDate>Mon, 23 Mar 2015 07:34:46 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-12/</guid>
      <description>gxuiを触ってた https://github.com/google/gxui
GoogleによるGolang制のクロスプラットフォームなGUIライブラリです。
クロスコンパイルは難しそうだけど、同じソースでビルドは普通にできそうです。
ざっと見た限り、必要最低限しか整ってない感じなので、これからに期待です。
Todoアプリ作成中 http://ota42y.com/pages/summary/2015/week-09/でやろうとしていたTodoアプリを作成中です。
今月中にはできるかと思っていましたが、このサイトの作成に思った以上に時間をとられてしまい、あんまり進んでいないです…
ただし、おおよその仕様はまとまりましたし、機能を大幅に削った最低ラインを決めました。
最低限使い物になるまで半分ぐらいなので、後2週間ぐらいで作り上げたいと思います。
https://github.com/ota42y/plaintodo
Hugoからの出力をGithub Pagesにアップロードする Octopressの場合はGithub Pagesにアップロードしてくれるコマンドがありましたが、
Hugoの場合、特にそんなコマンドはないようです。
そのため、HugoがHTMLを出力するフォルダに、Github Pagesをcloneしておく必要があります。</description>
    </item>
    
    <item>
      <title>Windowsにgxuiをインストールする</title>
      <link>http://ota42y.com/blog/2015/03/22/gxui-install/</link>
      <pubDate>Sun, 22 Mar 2015 10:27:54 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/03/22/gxui-install/</guid>
      <description>gxuiは、GoogleによるGo製のクロスプラットフォームなGUIライブラリです。
WindowsへのインストールはGoの環境を整えるところでだいぶ大変だったので、手順を書いておきます。
なお、一部の依存ライブラリが64bitに対応していないため、すべて32bit版を使います。
Go言語の環境構築 Go本体に加えて、依存ライブラリのためにgcc,hg,gitが必要になります。
元々SouceTreeがgitとhgを内部に持って、しかもコンソールまで提供していたのでそれを使っていましたが、
mingwのgccを認識してくれないため、コマンドライン版をインストールし直しました。
VCSのインストール git(http://git-scm.com/)とmercurial(http://mercurial.selenic.com/)をインストールします。
gitの場合、git bash onlyではなく、コマンドラインからも使えるようにしてください
mingwのインストール http://sourceforge.net/projects/mingw/からmingwを入れ、
mingw32-baseとmingw32-gcc-g++にチェックを入れて、メニューのInstallationからApply Changesを選択します。
なお、Goの64bitとmingwの64bitを使ったところ、glfwのインストール時にサポートしてないよって言われました。
これは両方とも32bitに揃えることで回避できました。
コマンドラインからgccが使えるようになっていれば大丈夫です。
gxuiのインストール gxuiと4つの依存するパッケージをインストールします。
go get http://github.com/google/gxui go get http://code.google.com/p/freetype-go/freetype/raster go get http://code.google.com/p/freetype-go/freetype/truetype go get http://github.com/go-gl/gl/v3.2-core/gl go get http://github.com/go-gl/glfw/v3.1/glfw  これだけでインストールはおしまいです。 gxui内のsample/下にあるサンプルを動かして確認をしてください。</description>
    </item>
    
    <item>
      <title>OctopressからHugoに乗り換えた</title>
      <link>http://ota42y.com/blog/2015/03/16/octopress_to_hugo/</link>
      <pubDate>Mon, 16 Mar 2015 07:40:11 +0900</pubDate>
      
      <guid>http://ota42y.com/blog/2015/03/16/octopress_to_hugo/</guid>
      <description>このサイトは元々静的サイト作成ツールのOctopressを使い、Github Pages上に構築していましたが、
サイト作成ツールの部分をGolangで作られたHugoに置き換えました。
まとめ  Octopress  Ruby制の静的サイト作成ツール 大量の記事を扱うと遅くなっていく  100記事で新しい記事のHTML出力まで10秒ぐらいかかる 見た目を確認したいときなどにとても不便   Hugo  Golang制の静的サイト作成ツール 利点 早い  100記事で200msぐらい  環境構築いらず  公式がバイナリ配布 手を加えないならそのまま使える Win-Mac両方使う人にはとても楽  欠点  テーマが少ない  このサイトも自作 https://github.com/ota42y/orange42   手を加えにくい  手を加えると環境構築いらずの利点が失われる クロスコンパイルは楽なのでそれほどでもない？  手を加える必要が無いのでそのとき考える     Octopressの問題点 HTMLのレンダリングが遅いです。
Octopressにはローカルにサーバを立てて、実際に表示される画面をブラウザで表示する機能があります。
この機能はファイルを監視しており、変更があるたびに再読込をしてくれるので、
表示されるHTMLをみながらmarkdownを編集でき、とても役に立っていました。
現在このブログは100記事ぐらいありますが、その状態だと1記事のHTMLを作るのに10秒ぐらいかかってしまいます。
ちょっとした修正ごとに10秒待つのはなかなかにつらく、
かつ記事が増えて行くにしたがって速度がより遅くなっていくことが予想できました。
そのときちょうどGolangで作られたHugoのことを知り、速度もとても速いとのことなので乗り換えを検討しました
乗り換え方はこちらのサイトを参考にさせていただきました。
OctopressからHugoへ移行した
Hugoの利点 HTMLのレンダリングが早い Octopressだと10秒ぐらいかかっていた状態をそのまま移行しましたが、
ファイルを更新してからHTMLに変換されるまでの時間が400msにまで短縮され、
ほとんど待ち時間が感じられないレベルになりました。
特にチューニングとかを考えずにこの速度なので、とても助かります。
環境構築いらず Hugo本体に手を入れないのであれば、様々な環境用の実行ファイルが配布されているため、
環境構築でがんばる必要がありません。
私はWindowsとMacの両方を使っていますが、WindowsでのRubyはつらいものがあるので、
Windows向けのバイナリをダウンロードするだけですむのは大変便利です。
Hugoの問題点 テーマがない テーマの数が圧倒的に少なく、思った通りのサイトを作るためには自分で作らないと行けません。</description>
    </item>
    
    <item>
      <title>2015年11週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-11/</link>
      <pubDate>Mon, 16 Mar 2015 07:34:46 +0900</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-11/</guid>
      <description> Hugoに移行した このサイトは元々Octopressで動かしていましたが、Hugoに移行しました。
OctopressからHugoに乗り換えた
GoのWindows開発環境を整えた IntelliJを使うとWindowsでも問題なく扱えます。
ただし、テストを実行するときはexeファイルを作ってから実行します。
そのため、テストのたびにウィルスソフトのスキャンが挟まるため、テストが無駄に遅いです…
特定のフォルダにexeファイルがはかれるため、そこだけ除外設定をしようかと検討中です。
google breakpad マルチプラットフォームなクラッシュレポートみたいです。
iOS,Android,Mac,Windows等かなりのプラットフォームに対応しているのでよさそうです。
https://code.google.com/p/google-breakpad
ちなみにMac OS10.9では以下のコマンドでビルドする必要があります。
(引用元は忘れました…stackoverflowだったはず）
xcodebuild -sdk macosx10.9 -project src/client/mac/Breakpad.xcodeproj -configuration Release -target Breakpad ARCHS=x86_64 ONLY_ACTIVE_ARCH=YES MACOSX_DEPLOYMENT_TARGET=10.8 GCC_VERSION=com.apple.compilers.llvm.clang.1_0  </description>
    </item>
    
    <item>
      <title>2015年10週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-10/</link>
      <pubDate>Mon, 09 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-10/</guid>
      <description>Hugoを実験中 今週はHugoに乗り換えるために、新しいサイト用のテンプレートを作っていました。
ゼロから作ると、テンプレート作成の大変さが身にしみます…
デザインを考える部分はもちろんのこと、CSSとの格闘や、
フレームワークの使い方とかを調べる必要があり、中々大変です。
また、golangのテンプレート機能については情報が少ないので、
やりたいことがあっても、普通に書くとうまくいかずに実現できない事が多々あったので、
あとでちゃんと勉強し直した方がよさそうです。
最も、本来テンプレート層で複雑な操作はするべきではなく、コントローラー側で処理をし、
テンプレートは渡されたデータを表示するだけにした方が望ましいです。
ただ、今回はhugo側にはあまり手を加えるのは避けたいため、
コントローラ側に処理を追加する事が出来ず、テンプレート部分で頑張る必要がでてしまいました。
テンプレートとhugoの間に普通のgoで処理を追加できたらいいのですが…
また、hugo側で用意されている奴を上手く使えばできる事も、
わざわざ自前でがんばって実装しようとしていたりしてて無駄が多いです…
サンプル集みたいなのがもう少し整っていると良い気がします。
ただ、やはり速度は本当に早く、編集してから描画まで10秒ぐらいかかっていたのが一瞬で終わります。
テンプレートは今週末ぐらいに完成するはずなので、土日に入れ替えようと思います。</description>
    </item>
    
    <item>
      <title>2015年09週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-09/</link>
      <pubDate>Mon, 02 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-09/</guid>
      <description>今年も15%が終わった…
GoのcronにPR送った Goのcronとしてhttps://github.com/robfig/cronを使ってますが、
実行中か否かを取得できずに不便でした。
内部では管理しているので、それに対するゲッターを追加したPRを送りました。
https://github.com/robfig/cron/pull/21
Hugoが良さそう このサイトはOctopressで運用しています。
ですが、記事数が増えてきたためか、変更して作り直すのに5秒ぐらいかかってしまいます。
ちょっとした変更でも若干時間がかるため、かなりストレスがたまる仕組みです…
同じく静的サイトを作成するソフトウェアで、Hugoというものがあるそうです。
https://github.com/spf13/hugo
Goで書かれており、作成する速度がかなり早いらしいので、乗り換えを検討中です。
残念ながら、良い感じのテンプレートがないので、まずはそこから作る必要がありそうです。
テキストベースのToDoアプリを考えた  Todo.txtが行けてないので自分に合ったやつを考える スペースと改行ででサブタスクを指定  箇条書きがそのままサブタスクになる  タスク名とサブタスク以外は全てオプションとして提供  優先度とかをベースシステムとしては持たない :due 2015-02-01とか、オプションとしてつける ベースはサブタスクとタスク名だけ   多分こんな感じになります。
タスク１ :due 2015-02-01 サブタスク1 :due 2015-01-31 サブタスク2 :due 2015-01-01 :repeat 1 week サブタスク3 :url http://example.com/  テキストベースで編集するのとは別に、CLIも作らないとダメですね。
フィルターとか、デフォルト値を入れて追加とかしにくいので…
リポジトリはこれになります。
https://github.com/ota42y/plaintodo
今やってるもの＆これからやりたいこと  今やってること
 Goによるtumblr apiライブラリ 進捗どうですか？時に画像もつけてほしくなった tumblr apiからランダムで取ってくるために、tumblrライブラリを作る  ついでにリブログもする node-tumblrがリブログしないので作った  goでやりたかっただけ 予定していた機能(投稿取得、リブログ)はだいたい完成
 完全にカバーしてないけど、ほしい部分は出来た  自分用自動化システム</description>
    </item>
    
    <item>
      <title>JenkinsをHTTP経由で叩く</title>
      <link>http://ota42y.com/blog/2015/02/27/jenkins-remote-api/</link>
      <pubDate>Fri, 27 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2015/02/27/jenkins-remote-api/</guid>
      <description>まとめ  JenkinsはWebベースのUI  操作の敷居が低い 使い込むと使いづらくなる 反応速度 複数のビルド実行  Remote access API  HTTP経由でJobを実行できる ページ表示を待たなくていいため高速 プログラムから実行可能 ただし、Jenkinsの仕様上、実行したビルドのビルド番号は解らない
   JenkinsのJob実行は大変 JenkinsはWebブラウザを使ってGUIで操作するため、
操作の敷居が低く、簡単に扱えるようになっています。
ですが、ブラウザやJenkins本体の状態によってはとても遅くなってしまい、
ページを切り替えるのに3，4秒待つといった状態まで遅くなると、
ビルドをするのがとても辛くなります。
また、パラメータの組み合わせの分だけビルドしようとすると、
さらに辛くなってしまいます。
このような場合に、Jenkinsに用意されているRemote access APIを使うことで、
Jenkinsをプログラムから制御でき、反応の遅さに悩まされたり、
パラメータの数だけクリックをする必要性から逃れられます。
Remote access API 詳しくはこちら
https://wiki.jenkins-ci.org/display/JENKINS/Remote+access+API
https://wiki.jenkins-ci.org/display/JENKINS/Parameterized+Build
要するに、パラメータが無い場合は
http://HOST/job/JOB_NAME/build に、ある場合は
http://HOST/job/JOB_NAME/buildWithParameters にPOSTで投げるとビルドできます。
パラメータの投げ方はフォームデータとしてでもいいですし、
http://HOST/job/JOB_NAME/buildWithParameters?PARAMETER=Value のように、URLに直接入れても大丈夫のようです。
サンプルスクリプト 3*3=9種類のビルドを一気に実行するスクリプトです
require &#39;open-uri&#39; require &#39;net/http&#39; platforms = [&amp;quot;ios&amp;quot;, &amp;quot;android&amp;quot;, &amp;quot;windows&amp;quot;] settings = [&amp;quot;debug&amp;quot;,&amp;quot;release&amp;quot;, &amp;quot;store&amp;quot;] platforms.product(settings).each do |platform, setting| params = {:PLATFORM =&amp;gt; platform, :SETTING =&amp;gt; setting} p params url = URI.</description>
    </item>
    
    <item>
      <title>2015年08週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-08/</link>
      <pubDate>Sun, 22 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-08/</guid>
      <description>Todo.txtは思想は良いけど実際使うと微妙 Todo.txt テキストオンリーでTodo管理ができる手法です。
ファイルをDropboxに置くことで、
どこでも同期してTodo管理ができるようになります。
本当に単なるテキストなので管理が簡単で、エディタで編集も容易です。
コマンドラインから条件を指定して表示できるのも便利で良い感じでした。
RTM CLIのオフライン版みたいな使い勝手です。
ただし、サブタスクが使えないという問題があります。 一応プラグインとして用意されてはいますが、
そうすると他のプラグインが使えなくなってしまうため、別の問題が発生してしまいます。
このテキストベースという考え方はすごく良く、かつ作るのもそんなに大変じゃなさそうなので、
自分で作ってみた方が早そうです。
Windown on Linuxで開発環境を整える Windowsで開発環境を整えるのは恐ろしく大変です。
幸いなことに私のWindowsマシンは十分なスペックがあるため、
VMware上にLinuxを立てて、SSHとファイル共有でしのぐことにしました。
IDEとかはWindows側で動かしているため、IDEの実行ボタンが使えなくなるのがかなりつらいですが、
Windows上で開発するよりかは幾分マシになっていると思います。
ただし、IDEからデバッガが使えないため、そのあたりでつらい現実が待っている気がします。
実行環境はリモートで、みたいなことができれば良いのですが…
普段ターミナルでしか作業をしないのであれば、ターミナルの中が変わるだけなので問題ないと思います。</description>
    </item>
    
    <item>
      <title>2015年07週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-07/</link>
      <pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-07/</guid>
      <description>既に今年の1/8が終わっているという事実…
JenkinsのWorkflow Pluginがとても便利 Jenkins Workflow Pluginで複数slaveを扱うのが楽になる
Jenkins Workflow Pluginでリポジトリ内のスクリプトを読み込む時の注意点 
にまとめています。 Build Flow Pluginを多用していたならば、こっちに乗り換えた方がいろいろ楽ですね。
ただし、まだ出たばかりでバグがあったり、所々かゆいところに手が届かなかったりするので、
その辺の辛いところは結構あります…
Remember The Milkが使いにくく感じる  良い点
 タスクの追加時に独自の記法で様々な条件をつけられる オフラインでAndroid, iOSで見られる(有料) 繰り返し処理、期日等の設定が十分に豊富 Evernotのノートとタスクを関連づけれる タスクと参考資料をくっつけられる ただし、RTM側でつけられる細かい情報をつけられない そのため設定はRTMでする必要があり、そこまで便利ではない  悪い点
 PC上で使いづらい ブラウザアプリはURLをショートカットキーで使えない 送信済みリスト等、複数人での作業用機能がOFFにできない 別のリストに移動するショートカットキーがない サブタスクが使えない 特定のタスクを細かく分割できない   特にサブタスクが使えず、かつリスト操作が弱いのがものすごく不便で、
特定の大タスクに対してやることをざっと書き出したときに、
それを上手くまとめられず、タスクの優先順位等がわかり辛くなります。
もっとも、オフライン機能とか追加時のUIとかはすごく良いのでまだぎりぎり使えますが、
代わりのサービスを探すか作るかしようと思います…</description>
    </item>
    
    <item>
      <title>Jenkins Workflow Pluginでリポジトリ内のスクリプトを読み込む時の注意点</title>
      <link>http://ota42y.com/blog/2015/02/11/jenkins-workflow/</link>
      <pubDate>Wed, 11 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2015/02/11/jenkins-workflow/</guid>
      <description>まとめ  ブランチをパラメーターにするとスクリプトを読めない  中身ではなく変数名のブランチを探しに行く 多分バグ  ファイルから読み込むるスクリプトを書けば解決  公式のflow.groovyを参考に 変数を使おうとすると面倒 java.io.Serializableを実装する必要あり   gitリポジトリ内のスクリプトを指定できない (以下に用意されているdocker上のJenkinsで確認しました)
https://github.com/jenkinsci/workflow-plugin/blob/master/demo/README.md
Workflow Pluginでは、リポジトリ内のgroovyスクリプトを読み込んで実行する機能があります。
この機能を使うことで、リポジトリの内容とそれに対応するビルド手順を同時にバージョン管理出来るため、
ビルド手順の変更がとてもやりやすくなります。
ですが残念ながら、パラメータで指定したブランチをチェックアウトして読み込むと、以下のエラーになります。
&amp;gt; git config remote.origin.url /var/lib/jenkins/workflow-plugin-pipeline-demo # timeout=10 Fetching upstream changes from /var/lib/jenkins/workflow-plugin-pipeline-demo &amp;gt; git --version # timeout=10 &amp;gt; git -c core.askpass=true fetch --tags --progress /var/lib/jenkins/workflow-plugin-pipeline-demo +refs/heads/*:refs/remotes/origin/* &amp;gt; git rev-parse origin/$BRANCH_NAME^{commit} # timeout=10 &amp;gt; git rev-parse $BRANCH_NAME^{commit} # timeout=10  パラメータの内容ではなく、パラメータの名前そのものを探しに行っており、おそらくバグと思われます。
通常のJobでGit Pluginを使うと問題なくパラメータ指定が出来るため、Workflowのバグと思われます。
このバグは、Jenkinsでは事前に設定した特定ブランチしかビルドしない場合は問題ありません。
ですが、様々なブランチで実行する可能性がある場合、JenkinsのJob設定にスクリプトを書かなければならず、
ビルド手順自体の管理が大変になります。
このような場合、スクリプトをロードして実行するスクリプトをJobに設定することで、
指定したブランチからスクリプトを読み込んで実行できます。</description>
    </item>
    
    <item>
      <title>Jenkins Workflow Pluginで複数slaveを扱うのが楽になる</title>
      <link>http://ota42y.com/blog/2015/02/10/workflow-plugin/</link>
      <pubDate>Tue, 10 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2015/02/10/workflow-plugin/</guid>
      <description>まとめ  Jenkinsはjob単位でしかノードに割り振れない  複数ノードで分散ビルドするには設定を駆使する必要がある 結果としてjobの数が増えて管理コストが増大する  Workflow Pluginで大幅に改善する  スクリプトからノードを指定してコマンドを実行できる 複数のjobを組み合わせていたのが一つのスクリプトですむ スクリプトをVCSに入れればビルド設定のバージョン管理も可能   複数ノードをコントロールするのは難しい Jenkinsを一つのノードで運用している場合はそれほど問題になりませんが、
複数のノードで、jobの一部の部分だけを別のノードで実行するなど、
ある程度複雑な分散をやろうとすると、Jenkins本体の機能では不足してきます。
このような問題に対して、これまではBuild Flow Pluginを使う事で解決が可能でしたが、
Workflow Pluginを使う事で、さらに簡単に解決することが出来ます。
Build Flow Pluginの問題点 Build Flow Pluginは基本的に複数の下流ビルドを管理するために作られているため、
処理を分けようとするとjobの数が増加していきます。
例えばネイティブアプリのビルドのようなCPUパワーを使う処理と、パワーを使わないアップロード処理がある場合、
一つのノードで全てやるよりも、非力なサーバを確保してアップロード処理をそちらで実行した方が、
ビルド用のノードはビルドに専念でき、無駄なくjobを実行できます。
(また、masterとビルドを同じノードでやるとJenkins本体の処理が遅くなるため、分割する利点は他にもあります)
(上段のup#4はup#3の間違いです…)
ビルドが数十分、数時間かかるような巨大な処理の場合、
処理を分散することで稼げる時間はかなりのものになります。
従来では分割する作業を別のjobにし、かつBuild Flow Plugin用のjobを作る必要があります。
さらに、ファイルの受け渡しもできないため、成果物として保存して、
次のjobは前のjobの特定のビルド番号の成果物を取り出す…といった風になります。
jobの数が増えると管理も大変ですし、使う方もどれを使えば良いのか解らなくなります。
ここで、Workflow Pluginを使うことで大きく改善することが出来ます。
Workflow Pluginを使ったビルド Workflow Pluginでは、slaveを選択してコマンドを実行、
特定のファイルを別のslaveにコピーして処理を実行ということががスクリプトで書けます。
例えば以下のように書くことで、masterでファイルを生成してslaveで実行、
その後結果をmasterにコピーしてアップロードみたいな事が出来ます。
node(&amp;quot;master&amp;quot;){ sh &amp;quot;rake config&amp;quot; archive &amp;quot;config.yml&amp;quot; } node(&amp;quot;slave&amp;quot;){ unarchive mapping: [&#39;config.yml&#39; : &#39;./&#39;] sh &amp;quot;rake build&amp;quot; archive &amp;quot;result.</description>
    </item>
    
    <item>
      <title>2015年06週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-06/</link>
      <pubDate>Mon, 09 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-06/</guid>
      <description>ちゃんとその週に何をやったかをまとめると、
今週はあんまり活動していない事がよくわかりますね…
Jenkins Workflow Pluginが素晴らしそう Jenkins ユーザ・カンファレンス 2015 東京で知った、Workflow Pluginが思った以上に良さそうです。
Jenkins本体の機能では出来ないmaster/slaveを選択してコマンドを実行するといったことが可能になり、
効率の良い分散ビルドが出来るようになります。
そのうち詳しくまとめる予定…
思った以上にTwitterに時間を取られる Aptrax | App Usage Trackerというツールで、
スマホの各アプリの使用時間を計測していますが、Twitterの使用率が凄い結果になっていました。
PC上で見ている時間も結構多く、かなりの時間を浪費している気がします…
重要なことだけ抜き出してできる限り見ないみたいな事が必要ですね。
twieveでツイートをevernoteに保存 twieveを使って、ツイートをevernoteに保存するようにしました。
これでやってることを呟いておけば、今週何をやったかを簡単に思い出せるようになるはずです。</description>
    </item>
    
    <item>
      <title>Go言語で一次の最小二乗法を実装した</title>
      <link>http://ota42y.com/blog/2015/02/03/leastsquaresmethod/</link>
      <pubDate>Tue, 03 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2015/02/03/leastsquaresmethod/</guid>
      <description>こんな感じです。
暗黙的にキャストしてくれないので若干面倒です。</description>
    </item>
    
    <item>
      <title>2015年05週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-05/</link>
      <pubDate>Mon, 02 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-05/</guid>
      <description>今週は二日ほど有給を取っていました。
家で引きこもっていましたが、かなり有効に引きこもれたと思います。
毎日色々していますが、やりたいことがどんどん増えてきます…
効率をあげるにも限度がありますし、どうしたものですかね。
Jenkins Workflow Plugin便利そう Jenkins ユーザ・カンファレンス 2015で聞いたjenkinsci/workflow-pluginを試しています。
スレーブを指定してタスクを実行できたり、ファイルをスレーブから取ってきて別のスレーブに渡す等、
今までBuild Pipeline Pluginでやっていたことがだいたい実現できて凄く良いです。
あとはプロパティ等をファイルから読み込む方法さえ解れば完璧なのですが…
Evernoteリンクを開くChrome拡張の更新 http://ota42y.com/blog/2015/01/31/evernote-opener-update/
便利なのですが、新しいPCに設定する際にとても不便だったので楽に修正できるように機能拡張しました。
Jenkinsのライブラリよさそう https://github.com/yosida95/golang-jenkins
Webの反応めちゃくちゃ遅いので、よく見る結果とかはチャットに流せないかと思って見ています。
(こっちの方がいろいろそろってるかも https://github.com/bndr/gojenkins)
Remember The MilkのCLIを使ってみた RTM-CLIなるものがあるらしいので使ってみました。
Web版は微妙に使いにくいので反応が早いこいつを試してみましたが、微妙でした。
キャッシュされないために毎回認証して取ってくるので反応が悪く、
速度を求めた場合の乗り換え先としては良くありませんでした。
これにキャッシュ機能がつけばおそらく完璧なのですが、現状のコマンドラインツールだと厳しそうですね…
ライブに参加していた μ&amp;rsquo;s Go→Go! LoveLive! 2015 ～Dream Sensation!に参加してました。
風邪やインフルエンザの可能性を減らすために、2日前から有給取って家に引きこもっていました。
1日目はチケットが取れなかったのでLVで、2日目は開場でしかもアリーナAブロックでした。
前回の映像は何回も見ていましたが、やっぱり映像よりもLVがいいですし、
LVよりも会場の方が何十倍も凄かったです。
文才が無いのでこれぐらいにとどめておきますが、次のライブは両日当たるまで応募券を買おうと思います。</description>
    </item>
    
    <item>
      <title>Evernoteのリンクをアプリで開くChrome拡張を更新した</title>
      <link>http://ota42y.com/blog/2015/01/31/evernote-opener-update/</link>
      <pubDate>Sat, 31 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2015/01/31/evernote-opener-update/</guid>
      <description>evernote url openerを更新しました。
使うためには、EvernoteのユーザIDとShardを調べる必要がありますが、
ちょっと面倒だったので、ユーザが自分のノートリンクをペーストするだけで、設定するように変更しました。
Evernoteからコピーできる、
https://www.evernote.com/shard/USER_SHARD//WORD/USER_ID/NODE_ID/
といった形式のリンクを張ることで、USER_SHARDとUSER_IDを保存します。</description>
    </item>
    
    <item>
      <title>bundle中に別のbundleを呼ぶと予期しない結果になる対策</title>
      <link>http://ota42y.com/blog/2015/01/28/bundle-in-bundle/</link>
      <pubDate>Wed, 28 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2015/01/28/bundle-in-bundle/</guid>
      <description>bundle execでrubyファイルを実行し、別のGemfileのあるディレクトリに移動してbundle系のコマンドを実行すると、
一回目のbundlerに対してbundleが実行されてしまい、
別のbundleを呼び出せないという問題が起きました。
これはbundlerが設定する環境変数が原因でした。
まとめ  bundle exec中に、別のbundlerを実行するとおかしくなる  主にsystemやspawn等を使った場合 最初のbundle execと同じものであれば問題は起きない  bundlerが設定をしている環境変数が問題  別のbundlerを呼ぼうとして元のbundlerが呼ばれている  Bundler.with_clean_envで回避可能  Bundler.clean_systemでも可   問題 以下のような構成かつtestフォルダにいる状態で、
bundle install --path vendor/bundle
bundle exec test.rbを実行すると、
test2内でbundlerを呼んだ時にエラーになります
. ├─ test │ │ Gemfile (gemのhello_world_gemを使用) │ └─ test.rb | └─ test2 └─ Gemfile (gemのhello-worldを使用)  system &#39;hello_world_gem&#39; Dir.chdir(&amp;quot;../test2&amp;quot;) do # error system &#39;bundle install --path vendor/bundle&#39; system &#39;bundle exec hello-world&#39; end  エラー文言
oh hai thar Using hello_world_gem 0.</description>
    </item>
    
    <item>
      <title>2015年04週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-04/</link>
      <pubDate>Mon, 26 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-04/</guid>
      <description>先週やったことまとめ
以外とあまりやってない。
作りたいものは増えていくけど時間は無い。
tumblrクライアント作ってる info APIだけはできた。Posts APIを作りたいけど、ちょっとめんどくさそう。
複数の型を一つの配列で返して来るので、どうやって同じ配列に入れようか…
たぶん全部入りのクラスで取り出して、個別のクラスに入れていく形になるかと。
http://play.golang.org/p/tVLoIDVk&amp;ndash; この技を使えば違う型でも共通の配列に入れられるはず
情報科学若手の会冬の陣2015に参加した 参加記録 情報科学若手の会冬の陣2015
発表したい人生だった… _ (:3 」∠)_
が、ここ半年間はずっと一日に使える時間が1〜2時間程度、かつ仕事は情報科学っぽい事をしてないので、
発表ネタが全く作れないという…
というか、特に技術的に成長してない気がするし、このままの生活を続けて大丈夫なのだろうか…
Remember The Milk使いやすいが使いにくい わりといい線行ってるサービスだけど、Webクライアントが使いにくいのでちょっと残念。
URLをショートカットから開けなかったり、ヘッダー画像が邪魔だったり…
サブタスクが無いのもそれはそれで使いにくいですね。
ただ、オフラインでスマホで見られますし、繰り返し設定など様々な設定が凄く楽にできる等
機能的にはいいですし、UIもいいのであともう一声って感じですね…
コマンドラインから対話的に操作できれば解決できそうな気がします。</description>
    </item>
    
    <item>
      <title>参加記録 情報科学若手の会冬の陣2015</title>
      <link>http://ota42y.com/blog/2015/01/25/wakate2015w/</link>
      <pubDate>Sun, 25 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2015/01/25/wakate2015w/</guid>
      <description>情報科学若手の会冬の陣2015 #wakate2015wに参加しました
Togetterまとめ
会場は電源完備＆大きめの机が椅子に着いているという豪華仕様でした
まとめ  黒崎優太 (@kuro_m88) ICTトラブルシューティングコンテストの紹介
 http://icttoracon.net/ ネットワーク版CTFみたいなコンテスト Ciscoがスポンサーについているので、高価な機材がいっぱい用意されるみたい 大きな規模のネットワークであれこれするのって普通じゃ出来ないし、面白そうです。  浅野智之 (@asanon_s) Webの世界に散らばるデータをつなぐ技術
 semantic webやPDFの話 人間が読めれば良い形式から、構造化されて機械も読める形式を加えるみたい なんとなく理解が深まった気がします 完全なsemantic webへの道のりはまだまだ  門脇香子 証明プログラミング入門
 http://www.slideshare.net/ussopyon/ss-43846794 OpenSSLの脆弱性をCoqで見つけた話を聞いてから、証明プログラミング凄そうだけどよくわからない状態だったので、 理解が深まりました。 何を証明すれば良いかを補完してくれるのは便利そうでした  佐野岳人 コードを書けば複素数が分かる
 http://www.slideshare.net/taketo1024/ss-43853048 初めて複素数が何なのかを感覚的に理解できた気がする… デモで関数をインタラクティブに可視化しているのが本当に凄かった ここから使える https://github.com/taketo1024/SwiftComplex  辻順平 (@tsujimotter) 日曜数学のススメ
 http://www.slideshare.net/junpeitsuji/2015-43856733 コンパスと定規で作図可能かどうかを判定するガウスの判定法凄い 折り紙だと四次方程式が解けるので、作図できなくても折れる場合があるとか凄い  池尻良平 ニュースと類似した歴史を求めるアルゴリズムを考えてみた – 役に立つ歴史教育を目指して
 現代のニュースと、それに類似する歴史上の出来事を提示するシステムを作っているそうです 過去にどんな解決策をとって、どういう結果になったかを考えるのにとても便利そうです 早く使ってみたいですね  後藤紳 モバイルデータを用いた行動予測
 あんまり書いちゃいけなさそうなので割愛 GPSの行動ログを蓄積して、次に何をしそうか提示するみたいです 同じような日々を送っている人には効果的っぽいです  宮代理弘 (@3846masa) Processin.</description>
    </item>
    
    <item>
      <title>RAMディスクでiOSのビルド時間を短くする</title>
      <link>http://ota42y.com/blog/2015/01/23/ram-disk/</link>
      <pubDate>Fri, 23 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2015/01/23/ram-disk/</guid>
      <description>まとめ  RAMディスクを作ってビルドすると早くなる  SSD上でビルドすると9分 RAMディスクでビルドすると6分 Androidは未検証だが同じと思われる  Macだととても簡単にRAMディスクが作れる Jenkins等、ビルドが主目的の場合に有効と思われる メモリが余っているならやる価値はある  Mac OS XでのRAMディスク作成 Macではhdidコマンドを使うことで、メモリ領域をディスクとして使用することが出来ます。
メモリはSSDと比べても遙かに早く、R/Wの激しい処理にうってつけです。
また、rootでなくても作成できるため、非常に便利です
ディスク作成手順 2つコマンドを実行するだけです。
hdid -nomount ram://4096000 # 指定した容量でRAMディスクが作成されます。 # /dev/disk2等作成された場所が返ります。 # ファイルシステムがないためマウントは失敗します。 # そのため、-nomuntをつけています。 diskutil eraseDisk HFS+ RAM /dev/disk2 # 先ほど作成したディスク(/dev/disk2)をHFSでフォーマットします # さらに、RAMという名前でマウントします。 # これにより、/Volumes/RAMでアクセスできるようになります。 hdiutil detach disk2 # 作成したディスクをアンマウントして削除します # ディスクは消滅するのでご注意ください # diskutil unmountDisk # diskutil eject # の組み合わせと同じですが、コマンド一つになるため楽です  速度比較 Mac Book Air 13-inch Mid 2013のSSDと速度比較しました。
速度計測にはXbenchを使用しました。
結果は以下の通りになりました。
どれをとってもRAMディスクの方が圧倒的に早い結果になりました。
ブロックサイズが小さいwriteの場合は2倍程度になっていますが、それ以外は5倍〜10倍以上の差がついています。</description>
    </item>
    
    <item>
      <title>Androidでクリップボードの中身を自動で辞書検索</title>
      <link>http://ota42y.com/blog/2015/01/21/android-dict/</link>
      <pubDate>Wed, 21 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2015/01/21/android-dict/</guid>
      <description>まとめ  clip2dicを使うとクリップボード内を自動で辞書検索する オフラインの別アプリからも検索可能 ColorDictとGolden Dictが連携先として優秀 英辞郎は形式が違うため辛い Google Driveをビューワーとして使うといい  ポップアップ辞書アプリ iOSやMacでは辞書機能がOSに統合されているため、英単語を選択するとそのまま辞書を引くことができます。
Androidで同じぐらい楽に辞書を引けないかと思って調べたところ、
clip2dicというアプリが、クリップボードの中身を自動でネット上の辞書から検索してくれました。
私はタブレットをオフラインで使用していますが、このアプリは別のアプリを呼び出して検索できるため、
他の辞書アプリを入れる事でオフラインでも辞書が引けるようになります。
clip2dictから使えるオフライン辞書アプリ ColorDict https://play.google.com/store/apps/details?id=com.socialnmobile.colordict
無料の辞書ソフトです。
初期状態では日英、英日辞書は入っていませんが、
別アプリとして配布されている辞書アプリをインストールすることで、 対応辞書を増やすことができます。 https://play.google.com/store/apps/details?id=colordict.dictdata.japanese.jmdict
(おそらくこちらの辞書を使用しています)
StartDict形式の辞書ファイルを持っているならば、端末内のdictdata内に置くことで、さらに辞書を追加できます。
GoldenDict https://play.google.com/store/apps/details?id=mobi.goldendict.android&amp;amp;hl=ja
Free版
こちらもColorDictと同じく、辞書ファイルが必要になります。
ただし、こちらはアプリ内から先ほどの辞書データをダウンロードできるため、
より導入が簡単です（辞書としてはおそらく同じだと思います)。
このアプリには、複数辞書を同時に検索できる利点があるらしいです（無料版は5個まで同時検索）
複数の辞書ファイルを持っている場合は便利ですが、
私はそんなに辞書ファイルを持っていないので無料で十分でした。
その他 手元に英辞郎のだいぶ古い版が手元にありますが、上記二つのアプリでは対応していないpdic形式になります。
pdicからStartDict形式の変換はとても手間がかかるのと、
古すぎてネット上にある手順では出来なさそうなので諦めました。
また、Adobe Readerは選択をすると単語ではなく文章を丸ごと選択するため、辞書検索に使えません…
いくつかPDFビューワーを試しましたが多くは文字選択が出来ず、
今のところ唯一出来たGoogle Driveをビューワーとして使っています。
(オフラインでも別アプリからPDFビューワーとして呼び出せる)
辞書は透明な別アプリとして元のアプリの上に表示されるらしく、閉じてPDFビューワーに移動するのが若干遅いです。
これはおそらくclip2dicの仕様だと思われるため、解決方法は無さそうです。</description>
    </item>
    
    <item>
      <title>2015年03週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-03/</link>
      <pubDate>Sat, 17 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-03/</guid>
      <description>Jenkins ユーザ・カンファレンス 2015 東京 参加しました
Workflow Plugin等を導入したいのですが、
現在クリティカルパス上の作業をしているために手が回りません…
運用コストを下げられるので是非入れたいのですが、
どうしても優先度は下がってしまいますね…
Tumblrのgolang用ライブラリ 見当たらなかったので作成中(まだ何も動きません)
https://github.com/ota42y/go-tumblr
goのテストは特殊なフレームワークとか作らずに、普通にコードとして書くのを推奨しているそうです。
rspecとかで専用のフレームワークにそって作るのとは大違い。
書くのは面倒だけど、フレームワークの知識が無くても大丈夫というのは良いかもしれません。
フレームワークの学習コストとそれによって得る効率化と、使わないことによる効率の低下とどっちが得なのでしょうか。 あと外部APIなので秘密情報をどうやってテストの時に設定するかが課題です。
毎回書いては消すのはとても面倒なので…
UIと強結合しているテスト cocos2d-xを使ってiPhoneアプリを作成していると、ゲームUIでTDDとかするのは実質不可能ではないかなと思います。 これはおそらくUI層の正しさが、状態に強く依存するのが原因です。
ボタンを配置してタッチで特定のメソッドを呼び出すような場合、
ボタンが画面上に表示されるかは、他のよりZ座標が大きいオブジェクトと被さっていないかをチェックする必要や、
より優先度の高いタッチオブジェクトが存在しないかといったことをチェックする必要があります。
さらに、演出中は表示されるけどタッチは出来ないなど、その部分以外の所の状態によって結果を変える必要があります。
つまり、新しいボタンを配置するためには様々な状態のテストを作成し、
かつほかのオブジェクトに対して新しい状態を追加することになるため、テストの変更がとても大きくなります。
そのため、UI層でテストをする場合、作成・維持ともにとても大きくなっていくため、
完全に不可能ではないですがコストか高く、現実的に出来るものではありません。
ただし、UIコンポーネントについて個別にテストするのは有効だと思います。
例えばボタンクラスであればタッチした時に、コールバックが呼ばれるか、範囲外の時に呼ばれないか等です。
ただし、UIを作る際に個別にクラスを作ることはそんなに多くなく、
多くがコンポーネントの配置と、その画面専用ロジックとのつなぎになるため、大きな効果は見込めなさそうです。
なお、ゲームはそもそも共通で使う部分が少なく、画面内の状態がとても多いという前提があります。
例えば通販サイトではヘッダーやサイドバーは共通で使い、メインの部分とは独立していますし、
メインの部分も商品データが違うだけでテンプレートは同じといったように、共通で使う部分が多いので、
そのような部分のテストは有効ではないかと思います。</description>
    </item>
    
    <item>
      <title>参加記録 Jenkins ユーザ・カンファレンス 2015 東京</title>
      <link>http://ota42y.com/blog/2015/01/11/jenkins-conference/</link>
      <pubDate>Sun, 11 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2015/01/11/jenkins-conference/</guid>
      <description>Jenkins ユーザ・カンファレンス 2015 東京に参加しました。
Togetterまとめ
発表まとめ アンケート結果  参加申し込み時のアンケート結果 多くの人のjobの数が1〜数個程度 Jenkins無くても生きていける人が30％ぐらい
 思ったよりもみんなJenkinsを使い込んでいない印象 アンケート結果は後日公開  Jenkinsプロジェクトの現状とワークフロー  DotCiが良さそう  https://github.com/groupon/DotCi Jenkinsを他のCIサービスみたいにする Github限定 設定をYAMLファイルで管理出来たりする  Workflow Pluginすごい  Build Flow Pluginをいっそう強化した版 ビルドが失敗しても途中から再開とかも出来る 乗り換えよう ただ、DSLを覚えたりと学習コストが高くなるので秘伝のタレ化の危険が   JenkinsとSeleniumの活用事例：試験自動化のプロジェクトへの導入  アジャイル開発だとイテレーション毎に試験項目は増えていくが、開発後半になるほど試験に割ける時間は減る  あるあるすぎる 試験自動化で試験項目の消化にかかるコストを下げる  スクリーンショットを撮って画像比較して変更検知  静的な画面なら効果的っぽい アニメーションする場合は撮るタイミングによりそうなので、目で比較かなぁ…  テストケースの保守大変そう  ちょっと変わっただけで使えなくなるし   Jenkinsを使ったコンシューマゲームでのデプロイとテスト  http://www.slideshare.net/swiftnest/jenkins-43394510 圧縮して6GB、11万ファイルのプロジェクト  2プラットフォーム2言語にビルド これで中規模ぐらいと凄い世界が…  FFとかどんだけなんだろう…   フルビルドに14時間かかる  リソースの事前処理に時間がかかる  リソース内での分岐処理が遅いため、全条件分のリソースを作る  データを実機上でビルドする必要がある  携帯機は貧弱なので差分ビルドでも4時間かかったりする プラットフォームで互換性なし PS3はビッグエンディアン VITAはリトルエンディアン  ファイルをパックするのでキャッシュが効かない  ファイルアクセス回数を減らすため必須 全データを実機でパックするため6時間ぐらい   頑張って並列化して5〜9時間ぐらいまで減らす  データの処理を4台で並列化  一台4時間ぐらいに短縮   テスト  テストフレームワークなど無い スモークテスト  特定ミッションに入れるかどうかをテスト 引数で専用のミッションを始められるように  一定時間たったら終了させる 正常終了しなかったらエラー  起動からスタート、終了までがちゃんと動くか リソースに問題ないか エフェクト再生チェック 一定時間内に終わるか データが変換可能か  モンキーテスト  開始からエンディングまでの通しプレイAI 良い感じに攻撃する  適当にボタン連打  適当に移動 何回もゲームオーバーになったら無敵＆攻撃力１００倍 メニュー操作テスト 普通のUIテストっぽい 長時間テスト特有のバグ発見に繋がる リソースの解放漏れ  携帯機はスリープできるので普通は電源を切らない   問題点  作成やメンテナンスにコストがかかる ゆとりがある時期にどれだけ作れるか勝負 チェックに時間かかる 一日じゃ終わらない 手動テストの置き換えにはならない テストのパターンが固定化されるため、外れた部分のバグは検知不可 人の手のかかるテストを減らす効果    おばかXFDコンテスト  人は慣れる生き物  何度も通知してるとそのうち無視するようになる  エラーメールを迷惑フォルダに入れたり パトランプの上に被せたり  XFDによる通知で、慣れに逆らう   LT大会  Jenkinsを使った継続的Webセキュリティテスト  継続的にセキュリティテストを実施するVAddyの話  http://vaddy.</description>
    </item>
    
    <item>
      <title>2015年02週目まとめ</title>
      <link>http://ota42y.com/pages/summary/2015/week-02/</link>
      <pubDate>Sat, 10 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/pages/summary/2015/week-02/</guid>
      <description>毎週やったことをまとめることで、その週を振り替えられるのではないかと思ったので、土曜か日曜ぐらいにその週のまとめを書きます。
主に1記事にするには難しいものや、失敗してダメだったものをまとめていく予定です。
思った以上に面倒なので、来週はしないかもしれません…
なんでブログに書くのか テキストでローカルに持っていても良いんですが、そうするとファイル管理が面倒になります。
こうやってブログに書いておけば検索エンジンからたどれますし、個々のファイルの管理をしなくて良くなります。
また、GithubのContributionsを使ってアウトプットを150日続けるでも書いたように、振り返りで1コミット稼げるので、振り返りをしっかりするようになるという狙いもあります。
node-tumblrのテストが動かせなかった node-tumblrでリブログする機能が欲しかったので追加しようかと思いましたが、 そもそも今の状態のテストができなくて諦めました。
例えばtwitterであれば、自分のfav一覧を正常に取れるかをテストするためには、 テストするアカウントでfavしておかなければなりません。 同じようなことがnode-tumblrのテストでも起きているのですが、 その準備に必要なものが何なのかがわからないため、テストできませんでした。
テストを書くことも大事ですが、テストを実行するための手順を整えておくのも大事ですね…
node-ircへのPRが取り込まれた 7月ぐらいに出したnode-ircのPRが取り込まれました。内容としてはIRCはCL-LFでメッセージを区切るに書いたように、古いサーバーは別の改行コードを送ってくるため、両方に対応できるようにする修正です。
https://github.com/martynsmith/node-irc/pull/280
ブランチ戦略が変わったらしく、出し直したためコミットは新しくなっています。
PRにたいしてツッコミが来たり、出された反論コードがおかしいので、ちゃんと修正すれば動くよと返したりと大変でした。 英語でのコミュニケーションは言いたいことをどう書けばいいか解らないので大変です…ボギャブラリー不足ですね。
インターステラーとベイマックスを観た ベイマックスは本国ではBig Hero 6というタイトルで公開されていて、 そのタイトル通り、ベイマックスのみが主人公ではなく、彼？を含む6人がアメコミヒーローになるという話だった。 また、映画の舞台が日本の風景をピクサー風に仕上げた街であり、主人公も日本人風の名前や顔立ちだったりと、 随所に日本リスペクトが見られました。
インターステラーは近年には珍しい本気で作られたSF映画で、これSF詳しくない人には辛いんじゃないかな？ と心配になるぐらい、しっかりとSFしています。 私は大好物ですが、興行収入が良いのかどうかは謎ですね。
映像としては専門家の監修の元でしっかりとした映像が作られており、 また、おそらく随所に2001年宇宙の旅のリスペクトっぽい部分があったのが面白かったです。
特に好きなシーンは、物語の中盤で主人公が仲間の感情的な意見をバッサリと切り捨てて合理的な判断をしたのと、 最後の方のシーンですね。</description>
    </item>
    
    <item>
      <title>Macのghcは改行コードがCRだと動かない</title>
      <link>http://ota42y.com/blog/2015/01/07/mac-ghc-cr/</link>
      <pubDate>Wed, 07 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2015/01/07/mac-ghc-cr/</guid>
      <description>Haskellでどう見ても文法的に間違っていないのに、
何故かコンパイルエラーになっていましたが、
改行コードが原因でした。
ちょうど始めたばかりで、実は文法ミスや、
バージョン違いとの切り分けが大変でしたが、
結果は残念な結果になりました…
まとめ  GHCではLF、CRLFは正しく動くがCRはダメ PDFからサンプルコードをコピペしたらCRになっていた  Macのプレビューでいくつか試したが、現状全てCRになる PDFが原因か、Preview.appがそういう仕様なのかは用検証   調査方法 以下のファイルを改行コードLFで保存します。
lf.hs
plus :: Integer -&amp;gt; Integer -&amp;gt; Integer plus a b = a + b main = print (plus 40 2)  LFのまま実行します
file lf.hs # lf.hs: ASCII text runghc lf.hs # 42  CRLFに変換して実行します
nkf -Lw lf.hs &amp;gt; crlf.hs file crlf.hs # crlf.hs: ASCII text, with CRLF line terminators runghc crlf.hs 42  CRに変換して実行します</description>
    </item>
    
    <item>
      <title>インライン展開についての追加調査</title>
      <link>http://ota42y.com/blog/2015/01/06/c-inline-postscript/</link>
      <pubDate>Tue, 06 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2015/01/06/c-inline-postscript/</guid>
      <description>昨日の記事で、インライン展開について調べましたが、
よくよく考えると片手落ちだったので追加調査しました。
インライン展開がどう展開されるのかを調べた
調査内容 昨日はヘッダと実装が書いてあるファイルとでの差は調べましたが、
同じファイル内でどのようになるかは調べていませんでしたので、
追加調査しました。
方法は昨日と同じく、-Sオプションをつけて結果を見ます。
ソースコード 以下のようなファイルを使います
test.cpp
#include &amp;lt;stdio.h&amp;gt; #include &amp;quot;func.h&amp;quot; int main() { TestA test; int a = test.getDirect(); int b = test.getThrough(); printf(&amp;quot;%d %d\n&amp;quot;, a, b); return 0; }  func.h
class TestA{ public: int getDirect(); int getThrough(); private: int getPrivate(); };  func.cpp
#include &amp;quot;func.h&amp;quot; int TestA::getDirect(){ return 42; } int TestA::getThrough(){ return getDirect() + getPrivate(); } int TestA::getPrivate(){ return 73; }  func.</description>
    </item>
    
    <item>
      <title>インライン展開がどう展開されるのかを調べた</title>
      <link>http://ota42y.com/blog/2015/01/05/c-inline/</link>
      <pubDate>Mon, 05 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2015/01/05/c-inline/</guid>
      <description>C++コンパイラは、関数呼び出し部分にその関数の内容を展開し、
関数呼び出しのオーバーヘッドを削減する、インライン展開をします。
インライン展開はコンパイル時にされるため、
実際に行われたのか、どう行われているかは出力されません。
そのため、コンパイルたコードがどうなってるかを調べ、
インライン展開がどう展開しているのかを調べました。
なお、アセンブラに関してはほとんど説明しません。
「callq シンボル名(文字列)」で関数呼び出しを実行する事だけ理解していれば大丈夫です。
ソースコード 以下のソースコードを使います
test.cpp
#include &amp;quot;stdio.h&amp;quot; #include &amp;quot;func.h&amp;quot; int main(){ TestA test; int a = test.getNumInCpp(); int b = test.getNumInH(); int c = test.getNumInline(); int d = test.getNumCallCpp(); printf(&amp;quot;%d %d %d %d\n&amp;quot;, a, b, c, d); return 0; }  func.h
class TestA{ private: int privateFunc(); public: int getNumInCpp(); int getNumInH() { return 42; } int getNumCallCpp(){ return privateFunc() + getNumInH(); } int getNumInline(); //int getNumNormal(); }; inline int TestA::getNumInline(){return 321;} // そもそも定義できない // int TestA::getNumNormal(){return 111;}  func.</description>
    </item>
    
    <item>
      <title>UNIXという考え方という本を読んだ</title>
      <link>http://ota42y.com/blog/2015/01/04/unix-boox/</link>
      <pubDate>Sun, 04 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2015/01/04/unix-boox/</guid>
      <description>UNIXという考え方という本を読みました。
原著は1994年に書かれている本で、 UNIXの哲学を一つ一つ、その意味と利点について解説しています。
「一つのことを、うまくやれ」や「小さいものは美しい」等、ある程度ハッカー文化に詳しい人ならばどれかは聞いたことがあるぐらい有名な哲学です。
面白いのは、例えば「できるだけ早く試作を作成する」という項目では、設計を完璧にしてから取りかかるのではなく、 プロトタイプを作り、それをユーザに見せてフィードバックをもらい、良い設計にしていくべきと述べられています。
これはアジャイル開発のイテレーションを回していく手法ととても似ており、開発手法の歴史を感じます。
また、「一つのことを、うまくやれ」は色々やる巨大な関数より、ちゃんと機能毎に分割しろと解釈できますし、「ソフトウェアの挺子（てこ）を有効に活用する」は他のいいプログラムからコードを借りてきたり、gemやnpmで既にあるソフトウェアを利用するといった、コードの再利用性の話だと解釈できます。
原著は1994年に書かれたにもかかわらず、今日でも普通に通用する哲学がちりばめられており、UNIX哲学の現代のプログラミング文化への強い影響と、その普遍性をかんじます。
ただ、本書の中でシェルスクリプトが移植性もいいし簡潔に書けるしと礼賛されていますが、デバッグしにくい上に、コマンドがGNUかBSDかで結果が変わる場合があるので実は移植性がそんなに無いですし、現代ではちょっとどうなのかな…みたいな部分もあります。
今ですと、スクリプト言語かクロスコンパイルが簡単にできるGo言語が良い選択肢でしょうか。
そういった具体的な部分に関しては時代の変化で合わなくなっている部分もありますが、その哲学自体は普遍的なもので、今読んでも、おそらくは10年後に読んでも得るものが多い本だと思います。</description>
    </item>
    
    <item>
      <title>fork関数がどうやってプロセスを分割しているか</title>
      <link>http://ota42y.com/blog/2015/01/03/unix-fork/</link>
      <pubDate>Sat, 03 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2015/01/03/unix-fork/</guid>
      <description>はじめてのOSコードリーディング ~UNIX V6で学ぶカーネルのしくみ
という本を読んでいます。
この中で、fork関数がどうやって子プロセスを作り、
親子かを識別して別の値を返しているのかが解説されており、
とても興味深かったです。
以下にその概要をまとめました。
fork関数 Cではfork関数を利用することで、子プロセスを作成することが出来ます。
コードとしてはこんな感じですね。
#include &amp;lt;stdio.h&amp;gt; #include &amp;lt;unistd.h&amp;gt; #include &amp;lt;sys/wait.h&amp;gt; int main() { pid_t pid = fork(); if (pid == 0) { sleep(1); printf(&amp;quot;child!\n&amp;quot;); return 0; } printf(&amp;quot;parent!\n&amp;quot;); int status; waitpid(pid, &amp;amp;status, 0); printf(&amp;quot;parent end\n&amp;quot;); return 0; }  子プロセスは親プロセスのデータをそのままコピーするため、変数などは全て同じ状態になります。
ですが、fork関数は親プロセスの場合は子プロセスのIDを、子プロセスでは0を返すため、
ユーザはfork関数の戻り値を見て、自身が親なのか子なのかを区別できるようになっています。
では、fork関数の中ではどのようにして、親プロセスか子プロセスかを判断し、
別の値を返しているのでしょうか。
これは(UNIX V6では)switch関数の仕様を上手く使った実装により実現されていました。
fork関数がプロセスの親子を区別する仕組み 親による子プロセスの作成 ライブラリのfork関数(source/s4/fork.s)を実行すると、
システムコールによってカーネルのfork関数(sys/ken/sys1.c)が実行されます。
fork() { register struct proc *p1, *p2; p1 = u.u_procp; for(p2 = &amp;amp;proc[0]; p2 &amp;lt; &amp;amp;proc[NPROC]; p2++) if(p2-&amp;gt;p_stat == NULL) goto found; u.</description>
    </item>
    
    <item>
      <title>大晦日ハッカソン2014&amp;正月ハッカソン2015に参加した</title>
      <link>http://ota42y.com/blog/2015/01/01/1231-hackathon/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2015/01/01/1231-hackathon/</guid>
      <description>2014/12/31にオンラインで行われた、
大晦日ハッカソン2014に参加しました。
Evernoteのリンクをアプリで直接開くChrome拡張を作りました。
evernote app opener - Chrome ウェブストア
(なお、細かい修正やこの記事の執筆等を、
正月ハッカソン2015でやりました)
どんなChrome拡張か 現在、Evernoteのノート固有のリンクを取得すると、
https://www.evernote.com/shard/文字列/文字列/ユーザID/ノートID/
といった、httpsのリンクが使われます。
このリンクを開くと、EvernoteのWebアプリ上でノートを開くことが出来ますが、
Evernoteはログインが最長で一週間しか保てず、また二段階認証をしている場合は、
頻繁に面倒なログインを求められます。
ですが多くの場合、開こうとしているノートはオフラインのEvernoteアプリでも開くことが出来ます。
そのため、Webではなくアプリで開く事が出来れば、わざわざログインせずともノートを開くことが出来ます。
そのため、上記のようなWebアプリでノートを開くようなリンクを、
オフラインのEvernoteアプリ上で同じノートを開くChrome拡張を作りました。
使い方 事前に、適当なノートのノートリンクを取得し、自分のユーザIDとshard名を取得する必要があります。
Chrome拡張のオプションページにあるように、ノートを右クリックし、ノートリンクを取得し、
そのURLに含まれているユーザIDとshard名をオプションページで設定してください。
その状態で、Evernoteのノートリンクを開くと、ログイン/非ログインに関わらず、
Evernoteアプリが立ち上がり、そのノートを開いてくれます。
これで、いちいちWebでログインすることなく、ノートリンクで指定したノートを表示することが出来ます。</description>
    </item>
    
    <item>
      <title>GithubのContributionsを使ってアウトプットを150日続ける</title>
      <link>http://ota42y.com/blog/2014/12/31/150-output/</link>
      <pubDate>Wed, 31 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/12/31/150-output/</guid>
      <description>というわけで、GithubのContributionsを利用して、
毎日何かしらのアウトプットを150日続けるということをやりました。
何故こんな事をしたのか 私はまじめ系クズな上に意志薄弱なので、やった方がいいようなことでも、
いろんな理由をつけてやらないのがいつものことでした。
特にインプットに重きを置く性質のため、いろいろなことを調べたりしますが、
アウトプットは何かしらの理由をつけて避けてしまう傾向があります。
実際、このブログにしたのは2013年9月ですが、そこから半年で1エントリのみ、
その後の3ヶ月も月に1回程度とほとんど書いてないに等しいです。
別にそのままでも問題は無かったのですが、せっかく色々インプットしているのに、
定着させないのはもったいないと思い、継続的にアウトプットする方法を模索しました。
継続的なアウトプットをどう実現するか 毎日継続的アウトプットを頑張る！と奮起しても、それが継続しないのが今回の問題です。
そのため、頑張る/頑張らないといった意思に関係なく続けられる、
もしくは歯磨きをせずに寝ようとした時のように、
継続してアウトプットしないと落ち着かないといった状態を目指します。
幸いにも、私の性格には続けたことを辞めるのを凄くもったいなく感じるというものがありました。
大学時代には、一日一冊づつ本を読んでいて、たまたま数十日続けたところで辞めるのがもったいなくなり、
結局1000日ぐらい毎日本を読み続ける、みたいな事をするぐらいには、
ある程度続いたものを途中で辞めるのを嫌う性格です。
この性質を利用し、最初のアウトプットを頑張って続けている状態から、
低いコストで継続してアウトプットを続けられれば、以降はそれを途中でやめるのがもったいなくなり、
継続的なアウトプットが出来るのではないか？と考えました。
おそらくは数日かけてじっくり練ったアウトプットよりかは質が落ちますが、
ほとんど出ない質のいいアウトプットより、継続的に低い質のアウトプットをした方が、
私にとっては良い影響になると考えました。
ルール設定 数日程度なら意思の力で何とかなるため、それを低コストで継続的に行いやすく、
かつ続けるうちに辞めるのが勿体ないようなルール設定を作ります。
1. アウトプットは外部に公開 自分だけにしか見えないところにアウトプットすると、
結局なんだかんだでやらない未来が容易に想像できるため、必ず外部公開します。
今回みたいに定期的に続けてますエントリを書くことで、万が一検証されたら嘘がばれる…という力が働き、
サボったのをごまかすことがなくなると考えました。
(検証されるかは重要ではなく、検証される可能性があるだけでサボり抑止には十分)
2. １日１コミット以上Githubにpush 毎日続けた事を記録するのはとてもとても面倒です。
ですが、Githubなら最初の画像のように、コミットがあった場合にそれを可視化して表示してくれます。
そのため、アウトプットをコミットすることでGithubにpushするだけで良くなり、
前述の外部公開するという条件も同時に満たせるためかなり敷居が下がります。
特に、プログラミング関係の作業がそのまま記録できたり、ブログをOctopressにしているため、
新しいエントリを書くのもコミットになっているという所も大きいです。
3. コミットを溜めるのは可 風邪を引いたり、仕事が忙しいときなどはどうしても何も出来ない場合があります。
そんな場合に、事前にブログの記事やコミット等を溜めておき、当日は公開するだけというのもありとしました。
今回の最大の目的は継続的なアウトプットであり、
毎日アウトプットするのは私の性格を利用して継続的アウトプットを成し遂げるための手段に過ぎません。
そのため、継続的なアウトプットが実現するならば、事前にそれを溜めておくのは良しとしました。
特にこの方法は途中で辞めるのがもったいないと感じるという所を利用して、継続的アウトプットを実現しているため、
一度でもそれが崩れると挽回は不可能であるためです。
また、私はゲームでもHPやアイテム、リソースを十分すぎるほどため込むのが好きな正確なため、
何日分の溜めがあると可視化されると、十分な量の溜めを確保しようと動くだろうという予想もしていました。
gitだと別の日のコミットも、git rebase &amp;ndash;ignore-dateでそのときのコミットに書き換えられるため、
溜めを作りやすい状態でもあります。
4. 意味のあるコミットだけカウント Githubだとpull-requestをマージするだけで1コミットが作られるため、ボタンを押すだけで1日分稼げてしまいます。
ですが、これは当初の継続的なアウトプットには繋がらないため、
最低限意味のあるコミットを行うのをルールとしました。
ただし、今回の目的は定期的なアウトプットであるため、アウトプットの質自体は考慮せず、
どんなに小さくてもバグ修正や、関数単位では出来ているものは1コミットとして考えるようにしました。
無意味なコミットは禁止するが、あまり価値のないコミットでも許可するといった具合です。
150日続けた結果 利点 誰かが「何かをやめる最良の方法は、別の何かを始めることだ」と言っていましたが、まさにそれを感じでいます。</description>
    </item>
    
    <item>
      <title>ASUS MeMO Pad 7 ME572CでLink2SDを動かす</title>
      <link>http://ota42y.com/blog/2014/12/30/link2sd/</link>
      <pubDate>Tue, 30 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/12/30/link2sd/</guid>
      <description>ME572CにはSDカードが挿せるため、
本体の容量が足りなくなっても気軽に容量を追加できます。
ですが、対応しているアプリしかSDカードにデータが保存できず、
対応しているアプリでも機種によっては、
上手くSDカードに入らない場合があります。
実際、私が使っているアプリですとEvernote、KindleがSDへのデータ保存が出来ず、
ニコニコ電子書籍アプリがGalaxy S5だとSDに保存できますが、ME572Cだと書籍データをSDに保存できません。
このような場合、Link2SDというアプリを利用することで、強制的にSDカードにデータを保存する事が出来ます。
ただし、Root化必須かつデータ部分をSDに移動するには有料アドオンが必要です。
なお、このアプリは通常の管理画面から無効化できないアプリを無効化する機能も有しています。
Link2SD Link2SDは、ほぼ全てのアプリをSDカードから読み込めるようにするアプリです。
おそらくは、アプリが参照するフォルダをシンボリックリンクを利用してSDカードの中に向けており、
アプリからの内蔵ストレージへのアクセスを、SDカードに飛ばしているのだと思われます。
SDカードの準備 このアプリを使うためにはSDカードに二つのパーティションを切る必要があります。
このうち1つめが普通のSDカード領域として認識され、
2つめのパーティションがLink2SDによってアプリが書き込まれる領域になります。
なお、2つめのパーティションがfat16もしくはfat32の場合、アプリのデータをSDカードに移動できません。
なお、Windowsユーザの場合、OSが一つのパーティションしか扱えないため、
Mini Toolのような専用ソフトでパーティションを作る必要があります。
このとき、どちらのパーティションもPrimaryに設定しておく必要があります。
パーティションのフォーマットは、端末によって上手くいく組み合わせとそうでない組み合わせがあるようです。
ネット上にはext2/ext2でいけるという情報が見られますが、ME572CではLink2SDが上手く認識しませんでした。
fat32/ext2もダメで、ext3/ext3だと第一パーティションは認識せず第二パーティションのみ認識、
NTFS/ext3だと両方上手く認識したためこれを利用します。
アプリをSDカードに移す Link2SDでシンボリックリンクを作成することで、アプリとデータをSDカードに移動できます。
ただし、データをSDカードに移動する場合は有料アドオンが必要です。
Link2SD上でアプリをクリックし、リンクを作成をすることでデータをSDカードに移動できます。
EvernoteやKindleではこのデータ部分が大きくなっていくため、実質必須になります。
ただし、画像の通り若干本体側にデータが残るため、完全に移動できるわけではありません。</description>
    </item>
    
    <item>
      <title>ASUS MeMO Pad 7 ME572CをRoot化した</title>
      <link>http://ota42y.com/blog/2014/12/28/memopad-root/</link>
      <pubDate>Sun, 28 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/12/28/memopad-root/</guid>
      <description>ASUS MeMO Pad 7 ME572Cを買いました。
タブレット - ASUS MeMO Pad 7 (ME572CL) - ASUS
Nexus 9とかと迷いましたが、
 コストパフォーマンスがいい  安くてそこそこ高性能  SDカードが使える  気軽に容量増強出来る   といった点から、これに決めました。
ですが実際に届いて起動してみたところ、
余計なアプリ(Flipboard、Yahoo、謎のタスクアプリ等)が入っていました。
特に「やることリスト」(ASUS Do It Later)というアプリは、
定期的にGoogleアカウントにアクセスしようとする上に無効化もできず、かなり邪魔なアプリになっています。
そのため、Rootを取って無効化することにしました。
Root化手順 RootZenFoneというアプリが、1.4.6.6r以降でME572Cに対応したため、これを使います。
(2014/12/28の最新版は1.4.6.8r)
手順としては以下の通りになります。
 RootZenFoneをインストール Wifiと3Gを機内モードにして切る RootZenFoneを起動 通信切れ＆危ないよ警告が出てくるので、通信が切れている事を確認して画面下のOKボタンを押す 変化がなくなるまで待つ  色々ポップアップが出たりする  もう一度RootZenFoneを起動し、rebootしろと出ているのを確認して再起動  この時点でRoot化出来ています  AsusLiveDemoというアカウントが追加されているため削除 RootZenFonを消す 通信を有効か SuperSuが自動でインストールされているので、最新版に更新 SuperSuを起動し、データ更新後に再起動を促されるので再起動  Root化ってこんなに簡単だっけ？とういぐらい簡単にできてしまいます。
あとは適当な無効化アプリで対象のアプリを無効化orアンインストールすればおしまいです。</description>
    </item>
    
    <item>
      <title>pixivのタグから読み解くラブライブイラスト投稿者分析</title>
      <link>http://ota42y.com/blog/2014/12/27/lovelive-age/</link>
      <pubDate>Sat, 27 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/12/27/lovelive-age/</guid>
      <description> この記事は2014年版で、2015年をまとめた版があります
(2015年のpixiv内ラブライブイラストの推移)
昨日、一昨日とラブライブイラストについて見ていきましたが、
最後にイラストを投稿している投稿者さんについて見ていきます。
投稿者の性別傾向 集計方法 キャラのイラストと、ラブライブ関連イラストを投稿した人全員に対して、性別と年齢を調べてまとめました。
なお、各データはユーザページで公開されているものを利用したため、
全体に情報を公開している人のみの集計になります。
各キャラ毎の性別傾向 各キャラ毎に、男性と女生徒でどれくらいの割合で投稿しているかを集計しました。
なお、キャラ毎に投稿数ユーザ数がそもそも違うため、
そのキャラに投稿した人のうち性別が判別できるユーザがどれくらいの割合なのかを集計しています。

ラブライブタグ全体では女性投稿者の方が多い結果になりました。
キャラ毎では、男性投稿者が多いキャラが５人に、女性投稿者が多いキャラ人気が４人と、
μ&amp;rsquo;sメンバーの中でも男女のユーザ数が半分に別れています。
特に男性からの投稿が多いのは高坂穂乃果、園田海未の二名です。
女性からの投稿が特に多いのは、南ことり、矢澤にこの二名になります。
また、星空凛は男女共にほぼ同数の投稿人数になります。
なお、昨日の記事でも紹介した、閲覧数の多いカップリングである、
「にこまき」「のぞえり」「ことうみ」「りんぱな」ですが、
カップリングの片方ずつで男性投稿者と女性投稿者の投稿数が割れています。
男性女性、それぞれから支持を得るキャラ同士のカップリングだから人気だったりするのでしょうか？
カップリング毎の性別傾向 前回の記事で集計した総閲覧数1,000,000以上のカップリングに対して、
性別ごとに投稿者の割合を集計しました。

人気のカップリングでは女性投稿者が多いことがわかります。
また、キャラごとの傾向では片方が6割を超えることはありませんでしたが、
カップリングごとでは6割超えが多く見られ、かなり偏りがあります。
なお、キャラ別に比べて有効なユーザ数が少ないため、
必ずしも正確な調査ではない部分にご注意ください。
ラブライブ投稿者の年齢層 集計方法 ラブライブのイラストを投稿した全ユーザから、年齢を公開しているユーザを集計しました。
各キャラ毎に年齢別にユーザ数を出していましたが、
ほぼ全キャラ同じ傾向になったため、ラブライブイラスト全体で出すことにします。
集計結果 有効なデータ数は男性3665、女性2771、性別不明765の7201件でした。
なお、10才から5才区切りで集計しています。

20〜25才未満が最も投稿している結果になりました。
そこから+-5才が二番目に多いすが、25才以上は男性がほとんどなのに対し、
20才未満は女性が多くを占めています。
そのため、20代男性と、15〜25才の女性投稿者が多いようです。
まとめ  ラブライブイラスト全体では女性投稿者の方が多い  キャラ毎では、男性の投稿者に人気、女性の投稿者に人気がわかれている 人気の4カップリングは、男性人気+女性人気の組み合わせキャラだった  カップリングイラスト投稿者は女性投稿者の方が多い  投稿者の男女別人気も、キャラのそれよりも偏りが大きい  投稿者は男女で若干世代が違う  女性は15〜25才、男性は20〜30才  最も投稿が多いユーザは一人で250件以上投稿していた  そのほぼ半分に西木野真姫タグがついており、同キャラの最多投稿ユーザ   </description>
    </item>
    
    <item>
      <title>pixivのタグから読み解くラブライブカップリング傾向(2014年11月現在)</title>
      <link>http://ota42y.com/blog/2014/12/26/lovelive-cp/</link>
      <pubDate>Fri, 26 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/12/26/lovelive-cp/</guid>
      <description>この記事は2014年版で、2015年をまとめた版があります
(2015年のpixiv内ラブライブイラストの推移)
昨日はラブライブタグ全体と、各キャラについて見ていきましたが、
今日はカップリングについて見ていきます。
なお、既に調べられていたので、半分ぐらいはその内容のアップデートになります。
pixiv のタグ情報を用いた「ラブライブ！ School idol project」のカップリングネットワークの構築
収集方法 pixivのタグから、各カップリングタグ（「にこまき」、「のぞえり」等）とラブライブタグがついているものを収集し、 そのイラストの閲覧数の和を集計しました。
条件にラブライブタグを含めないと、高坂雪穂と星空凛の「ゆきりん」タグで別のゆきりんが引っかかる等、
ラブライブとは関係のないものも含まれてしまうため、このような条件にしています。
また、「まきりんぱな」や「ことほのうみ」といった３人を含むタグについては、
「まきりん」と「りんぱな」として集計することはせず、結果から除外しています。
なお、元となったエントリの詳しい収集方法がわからないため、
単純な数値比較は出来ませんのでご注意ください。
順番を加味しない場合 「にこまき」と「まきにこ」を同一視する場合です。
収集結果 (クリックで大きく見られます)

にこまきの人気は相変わらず圧倒的です 時点がのぞえりなのも前回から変わりません。
ですが、３番目のことうみが、前回よりも他を大きく引き離してきています。
また、つばほのがマイナーカップリングを凌ぐほどの人気になっていたり、
ゆきありが地味に伸びていたりと、メインキャラ以外の部分にも多少変化があります。
グラフ 1,000,000以上のみ 元記事に習い、1,000,000未満の辺がﾐﾄﾒﾗﾚﾅｲ場合、組み合わせはこのようなグラフになります。

前回は生徒会グループと、広義の一年生グループとに別れていましたが、
ほのまきとのぞにこの人気が上がったことにより、全てが繋がりました。
また、高坂穂乃果は4種類のカップリングがあり、最もカップリングの種類が多いキャラになっています。
100,000以上のみ 100,000未満の辺がﾐﾄﾒﾗﾚﾅｲ場合は以下のようなグラフになります。

西木野真姫、絢瀬絵里、高坂穂乃果に線が集中し、りんぱなにはそれほど線がありません。
また、高坂雪穂と絢瀬亜里沙はどちらも実の姉とは繋がっていないのがわかります。
最もカップリングが多いのは、μ&amp;rsquo;sメンバー全員と繋がっている西木野真姫と、
8人＋綺羅ツバサと繋がっている高坂穂乃果になります。
順番を加味する場合 「にこまき」と「まきにこ」を別物として数えます。
集計結果 行＋列のカップリングスコアになります。
(クリックで大きく見られます)

複数のイラストを含んだ投稿などで、「のぞえり」と「えりのぞ」が両方つけられるような場合があり、
前述の順番を加味しない場合と、順番を加味した場合の和は異なっています。
基本的には「のぞにこ」と「ほのまき」が増えた以外は前の結果とほぼ変わりませんでした。
なお、前回の結果と比べて「まきりん」が減少していますが、
不審に思って調べたところ、「まきりん」タグ自体の登録数は少なく、
そもそも前回の結果のような大きい値になることは考えにくいです。
一方で、タグを完全一致ではなく部分一致にし、「まきりん」の集計に「まきりんぱな」も入れると、
似たような大きな値になるため、おそらくはこれが原因だと思われます。
グラフ 1,000,000未満の辺がﾐﾄﾒﾗﾚﾅｲ場合、グラフは以下のようになります。

南ことり、星空凛は常に先に来ているのに対し、西木野真姫、園田海未は誰が相手でも常に後に来ています。
また、「のぞえり」と「えりのぞ」だけがお互いに辺を作っています。
一番人気の「にこまき」の方向がほぼ固定なのと比較すると、かなり特徴的です。
なお、これ以下の閾値やR-18作品の結果などは前回と大きな変化が見られなかったため、省略します。
カップリングイラスト投稿数推移 カップリングは全部で72通りあるため、投稿数1,000,000以上のカップリングのアニメ化後の推移をまとめました。
なお、月ごとの投稿数で調べたところあまりにも乱高下が激しく、わかりづらいグラフだったため、
月ごとの累計投稿数で集計を行いました。
投稿数1,000,000以上 一期放映直後から、にこまきの投稿数が圧倒的であることがわかります。</description>
    </item>
    
    <item>
      <title>pixivのタグから読み解くラブライブイラストの歩み</title>
      <link>http://ota42y.com/blog/2014/12/25/lovelive/</link>
      <pubDate>Thu, 25 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/12/25/lovelive/</guid>
      <description>この記事は2014年版で、2015年をまとめた版があります
(2015年のpixiv内ラブライブイラストの推移)
⊂(・8・)⊃＜776O776J7722776B772s772n772n772n776dIQ==
さて、ラブライブ！ Advent Calendar 2014最終日は、
pixiv上のラブライブのイラストについて見ていこうと思います。
内容としては以下になります。
 pixiv上のラブライブイラストの推移(本記事) pixivのタグから読み解くラブライブカップリング傾向 pixivのタグから読み解くラブライブイラスト投稿者分析  なお、一つの記事にするには長すぎたため、
3記事に分割しています。
今日はpixiv上のラブライブイラストの推移について書いていきます。
集計方法 各キャラ毎のイラストに関しては、そのキャラの本名、
もしくは別名+ラブライブタグがついたイラストを集計しています。
(ことりちゃんや凛ちゃん、のんたん等、ラブライブ以外の同名タグを含めないため)
ラブライブ！タグとしては、ラブライブ！およびLoveLive!といった作品名のタグのついたイラストと、
各登場人物のイラストを重複排除して集計しました。
（ラブライブ！タグのついてないキャラのイラストもあるため）
集計期間は、ラブライブイラストが初めて投稿された2010年8月〜2014年11月になります。
年間イラスト数の推移 
一期放送の2013年に投稿数が増えた後、二期放送の2014年でさらに4倍近くイラスト数が伸びています。
ご覧の通り、アニメ化以前と以後とでは、投稿数が100倍ぐらい差があるため、
ここから先はアニメ化以前とアニメ化以後、およびキャラ毎の3種類に分けて見ていきます。
アニメ化以前の月刊イラスト投稿数 
各シングル発売のたびにイラストが増加しています。
1stシングル発売後は一度投稿数が落ち込みましたが、
2ndシングル「Snow halation」が発売してからは安定した投稿数を確保しています。
その後も、3rd、4thと着実に投稿数が伸びています。
4thシングル&amp;amp;1stライブ&amp;amp;アニメ化発表の時の盛り上がりが凄いですが、
同時に3月まで5thシングルの総選挙があったため、3月の投票数も伸びています。
なお、この月の誕生日の園田海未を除くと、
南ことり、矢澤にこ、西木野真姫の3人が他のキャラと比べて増加していましたが、
総選挙の上位三人になっており、データ数が少ないので偶然かもしれませんが、ちゃんと対応が取れていました。
ちなみに、5thシングル前の7月にも山が見られますが、調べたところにこの聖誕祭効果でした。
アニメ化以降の月刊イラスト投稿数 
アニメ化以前は月間投稿数が100を超えることはありませんでしたが、
アニメ化以降一気に投稿数が伸び、放送終了後以降は月刊投稿数が1000を切ることがないぐらいにまで成長しました。
投稿数の増加は夏コミまで続きますが、夏コミ(C84)が終わった後はいったん沈静化します。
ですが、12月の冬コミ(C85)と3rdのDVD/BD発売から再び火がつき、
2月のライブにかけて再びイラスト数が増加していきます。
特に、ライブ直後の二週間だけで投稿数が2000を超すなど、二期を直前にして盛り上がりは最高潮に達しています。
二期が放映されてからは一期以上のペースで伸びていき、
一期と同じく夏コミ(C86)まで増加して一度沈静化しました。
今月は冬コミ(C87)が控えているため、おそらくそれによってまた増加すると思われます。
学年ごとの集計 一週間筒キャラ毎にまとめたのを用意したので、もう少し細かく見ていくことにします。
なお、各キャラ共に誕生日とその次の日にかけて、聖誕祭イラストとして投稿数が増加します。
そのため、キャラの誕生日と次の日を別の週として集計すると、実態と集計結果がずれてしまいます。
幸いなことに、投稿数が増えた2013年〜2014年の間で水曜日が誕生日のキャラはいないため、
木曜〜水曜の一週間で集計し、誕生日と次の日が分割されることを回避しています。
1年生 
これは全てのキャラに言えることですが、各キャラの誕生日ごとにスパイクが発生し、
イラスト数が増加しているのがわかります。
また、こちらも各キャラ共通ですが、4thライブ直後にもイラスト数が増加しています。
キャラとしては、アニメ化前からちょくちょく人気でしたが、やはり西木野真姫の投稿数は安定しています。
また、2013年はイラスト数が少なかったため、矢澤にこの聖誕祭でにこまきカップリングとして、
投稿数が伸びていました。 なお、2014年はイラスト数が増えたため、相対的に影響が小さくなっています。</description>
    </item>
    
    <item>
      <title>Graphvizを使うと、グラフ描画がとても楽になる</title>
      <link>http://ota42y.com/blog/2014/12/22/graphviz/</link>
      <pubDate>Mon, 22 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/12/22/graphviz/</guid>
      <description>こんな感じの画像を作る際に
これまではパワポの図形機能とかペイントで頑張って作っていましたが、
Graphvizを使うと自動で出力できます。
Graphvizとは AT&amp;amp;Tが作ったグラフ描画のツールパッケージです。
http://ja.wikipedia.org/wiki/Graphviz
DOT言語というグラフ記述言語で記述されたグラフを、画像ファイル等に変換することができます。
グラフのレイアウト等はGraphviz内のアルゴリズムによって自動で配置されますが、
SVGで出力できるため、他のソフトで調整ができます。
使い方 インストール brew gts graphviz
dotファイルの作成 最初に見せたグラフは以下のように作ります。
digraph G { Hono[image=&amp;quot;th_hono.png&amp;quot;, label=&amp;quot;&amp;quot;, shape=plaintext]; Koto[image=&amp;quot;th_koto.png&amp;quot;, label=&amp;quot;&amp;quot;, shape=plaintext]; Umi[image=&amp;quot;th_umi.png&amp;quot;, label=&amp;quot;&amp;quot;, shape=plaintext]; Maki[image=&amp;quot;th_maki.png&amp;quot;, label=&amp;quot;&amp;quot;, shape=plaintext]; Rin[image=&amp;quot;th_rin.png&amp;quot;, label=&amp;quot;&amp;quot;, shape=plaintext]; Pana[image=&amp;quot;th_pana.png&amp;quot;, label=&amp;quot;&amp;quot;, shape=plaintext]; Nico[image=&amp;quot;th_nico.png&amp;quot;, label=&amp;quot;&amp;quot;, shape=plaintext]; Nozo[image=&amp;quot;th_nozo.png&amp;quot;, label=&amp;quot;&amp;quot;, shape=plaintext]; Eli[image=&amp;quot;th_eli.png&amp;quot;, label=&amp;quot;&amp;quot;, shape=plaintext]; Yuki[label=&amp;quot;雪穂&amp;quot;]; Nico -&amp;gt; Maki; Koto -&amp;gt; Umi; Rin -&amp;gt; Pana; Rin -&amp;gt; Maki; Nozo -&amp;gt; Eli; Nozo -&amp;gt; Nico; Koto -&amp;gt; Hono[dir = none]; Hono -&amp;gt; Umi; Hono -&amp;gt; Nico; Hono -&amp;gt; Maki; Hono -&amp;gt; Yuki; Hono -&amp;gt; Eli; Hono -&amp;gt; Rin; }  初めにレイアウトに使うアルゴリズムを書きます。</description>
    </item>
    
    <item>
      <title>参加記録 Android Bazaar and Conference 2014 Winter</title>
      <link>http://ota42y.com/blog/2014/12/21/abc2014w/</link>
      <pubDate>Sun, 21 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/12/21/abc2014w/</guid>
      <description>Android Bazaar and Conference 2014 Winterに参加してきました。
なかなか興味深いお話を聞けたので、気になった部分を抜粋
 携帯は９５％の人類が持ってる
 インターネットを利用するのは４割 スマホ利用者は３２％ スマホは１０年前の携帯と同じ水準まで来てる -１０年後は世界の携帯がスマホに置き換わる 今後１０年間で７０億人がインターネットに繋がる Androidは今のところ全世界で84.4%のシェア  各国のもの作り政策
 ドイツのINDUSTRIE 4.0 第四次産業革命 多数の企業を結んだバリューネットワーク 工場内の結合されたネットワーク　は出来てる
 アメリカ
 AMP立ち上げ
 アメリカで発明し、アメリカで製造する
 Made in Americaの復活
 中国
 2008年以降世界の製造業売り上げTop
 EU全体を合わせて、ようやく中国をちょっと上回る  第１２期５カ年計画
 ５年で１６８兆円使う IoTを重視する   Googleが買収したNest
 http://japan.cnet.com/news/commentary/35042541/ ネットに繋がるサーモスタット  Android Wear
 WearのNotificationはアプリの顔になる ノイジーな通知をするとアンインストールされる リストから選んで起動するのではなく、通知によってしかるべきタイミングで起動する わざわざ起動するのは減っていく   </description>
    </item>
    
    <item>
      <title>GoのORマッパーGORMが便利</title>
      <link>http://ota42y.com/blog/2014/12/19/gorm/</link>
      <pubDate>Fri, 19 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/12/19/gorm/</guid>
      <description>golangではmysqldriverでmysqlにアクセスできますが、
一つ一つ構造体に入れないといけなかったりと、けっこう辛いものがあります。
goでmysqlを使う
そこでいろいろ探していたところ、
ActiveRecordのように構造体を使ってDBにアクセスできるORMがありました。
https://github.com/jinzhu/gorm
自動でテーブル作ってくれたり、変更してくれたりと、他のORマッパーよりかはActiveRecordっぽいです。
リレーションも勝手に貼ってくれるみたいです。
ただし、取り出すときは元のオブジェクト→リレーションのオブジェクトと、
順に取ってくる必要があり、自動でリレーション先のオブジェクトの取得はしてくれるわけではありません。
(使わない場合は無駄なアクセスになるので、正しいと言えば正しいですが)
package main import ( &amp;quot;fmt&amp;quot; &amp;quot;github.com/jinzhu/gorm&amp;quot; _ &amp;quot;github.com/lib/pq&amp;quot; _ &amp;quot;github.com/go-sql-driver/mysql&amp;quot; ) type User struct { Id int64 Name string `sql:&amp;quot;size:255&amp;quot;` Emails []Email // One-To-Many relationship (has many) } type Email struct { Id int64 UserId int64 // Foreign key for User (belongs to) Email string `sql:&amp;quot;type:varchar(100);&amp;quot;` // Set field&#39;s type } func main(){ db, err := gorm.Open(&amp;quot;mysql&amp;quot;, &amp;quot;root@/testdb?charset=utf8&amp;amp;parseTime=True&amp;quot;) fmt.Println(err) db.</description>
    </item>
    
    <item>
      <title>golangでYAMLファイルを読み込んで構造体に入れる</title>
      <link>http://ota42y.com/blog/2014/12/03/go-yaml-struct/</link>
      <pubDate>Wed, 03 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/12/03/go-yaml-struct/</guid>
      <description>使い方がかなり特殊だったのでメモ
(ドキュメントには書いてありますが…)
goyamlでは、YAMLの構造とGoの構造体の構造を揃えておくと、
データを構造体にセットした状態で読み込むことが出来ます。
特にGoでは構造体を使わない場合、interfaceへの変換を書きまくる事になるので、
できる限り構造体を利用した方がお勧めです。
以下のように、YAMLのキーとGoの構造体の名前を揃えることで、
YAMLから構造体に直接データを代入できます。
type Data struct { UserId int UserName string `yaml:&amp;quot;user_name&amp;quot;` Follownum int `yaml:&amp;quot;followNum&amp;quot;` MessageText string invaliddata string }  userid: 123 user_name: name followNum: 42 messageText: text invaliddata: data  後述するコードでYAMLを読み込むと、出力は以下の通りになります、
=&amp;gt; {123 name 42 }
UserIdに123、UserNameにname、Follownumに42、
MessageTextとinvaliddataは空になっています。
構造体とYAMLの対応付け仕様 特に指定をしない場合、構造体の変数名に対応するキーと対応付けられます。
対応するキーは以下のような仕様になっているようです。
 指定が無い場合、変数名を全て小文字にしたYAMLのキーと対応付ける  UserIdはuseridと対応付けられます  後述する方法で明示的な指定をしない限り、YAMLのキーは全て小文字のみ受け付けます  messageTextはダメで、messagetextでないといけません  構造体のメンバは大文字から始まる  そのため、invaliddataにはデータ入っていません 大文字から始まれば、途中が大文字でも大丈夫です UserIdはuseridと対応付けられます 途中を大文字にしても、全て小文字のキーを見に行きます MessageTextはmessagetextと対応付けられます  明示的に対応を設定することもできる  UserNameをuser_nameと対応付けたり(通常はUser_Nameというメンバ変数にしないといけない) Follownumの変数をfollowNumと対応付けるなど、上記の制限は無くなる   暗黙のルールが多いですが、それさえ理解すればかなり簡単に書くことができます。</description>
    </item>
    
    <item>
      <title>参加記録 Go Conference 2014 autumn</title>
      <link>http://ota42y.com/blog/2014/12/01/gocon-2014-autumn/</link>
      <pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/12/01/gocon-2014-autumn/</guid>
      <description>Go Conference 2014 autumnに参加しました。
togetterまとめ
大体スライドが公開されているので、正確な内容はそちらをどうぞ。
#まとめ - Go言語のシンプルさへのこだわりが凄い - 開発陣の徳の高さが凄い - 難しい部分は俺らに任せておけ！的な - 明日から使えるGo言語的な情報が盛りだくさん - エラーを_で無視して済みません…(´･_･`) - 椅子が痛い - 1時間半じゃなくて、1時間ごとぐらいに休憩を挟んでほしかったです… - 英語頑張ろう - 日本語スライドありがたや…
#Keynote1: Rob Pike (@rob_pike) (45min) スライドは未公開？
Go言語の設計者ロブ・パイクさんのGo言語の思想とかについての話です。
Go言語がいかに単純さ(≒簡潔さ)を重要視しているかについてとても示唆のある話をしていただけました。
他の言語が相互に機能を取り込み、ほぼ同じ機能を持つように進化していっているのに対し、
Goは1.0の時点で言語の機能を固定し、機能をとても少なく持つようにしたそうです。
書く楽しさはなくなるけど、代わりに保守のしやすさを選んだとのことです。
プレゼンの中で特にハッとさせられたのは、単純なコードと簡潔なコードとは異なるということです。
言語の機能を使って、数行程度でいろんな事に対応しようとすると、コードの量自体は少なくて済みますが、
必要とする前提知識が増え、かつその数行を完全に理解するのにとても時間がかかります。
おそらく、適当な言語のワンライナーを理解するのに必要な知識と時間を想像すれば大体わかると思います。
簡潔な記述でも理解するのが大変な複雑な事ができるため、
簡潔なコード≠単純なコードと言えるのかなと思いました。
また、Goではコード側ができる限り簡単になるように、
複雑な事を可能な限り言語側で隠蔽しているとも言われていました。
実際、GoのGCや並行処理、パッケージなどは設計や実装自体は凄く複雑にも関わらず、
使う側からはそれほど大変さを感じることなく使えます。
ここ（複雑な部分）は俺にまかせろーといった感じで、開発者の方々には頭が下がります…
#Keynote2: Goに入ってはGoに従え @fumitoshi_ukai (45min) 資料
Google社内でGo Readability Approverをされている@fumitoshi_ukaiさんの発表です。
Go言語らしく書くにはどうすればいいのか？といった思想的な部分と、
ダメな例と良い例を挙げてひたすら赤ペン先生をする発表でした。
これがGo言語のやり方か！となりっぱなしで、まさに明日から使えるGo言語といった感じなので、
是非ともスライドが公開されるといいなーと思います。
LT1 Gardener &amp;amp; Go 資料
@nuki_ponさんが某位置情報ゲームの色をした、GoCon Tシャツを作ってくださったそうです。
GoCon 2014 Autumn Tシャツ | FreeGufo メール便対</description>
    </item>
    
    <item>
      <title>Octopressのアップデート手順</title>
      <link>http://ota42y.com/blog/2014/11/30/octopress-update/</link>
      <pubDate>Sun, 30 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/11/30/octopress-update/</guid>
      <description>見た目は変わっていませんが、このブログのOctopressをアップデートしました。
だいぶ前のOctopressをforkして、このブログ用の変更をコミットしていったため、
forkして手を加えたリポジトリにfork元の修正を取り入れる事になったので、やりかたをメモしておきます。
ブログ用リポジトリの状態 このブログは現在Github Pagesで運用しており、リポジトリはこちらになります。
https://github.com/ota42y/ota42y.github.io
ですが、これはOctopressの出力先のリポジトリであり、
bitbucket上にOctopress自体のリポジトリが存在します。
このリポジトリはだいぶ前のOctopressをベースに、このブログ用の修正や記事をコミットしていました。
Octopressのアップデート 普段はOctopress側のリポジトリは使わないため、remoteからも削除してあるのでそれを入れます。
その後、Octopress側のmasterとマージをするだけになります。
git remote add octopress git@github.com:imathis/octopress.git git fetch octopress git merge octopress/master  その後、bundle installすれば終了…のはずでしたが、1つ落とし穴がありました。
Jekyllがアップデートされたため、日付を出力するdate_time_htmlが変更されていました。
https://github.com/imathis/octopress/pull/1643/files
そのため、これを直すことで、無事アップデートは終了になりました。
date_time_htmlが無くなったことはすぐに発見できましたが、
このメソッドを持っていたオブジェクトが何なのかの発見に手間取り、結構時間がかかりました。
Rubyの特徴上、変数にあらゆるオブジェクトが入る事があるため仕方ないのですが、
その変数が何のオブジェクトなのかを簡単に調べる方法がほしいですね…</description>
    </item>
    
    <item>
      <title>参加記録 BPStudy ♯87</title>
      <link>http://ota42y.com/blog/2014/11/29/bpstudy-87/</link>
      <pubDate>Sat, 29 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/11/29/bpstudy-87/</guid>
      <description>BPStudy#87に参加しました。
togetterまとめ
じっくりと把握する iOS8 / iPhone6 最前線〜ユーザーにとって、開発者にとって何が変わったのか？ http://www.slideshare.net/yukio.andoh/bpstudy-87-ios8-iphone6
iOS8とiPhone6の新機能まとめと、それによって生じる問題紹介でした。
スライド46ページ目の親指の可動範囲の図を見せられると一目瞭然ですが、
画面サイズが大きくなったことによるタッチ領域の変化はとても激しく、
iPhone6や6+では従来のUIはかなり使いづらいものになってしまいます。
あと、リモートで任意の端末をレンタルできるRemote TestKitが凄い便利そうでした。
Remote TestKit
iPhone 6がリリースされて2ヶ月が過ぎました　〜iOSの分断化時代を乗り切ろう〜 http://www.slideshare.net/masashiono1/bpstudy-87
akisute/AutoLayoutTest
前半のスワイプUIのススメは、前の発表のiPhone6や6+向けのUIの回答として、
かなり良い回答ではないかなと思いました。
あと後半のSwiftのまとめが、Swiftの闇をひしひしと感じる内容で凄いです…(:3 」∠)
LT 一件目はiOS7と8で遭遇したブラウザバグの紹介でした。
資料
画像を範囲指定で切り抜くと何故か指定したサイズより大きくなるとか、
同時接続数が2個になったが、レスポンスの受信時間がかぶるとクラッシュするので同時接続してはいけないとか、
かなり辛い感じのバグが多かったです…
二件目ではペアプロ合コンに参加した人が、相手側の女性の手によって公開処刑される様子を見物していました…
恐ろしい…(((;ﾟДﾟ)))ｶﾞｸﾌﾞﾙｶﾞｸﾌﾞﾙ</description>
    </item>
    
    <item>
      <title>Jenkinsのbuild flow pluginを使うとjobの設定管理が少し楽になる</title>
      <link>http://ota42y.com/blog/2014/11/27/jenkins-build-flow-plugin/</link>
      <pubDate>Thu, 27 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/11/27/jenkins-build-flow-plugin/</guid>
      <description>まとめ  Jenkinsのjobの設定管理はリポジトリのバージョンと揃えないと行けないため面倒 全てをスクリプトで実行するのが理想だが、本体の機能を使いたい場合は対応できない build flow pluginがファイルを読み込んで実行できるようになった これにより、Jenkinsのjobを呼び出すスクリプトをリポジトリ内に入れてバージョン管理出来る  Jenkinsのjob設定問題 Jenkinsの大きな問題の一つは、Jenkinsのjob設定をどう管理するかだと思います。
例えばビルド手順を変更する場合、プラグイン設定に後方互換性がないような変更を行うと、
前のバージョンをビルドしたときにエラーになります。
このような場合、通常は最新の変更を取り込む事で解決しますが、
コードフリーズ中のリリースブランチのように、最新の変更を取り込めない場合はこの方法で解決できません。
このような場合、二通りの解決方法が存在します。
全部スクリプトで処理する方法 一つ目が全部スクリプトでやってしまう方法です。
Jenkinsのjobからは単一のスクリプトだけを実行し、その中で全てを行います。
この方法はJenkinsを単なるcronとしてしか使わないため、
全部自分でやる必要がありますが自由度がとても高いのが特徴です。
ですがこの方法では、プラグインや下流jobとの連携、特定の処理だけ別ノードで実行するなど、
Jenkinsの機能が使えなくなります。
新しいJobを作っていく方法 これに対し、jobの変更管理を諦め、どんどん新しいjobを作ってく方法があります。
Jenkinsはjobのコピーが容易なため、後方互換性のない変更を加える段階で新しいjobに切り替え、
以降はそちらでビルドし、過去のバージョン用のビルドが必要なときには残してある前のjobを実行します。
この方法の場合、Jenkinsの機能を利用しつつ、複数のビルド設定を同時に扱うことが出来ますが、
どのバージョンでどのjobを動かせばいいかを保存しておけないため、jobが増えていくと問題になります。
これに対し、build flow pluginを使う事で、どのjobでビルドするかといった情報を、
リポジトリ内に入れてバージョン管理することが出来ます。
build flow pluginでjobの関係をリポジトリに入れる Jenkinsのbuild flow pluginでは、複数のjobをスクリプトから実行することが出来ます。
Build Flow Plugin
具体的には以下のように書くことで、job1が成功したらjob2を実行するといったことが出来ます。
// 失敗から成功に変わったか判定 def isFixedBuild(result){ prev = build.previousBuild return prev.result != SUCCESS &amp;amp;&amp;amp; result == SUCCESS } // job1を実行 b = build(&amp;quot;job1&amp;quot;) // job1のログを出力する out.println b.getLog() if(b.result == SUCCESS){ // job1が成功していたらjob2を実行 b = build(&amp;quot;job2&amp;quot;) if (isFixedBuild(b.</description>
    </item>
    
    <item>
      <title>Dockerとは何か</title>
      <link>http://ota42y.com/blog/2014/11/26/docker/</link>
      <pubDate>Wed, 26 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/11/26/docker/</guid>
      <description>Dockerについて調べたので概要と簡単な使い方のメモ
Dockerとは何か いわゆるコンテナ型仮想化ツールの1つです。
VMwareやKVMのような完全仮想化と違い、
ユーザや、ネットワーク、ファイルシステムなどを隔離した環境を作ってそこで動くらしいです。
あくまで隔離された環境のため、環境ごと作る完全仮想化に比べてオーバーヘッドが少なく、
手元の数年前のmac book airでもかなり快適に動いています。
あくまでゲストOSを隔離環境に置いただけのため 、
ファイルシステムは分離されておらず、ホストからは中身が見えるのも特徴らしいです。
(ゲストからはホストの中身は当然見えない)
また、DockerではDockerfileというファイルに設定を書くことで、
その設定を反映した状態のコンテナを作成することが出来ます。
そのため、OSの状態をDockerfileとして管理でき、いわゆるimmutable infrastructureと相性がいいみたいです。
Docker上のCent OSにJenkinsを立てる Dockerの起動 Dockerコマンドを実行するためには、Macだとboot2dockerを
boot2docker startで起動し、そのとき提示された環境変数を設定する必要があります。
また、
boot2docker ip
を実行し、コンテナに対してアクセスできるようにする必要があります。
なお、この時出たURLは後で使います。
Cent OSの構築 Dockerfileの準備 以下のファイルをDockerfileという名前で適当なディレクトリに保存します。
FROM centos:centos6 RUN curl -o /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat/jenkins.repo RUN rpm --import http://pkg.jenkins-ci.org/redhat/jenkins-ci.org.key RUN yum -y install java-1.7.0-openjdk jenkins  見ての通り、Jenkins公式の入れ方のコマンドを実行しているだけです。
コンテナの作成 次に、Dockerfileを保存したディレクトリで
docker build -t jenkins . を実行し、カレントディレクトリのDockerfileを使って、jenkinsというタグをつけてコンテナを作成させます。
コンテナの起動 docker run -p 8080:8080 -i -t jenkins bash
を実行してホストの8080とコンテナの8080を結びつけ、先ほど作ったjenkinsコンテナにbashで入り、</description>
    </item>
    
    <item>
      <title>YAMLでnilをキーにしたハッシュを扱う</title>
      <link>http://ota42y.com/blog/2014/11/25/ruby-hash/</link>
      <pubDate>Tue, 25 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/11/25/ruby-hash/</guid>
      <description>例は全てRuby 2.0を利用しています
nilをキーとした値を持つハッシュをYAMLに書きたい場合、
以下のように書いても&amp;rdquo;nil&amp;rdquo;という文字列として認識されます。
--- nil : nil  =&amp;gt; {&amp;quot;nil&amp;quot;=&amp;gt;&amp;quot;nil&amp;quot;}
データがnilの場合は、データ部分に何も書かないことでnilを表現できます。
--- datanil :  =&amp;gt; {&amp;quot;datanil&amp;quot;=&amp;gt;nil}
キーをnilにしたい場合、以下のように書いても、(Rubyだと)パースに失敗します。
--- : &amp;quot;key nil&amp;quot;  このような場合もYAMLの仕様では想定済みらしく、
クエッションマークを使うことで、その後ろにあるものがキーであると明示できる仕様があります。
http://yaml.org/spec/1.2/spec.html#id2772075
これを利用して、以下のようにクエッションマークの後に何も書かず、
その後コロンと値を設定することで、nilをキーとして設定できます。
--- ? : &amp;quot;key nil&amp;quot;  rubyで長い文字列をキーにする Rubyが利用しているPsychでは、以下のように128byte以上のデータをキーにして書き出した場合、
?マークをつけて書き出します。
require &#39;yaml&#39; def mkhash(k,v) h = {} h[k] = v h end long = &amp;quot;a&amp;quot; * 129 open(&amp;quot;long.yml&amp;quot;, &amp;quot;w&amp;quot;) {|file| file.write YAML.dump(mkhash(long, &amp;quot;long&amp;quot;)) }  --- ? aaaaaaaaaaaaaaaaaaaaaaaaaa...aaa : long  どうやらRubyが使っているYAMLライブラリのPsych内で、</description>
    </item>
    
    <item>
      <title>Github Pagesの新しいIPアドレス対応</title>
      <link>http://ota42y.com/blog/2014/11/20/github-pages-new-ip/</link>
      <pubDate>Thu, 20 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/11/20/github-pages-new-ip/</guid>
      <description>これまでGithub Pagesで独自ドメインを運用していましたが、9月ぐらいから記事のアップロードのたびに、
新しいIDアドレスを利用するようにとのwarningが来るようになりました。
そのときに解消方法を調べたのですが、よくわからず放置していたところ、
12月から新しいアドレスに設定にしないと使えなくなるらしいので慌ててアップデートしました。
GitHub Pagesの旧IPアドレスが利用できなくなります。独自ドメインを利用しているユーザーで旧IPアドレスのままのレポジトリが影響されます。 https://t.co/1XI3X3NheA
&amp;mdash; GitHub Japan (@GitHubJapan) 2014, 11月 7 
確認方法 正しく設定されているかは、 GitHub Pages Legacy IP Deprecation に書いてある確認スクリプトを実行して、
OK以外が表示されたらダメです。
このサイトの場合は以下の通りですね。
dig ota42y.com | grep -E &#39;(207.97.227.245|204.232.175.78|199.27.73.133)&#39; || echo “OK”
修正手順 修正手順は簡単で、自分の持っているドメインを管理しているサービスに行って、
Aレコードを下記のページにあるように変更するだけです。
Tips for configuring an A record with your DNS provider - User Documentation
私はさくらインターネットを使っていたため、以下の手順で変更できました。
 ドメインメニュー
 対象のドメインのゾーン編集をクリック 左側の変更をクリック 既存のIPアドレスをクリックし、前述のGithubの新しいIPアドレスを設定する  変更をすると、先ほどのdigコマンドがOKを出すようになり、
Github Pagesにコミットしても警告メールが飛ばなくなります。</description>
    </item>
    
    <item>
      <title>golangでYAMLファイルを読み込む</title>
      <link>http://ota42y.com/blog/2014/11/13/go-yaml/</link>
      <pubDate>Thu, 13 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/11/13/go-yaml/</guid>
      <description>https://github.com/go-yaml/yamlを使う事で、 goでYAMLを扱うことが出来ます。
サイトにはメモリ上のデータに対してYAML化するサンプルしかありませんが、
以下のようにすることでファイルからYAMLを読み込み、Mapとして扱うことが出来ます。
またExampleには型を決めて読み込む方法しか乗っていませんが、
以下の例では、go で yaml 等を「map[interface{}]interface{}」型で読み込んだ際の動的型の参照方法
を参考に型を決めずにMapで読み込んでいます。
かなり冗長な表現になっていますが…(´･_･`)
a: Easy! b: c: 2 d: [3, 4]  package main import ( &amp;quot;fmt&amp;quot; &amp;quot;io/ioutil&amp;quot; &amp;quot;gopkg.in/yaml.v2&amp;quot; ) func main() { buf, err := ioutil.ReadFile(&amp;quot;test.yml&amp;quot;) if err != nil { return } m := make(map[interface{}]interface{}) err = yaml.Unmarshal(buf, &amp;amp;m) if err != nil { panic(err) } fmt.Printf(&amp;quot;%s\n&amp;quot;, m[&amp;quot;a&amp;quot;]) fmt.Printf(&amp;quot;%d\n&amp;quot;, m[&amp;quot;b&amp;quot;].(map[interface {}]interface {})[&amp;quot;c&amp;quot;]) }  出力結果
Easy! 2  </description>
    </item>
    
    <item>
      <title>スクフェス用パッケージの更新</title>
      <link>http://ota42y.com/blog/2014/11/12/scfes-update/</link>
      <pubDate>Wed, 12 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/11/12/scfes-update/</guid>
      <description>前に作ったスクフェス用パッケージを更新しました。
https://github.com/ota42y/hubot-scfes
今回は、いつぐらいにレベルアップするかを計算するコマンドを用意しました。
真夜中にレベルアップが起きたりすると面倒なので、これを見て事前に石を使うなり、
別の難易度をやって遅らせるなりして、レベルアップのタイミング調整をすることが簡単になります。
# 次のレベルアップまでexを何回プレイするか hubot scfes levelup count 830 ex =&amp;gt; 10 # 次のレベルアップはいつか hubot scfes levelup time 830 ex =&amp;gt; next levelup is Sun Aug 3 2014 2:52:52 GMT+0900 (JST)  なお、現在のスタミナ値は考慮していないため、実際は今たまってる分だけ早くなります。</description>
    </item>
    
    <item>
      <title>Hubotの追加機能作成をテストで楽にする</title>
      <link>http://ota42y.com/blog/2014/11/10/hubot-test/</link>
      <pubDate>Mon, 10 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/11/10/hubot-test/</guid>
      <description>Hubotで追加機能を作るときに一番大変なのは、やはりテストの部分だと思います。
普通に頑張ると、起動してbotにメッセージ送って、動かなければ止めて修正…といった、
面倒な手順を踏むことになります。
また、普通に起動するとエラーも吐いてくれないため、デバッグは非常に困難です。
しかし、テストを書いて開発をする場合、メッセージ送信やメッセージのデータ取得など、
Hubotに依存する部分をstubで置き換えるのはとても面倒な作業です。
そこで、作りたい機能をパッケージ化して作成することで、
トライアンドエラーをする部分を最低限に絞って開発することができました。
Hubot用パッケージの構成 Hubotはpackage.jsonのmainに指定したファイルをロードしてくれるため、
ここにhubot用のスクリプトを書くことで、Hubotスクリプトをnpmパッケージで管理できます。
さらに、以下のようにhubotスクリプトから、処理をまとめたオブジェクトの特定のメソッドを呼び出すことで、
Hubotの連携部分と実際の処理を分けることができます。
PackageClass = require(&#39;./package-name/package-class.coffee&#39;).PackageClass module.exports = (robot) -&amp;gt; package_class = new PackageClass robot.respond /test call (\d+)( \w+)?/, (msg) -&amp;gt; arg1 = parseInt msg.match[1] arg2 = msg.match[2] msg.reply package_class.testCall(arg1, arg2)  フォルダ構成は以下のようになります。
parkage-root/ ├ package.json │ ├ src/ │ ├ hubot-command.coffee │ └ package-name/ │ └ package-class.coffee │ └ test/ ├ test-helper.coffee └ test-package-class.coffee  このように構成し、package-classに対してテストを作成することで、
Hubot固有の部分を最小限に減らし、通常のnpmパッケージのように開発できました。
前述の通り、hubotとやりとりをするhubot-commandに対してテストをするのは手間がかかります。</description>
    </item>
    
    <item>
      <title>size_tは環境によって定義が変わるという話</title>
      <link>http://ota42y.com/blog/2014/11/08/size-t/</link>
      <pubDate>Sat, 08 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/11/08/size-t/</guid>
      <description>size_tが何bitになるかは環境によって定義が異なります。
そのため、以下のコードは多くの32bit下で上手くいきますが、64bit化などで環境が変わると動かなくなります。
#include &amp;lt;string&amp;gt; int main () { std::string test = &amp;quot;test text&amp;quot;; unsigned int pos = 0; printf(&amp;quot;size_t %lu\n&amp;quot;, sizeof(size_t)); pos = test.find(&amp;quot;ms&amp;quot;); printf(&amp;quot;pos %lu, %lu\n&amp;quot;, (size_t)pos, std::string::npos); if(pos != std::string::npos){ std::string text = test.substr(pos); printf(&amp;quot;%s\n&amp;quot;, text.c_str()); } return 0; }  std::stringのfindは引数の文字列が最初に出てくる位置か、見つからなかった場合にstd::string::nposを返します。
この時、戻り値の型はsize_tになります。
size_tは32bit上ではunsigned intの別名として定義される事が多いため、上記のコードは問題なく動きます。
ですが64bitにした場合、size_tはunsigned long(8bit)の別名として定義される事があるため、
unsigned int(4bit)で表せない範囲の値だった場合はデータが一部消滅します。
さらに、std::string::nposは-1として定義されており、unsignedとして解釈した場合にはその値の最大値になります。
size_tがunsigned intの場合、両者は同じ大きさのため特に意識する必要はありません。
ですが、size_tがunsigned longとして定義されている場合、その最大値はunsined intでは表せないため、
データが消滅し、結果として比較に失敗するという事が起きます。
私の環境では、上記のコードは-1をunsigend intにした4294967295と,
-1をunsigend longにした18446744073709551615とを比較し、
test textの4294967295文字目にアクセスして異常終了します。
やっかいなことに、size_tをunsigned intではなくintに代入した場合、
-1は-1として解釈されるため、unsigend longと比較した際に最大値に変換されるため、上手くいってしまいます。
とはいえ、安全性を求めるならば、出来るだけsize_tはsize_tとして扱うようにした方がいいと思います。</description>
    </item>
    
    <item>
      <title>void型のポインタとint型を相互変換するなという話</title>
      <link>http://ota42y.com/blog/2014/11/07/cpp-64bit-cast/</link>
      <pubDate>Fri, 07 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/11/07/cpp-64bit-cast/</guid>
      <description>int型をvoid *に変換する場合も、その逆の場合も、32bitだと問題なく動くことが多いため、
コンパイラもエラーにしない場合が多いです。
ですが、64bitだと問題が起きることが多いため、64bitを対象にした場合にエラー扱いをする場合があり、
突然わいて出る大量のエラーに悩まされる事があります…(´･_･`)
intからvoid型のポインタへの変換 int型の値をvoid *を利用して保持したい場合、前の例のように、int型を保持するオブジェクトを作り、
その中に値を入れた上で、そのオブジェクトへのポインタを持たせる必要があります。
struct Container{ void* data; }; struct Num{ int n; }; // int型を保存する Num* numPointer = new Num(); numPointer-&amp;gt;n = 42; Container con; con.data = (void *)numPointer; // 42を取り出す int number = ((Num*)con.data)-&amp;gt;n; // newしたので必ず破棄する delete con.data; con.data = NULL;  ですが、世の中にはたまにvoid *にint型（やその他のプリミティブ型）を代入する不届き者がいます。
int num = 42; Container con; con.data = (void*)num; // 42を取り出す int number = (int)con.data;  void *はポインタのため、32bit環境ではvoid*は32bitであり、intも基本的には32bitで同じサイズのため、</description>
    </item>
    
    <item>
      <title>汎用ポインタを使う</title>
      <link>http://ota42y.com/blog/2014/11/06/cpp-void-pointer/</link>
      <pubDate>Thu, 06 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/11/06/cpp-void-pointer/</guid>
      <description>int型のポインタとchar型のポインタは違う型のため、同じものとして扱うことは出来ません。
ですが、実際にはポインタ型はメモリ上の特定アドレスを示すもののため、
どの型のポインタであっても、データ自体はメモリ上のアドレスを示す何bitかの数値であり、全く同じです。
（勿論、ポインタの示すアドレスに何があるかは異なります）
そのため、ポインタ専用の変数を利用することで、あらゆる型のポインタを同じ変数に代入することができます。
ただし、コンパイラの型チェックが効かなくなる等の理由から、基本的にはオススメできない手法です。
C++の場合はテンプレートやクラスの継承、dynamic_castで解決できる場合はそちらを利用した方が安全です。
汎用ポインタ void *型は汎用ポインタと呼ばれ、あらゆるポインタを代入することができます。
これはたとえば以下のように、何の型かは指定しないけど、変数として持ちたいという場合に利用できます。
この場合、変数定義をvoid *型にしておき、使う前後に目的の型にキャストすることで、
様々な型を1つの変数で扱うことが出来ます。
struct Container{ void* data; }; struct Num{ int n; }; const char* text = &amp;quot;text&amp;quot;; Container con1; con1.data = (void*)text; // dataにconst charのポインタを入れる Num* numPointer = new Num(); numPointer-&amp;gt;n = 42; Container con2; con2.data = (void*)numPointer; // dataにNum型のポインタを入れる printf(&amp;quot;%s\n&amp;quot;, (const char*)con1.data); // 取り出す際にキャストする printf(&amp;quot;%d\n&amp;quot;, ((Num*)con2.data)-&amp;gt;n);  なお、違うポインタを違う型にキャストして使うと、メモリ破壊などの予期せぬエラーを引き起こしますが、
構文上はvoid *から元の型に戻す際のキャストは全て正しいと処理されます。
そのため、おかしくなるキャストをしていてもコンパイラの型チェックでエラー検出が出来ません。
前述の通り、C++の場合はvoid *を使わず、テンプレートやクラスの継承、dynamic_castで解決した方が安全です。
mallocの戻り値 void *はmallocの戻り値としても使われています。</description>
    </item>
    
    <item>
      <title>hubot-ircではmsg.replyのリプライ先が変わるので注意</title>
      <link>http://ota42y.com/blog/2014/11/01/hubot-reply/</link>
      <pubDate>Sat, 01 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/11/01/hubot-reply/</guid>
      <description>hubot-ircを使い、こういうコードで一定時間後に後からユーザに通知しようとしてたところ、
replyしてるのに発言元とは別のチャットに送信してしまうという問題が起きました。
robot.respond /進捗 start/i, (msg) -&amp;gt; setTimeout(-&amp;gt; msg.reply &amp;quot;進捗どうですか？&amp;quot; return , 30 * 60 * 1000) msg.send &amp;quot;進捗 start&amp;quot;  何度か意図的に起こしてみたところ、どうやらmsg.replyの送信先は、
そのユーザがreplyする時に最後に発言したチャットに対して行われるらしく、
メッセージが作られた後に別のチャットに発言した場合、そちらに送られてしまうようです。
そのため、以下のように送信先を待避することで回避できます。
robot.respond /進捗 start/i, (msg) -&amp;gt; user = msg.message.user.name room = msg.message.user.room setTimeout(-&amp;gt; robot.send {room: room}, &amp;quot;#{user} 進捗どうですか？&amp;quot; return , 30 * 60 * 1000) msg.send &amp;quot;進捗 start&amp;quot;  原因となるコードを捜す というのは振る舞いから推測したものなので、実際にコードを追ってみます。
hubot-irc.reply まずはhubot-ircのmsg.replyの中を見ます。
https://github.com/nandub/hubot-irc/blob/master/src/irc.coffee#L78
reply: (envelope, strings...) -&amp;gt; for str in strings @send envelope.user, &amp;quot;#{envelope.user.name}: #{str}&amp;quot;  メッセージをユーザ宛に変換し、sendメソッドにenvelope.</description>
    </item>
    
    <item>
      <title>スクフェス用の機能が詰まったHubotパッケージを作った</title>
      <link>http://ota42y.com/blog/2014/10/28/hubot-scfes/</link>
      <pubDate>Tue, 28 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/10/28/hubot-scfes/</guid>
      <description>スクフェス用の色々便利な機能が入ったHubotパッケージを作りました。
https://github.com/ota42y/hubot-scfes
今のところ主な機能は2つです。
スタミナがMaxになる時間になったら通知する hubot scfes remind stamina 10 50
で、スタミナの現在値が10、最大値が50の場合に、Maxになる時刻に通知してくれます。
それ以外の値の場合は調節してください。
スタミナがnの倍数になったときに通知する hubot scfes remind stamina 10 50 25
で、スタミナの現在値が10、最大値が50として、25の倍数の時に通知してくれます。
EXでちょうど使い切れるタイミングで通知するといった使い方を想定しています。
未実装機能 イベント終了までにどれくらいスタミナが回復するかとか、
レベルアップするのはいつぐらいになるかとか、
そういった頭の中で適当に計算してる奴を機能化していこうと思います。
ちなみにこいつは前に作ったstamina-calculatorを内部で使用しています。
node.jsで細かくパッケージに分けて開発ってどうやるんだろうなーと思って、実益と練習がてら作った感じです。</description>
    </item>
    
    <item>
      <title>Linux上でrubyのPTYを使うと、Errno::EIOが出る</title>
      <link>http://ota42y.com/blog/2014/10/26/pty-ieo-error/</link>
      <pubDate>Sun, 26 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/10/26/pty-ieo-error/</guid>
      <description>以下のコードはMac OS X上だと上手く動きますが、Linux上だと
Errno::EIO: Input/output error @ io_fillbuf というエラーが起きます。
require &#39;pty&#39; PTY.spawn(&amp;quot;ls&amp;quot;) do |r,w,pid| until r.eof? do puts r.readline end end  どうやら、読み込んだ際にBSDだとnilになりますが、GNU/LinuxだとErrno::EIOが発生する仕様らしいです。
Ruby on Linux PTY goes away without EOF, raises Errno::EIO
にあるように、resqueするSafePtyを作ることで回避できます。</description>
    </item>
    
    <item>
      <title>javascriptの関数リテラルではインスタンス変数にアクセスできない</title>
      <link>http://ota42y.com/blog/2014/10/23/javascript-callback/</link>
      <pubDate>Thu, 23 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/10/23/javascript-callback/</guid>
      <description>関数リテラルではローカル変数には自由にアクセスできるので、
ついインスタンスメソッド等にもアクセス出来ると思ってしまいましたが、違うようです。
以下のように、コールバックとして自分のインスタンスメソッドを呼び出す関数を渡した場合、
実行時にエラーになります。
(coffeescriptで書いていますがjavascriptと同じ結果です)
class Test hello: (num) -&amp;gt; console.log &amp;quot;hello &amp;quot; + num call: (test2) -&amp;gt; @hello(1) test2.call( -&amp;gt; @hello(2) ) class Test2 call: (callback) -&amp;gt; callback()  関数リテラルはそれを作ったオブジェクトとは別のオブジェクトから呼び出されるらしく、
またその時のthis(coffeescriptなので@hello(2)はthis.hello(2)と等価です)は、
そのオブジェクトになり、メソッドがないため失敗するようです。
以下のように、一度thisを待避させることで呼び出すことが出来ます。
class Test hello: (num) -&amp;gt; console.log &amp;quot;hello &amp;quot; + num call: (test2) -&amp;gt; @hello(1) self = this test2.call( -&amp;gt; self.hello(2) ) class Test2 call: (callback) -&amp;gt; callback()  完全なコードはこちら
class Test hello: (num) -&amp;gt; console.log &amp;quot;hello &amp;quot; + num call: (test2) -&amp;gt; @test = &amp;quot;test&amp;quot; @hello(1) self = this test2.</description>
    </item>
    
    <item>
      <title>goでtime.Timeをmysqlから読む</title>
      <link>http://ota42y.com/blog/2014/10/08/go-mysql-time/</link>
      <pubDate>Wed, 08 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/10/08/go-mysql-time/</guid>
      <description>goでtime.Time型をmysqlのDATETIME型として保存すると、以下のエラーが出て読み取りに失敗します…
sql: Scan error on column index 3: unsupported driver -&amp;gt; Scan pair: []uint8 -&amp;gt; *time.Time
どうやらDSNにparseTime=trueオプションをつける必要があるようです(何故かは不明)
db, err := sql.Open(&amp;quot;mysql&amp;quot;, &amp;quot;username:passy@/database_name?parseTime=true&amp;quot;)
参考リンク
https://github.com/go-sql-driver/mysql#timetime-support</description>
    </item>
    
    <item>
      <title>進捗どうですか？を訪ねるhubotスクリプトを作った</title>
      <link>http://ota42y.com/blog/2014/10/05/shinchoku/</link>
      <pubDate>Sun, 05 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/10/05/shinchoku/</guid>
      <description> 作りました。
30分ごとに進捗どうですか？と聞いてきます。
何をやったかを計測するために定期的にログを残そうと思うのですが、
大体忘れるので、こうやって適度に通知することで思いだそう！という試みです。
使い方 進捗 startで30分ごとに進捗どうですか？と聞いてきます。
進捗 stopでストップします。
ホラー体験 setTimeoutが秒設定だと間違えて…(´･_･`)
コード  </description>
    </item>
    
    <item>
      <title>goでmysqlを使う</title>
      <link>http://ota42y.com/blog/2014/10/04/go-mysql/</link>
      <pubDate>Sat, 04 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/10/04/go-mysql/</guid>
      <description>http://github.com/go-sql-driver/mysql
がありましたので、それを使います。
以下のように読み込むことで、sql.Openでmysqlを開くことが出来ます。
import ( _ &amp;quot;github.com/go-sql-driver/mysql&amp;quot; )  DB設定 以下の用に指定する事で、ローカルのmysqlの指定したデータベースにアクセス出来ます。
db, err := sql.Open(&amp;quot;mysql&amp;quot;, &amp;quot;user:password@/dbname&amp;quot;) if err != nil { panic(err.Error()) } defer db.Close()  サーバやデータベース名などはDSN (Data Source Name)で指定するようです。
あまり聞かない方法ですが、公式のREADMEに書いてあるのでそれを参考にすると良いと思います。
使い方 前提条件 上記の方法でsql.Openの結果を変数のdbに保存済み、
以下の構造体をDBに書き込むとします。
type Post struct { RoomName string Message string MessageId string IsSend bool }  また、tableNameに書き込むテーブル名が保存されているとします。
INSERT post := getPost()　// 書き込むためのデータを取得する stmtIns, err := db.Prepare(fmt.Sprintf(&amp;quot;INSERT INTO %s (room_name, message, message_id, is_send) VALUES (?, ?, ?</description>
    </item>
    
    <item>
      <title>HDDのパーティションテーブルが消えてからのデータ復旧</title>
      <link>http://ota42y.com/blog/2014/10/03/hdd-post/</link>
      <pubDate>Fri, 03 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/10/03/hdd-post/</guid>
      <description>先日デスクトップPCのデータ用HDDが吹っ飛びました。
原因は不明ですがデータは消えておらず、パーティションテーブルか完全に消えていました。
復旧に成功したのでその手順をメモっておきます。
問題推定 初めは何故データが消えたのかわからなかったため、その調査から始めました。
消えたのはデータ用HDDで、Windows本体は問題なく起動していたため、WindowsからHDDを見ると、
未フォーマット状態のディスクとして認識され、パーティションスタイルの選択から始まっていました。
HDDは数ヶ月前に新調したものであり、異音やエラーなどの問題も無かったため、物理的な可能性は薄いと考えました。
また、直前に大量にデータを書き込んでもいないため、データを削除した線も薄そうです。
さらに、一部のデータが読み込めなくなるのではなく、全データが一度に読み取り不能になったことから、
何らかの要因でパーティションが消えてデータが読み取り不能になり、
データ自体は残っているのではないかと考えました。
パーティションの確認 運良く手元にLinuxの起動ディスクがあったため、とりあえずパーティションがどうなっているかを確認しました。
手順としては以下のようになります。
なお、HDDがsdaにマウントされていると仮定します。
 ddコマンドでHDDの先頭をコピーする
ddコマンドはディスクからファイルやディスクにデータをコピーできるコマンドです。
sudo dd if=/dev/sda of=/tmp/hdd count=100
のようにすることで、sdaの先頭から100ブロック分を/tmp/hddファイルにコピーします。
 hexdumpコマンドで中身を見る
hexdumpコマンドはファイルの中身を16進数で出力します。
hexdump /tmp/hdd  私の場合、hexdumpの出力が先頭50MBぐらい全て0だらけだったので、
パーティションが全て0埋めされて消えていました。
そのため、次にパーティションの復活を試みます。
TestDiskでパーティション復活 壊れたディスクはPBRにしていたため、MBRと比べて大変になる場合が多いらしいですが、
TestDiskを使うと特に問題なく修復できました。
詳しい使い方はこちら。
【TestDisk】について
私の場合、これでNTFSのパーティションを修復できましたが、
何故か書き込んでいないはずのWindowsディスクが壊れてしまい、
データと引き替えにWindowsが消えてしまいました…
幸いWindowsディスクには特に重要なデータを入れていなかったのと、
Macからデータを読み出すことができたために難を逃れました。
念のため、Windowsディスクは外して作業をした方がよさそうです。</description>
    </item>
    
    <item>
      <title>ソシャゲ用のスタミナ計算機を作った</title>
      <link>http://ota42y.com/blog/2014/10/02/social-game-stamina-calc/</link>
      <pubDate>Thu, 02 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/10/02/social-game-stamina-calc/</guid>
      <description> 作りました。
スタミナmaxまでどれくらいかなー？とか、次イベント曲(25消費)をやるのに区切りがいいのはいつかなー
みたいなのを計算するのが面倒だったので、その辺の計算を自動でやってくれます。
使い方 初期化 スタミナ1あたりいくつ回復するかはゲームによって違うので、コンストラクタで渡します。
なお、指定は秒で行います。
stamina_calculator = new StaminaCalculator 6*60
この例ですと、6分で1スタミナ回復するゲームになります。
指定した値までの回復時間(getNextMaxStaminaTime) 現在値と指定値を渡すと、指定した値までに何秒かかるかを返します。
Maxまでどれくらいかな？とか、50になるのは何分後か？みたいな計算に使います。
stamina_calculator.getNextMaxStaminaTime(10, 60)
指定した倍数の値まで回復する時間(getMultipleRecoveryTime) 現在値とMAX値、それと倍数を渡すことで、指定した倍数になる時間を返します。
今30で、一回25消費できるから、50か75か100になる時間を知りたい…みたいな時に使います。
next_multiple_time = calc.getMultipleRecoveryTime(10, 80, 25)
結果をDateオブジェクトにする メソッドを呼んだ時間から指定秒たった時点のDateオブジェクトを返します。
Dateオブジェクトのコンストラクタには、指定時間後のオブジェクトを作る方法が見当たらなかったので作りました。
stamina_calculator.convertToDate(second)
サンプルコード StaminaCalculator = require(&#39;stamina-calculator&#39;).StaminaCalculator; stamina_calculator = new StaminaCalculator 6*60 second = stamina_calculator.getNextMaxStaminaTime(10, 60) console.log second # 今から何秒後にスタミナが５０回復するか console.log stamina_calculator.convertToDate(second) # 現在時刻からスタミナが５０回復する時間がDateオブジェクトで帰ってくる  </description>
    </item>
    
    <item>
      <title>npmにパッケージを公開する手順</title>
      <link>http://ota42y.com/blog/2014/09/29/npm-publish/</link>
      <pubDate>Mon, 29 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/29/npm-publish/</guid>
      <description>とても簡単でしたがつい忘れるのでメモ。
公開手順  npmjsに開発者登録をします。
https://www.npmjs.org/
 npm adduserで~/.npmrcにnpmへのログイントークンを保存します。
 package.jsonに必要事項を書きます。
こんな感じですね。
 npm publishで公開
 アップグレード版の配布もnpm publishで可能です。
(ただし、package.json内のバージョンを変えないと新しいバージョンにはなりません)
  非公開手順 間違えて変なバージョンを公開した場合など、公開したパッケージを削除したい場合は、
npm unpublish パッケージ名前@バージョンで削除出来ます。
何も指定しない場合全てのバージョンが対象になりますが、--forceをつける必要があります。
ただし、削除してしまうので、そのパッケージに依存しているパッケージが悲惨なことになります。
そのため、npm deprecateの方がアップグレードを促せるし推奨すると公式サイトには書いてあります。
unpublishは間違えて公開した場合用ですね。
なお、全てのバージョンを削除するとnpmからパッケージの情報も削除されます。</description>
    </item>
    
    <item>
      <title>Debian squeezeでShellShock対策</title>
      <link>http://ota42y.com/blog/2014/09/26/shell-shock/</link>
      <pubDate>Fri, 26 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/26/shell-shock/</guid>
      <description>Debianは探しても見つからなかったので…(´･_･`)
ShellShock(CVE-2014-6271) CVE-2014-6271(とCVE-2014-7169)として、bashに第三者から任意のコードが実行される脆弱性が見つかりました。
GNU bash の脆弱性に関する注意喚起
bashの脆弱性(CVE-2014-6271) #ShellShock の関連リンクをまとめてみた
Blog: bashの脆弱性がヤバすぎる件 – x86-64.jp - くりす研
bashで以下のスクリプトを実行し、出力文字列にvulnerableが出てきたらまずい状態です。
env x=&#39;() { :;}; echo vulnerable&#39; bash -c &#39;echo hello&#39;  修正パッチ状況 この問題に対しては(まだ不十分ですが)修正パッチが公開されており、また各ディストリビューションでも修正したbashが配布されています。
Redhad系の対策はこちら
2014/09/24に発表されたBash脆弱性と解決法(RedHat系)
Mac OS X系の対策はこちら
CVE-2014-6271のbashの脆弱性に対応する方法　私の使っているDebian(squeeze)でも、パッチが適応されたbashを配布しています。
https://security-tracker.debian.org/tracker/CVE-2014-6271
修正版へのアップデート 修正されたbashはsqueezeの通常のリポジトリには修正版は公開されておらず、
ltsリポジトリを参照する必要があります。(2014/09/26 7:00現在)
LTS/Using - Debian Wikiにリポジトリが書いてあるので、それをapt-getの参照先に追加します。
/etc/apt/sources.list.d`に、lts.listを作り、以下のように書き込みます。
deb http://http.debian.net/debian/ squeeze-lts main contrib non-free deb-src http://http.debian.net/debian/ squeeze-lts main contrib non-free  (手元に環境がないので確認できませんが、おそらくWheezyでは以下のリポジトリで行けると思います)
deb http://security.debian.org/ wheezy/updates main deb-src http://security.debian.org/ wheezy/updates main  この状態で</description>
    </item>
    
    <item>
      <title>C&#43;&#43;で少しでもビルド速度を速くする方法</title>
      <link>http://ota42y.com/blog/2014/09/23/cpp-build/</link>
      <pubDate>Tue, 23 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/23/cpp-build/</guid>
      <description>結論  キャッシュを使う 不要なinclude削除 static or dynamic library化する 並列コンパイルする 分散コンパイルする いいパソコンを使う  C++のビルドをちょっとでも高速化したかったので、
どうすれば早くなるのかを調べたのでまとめました。
どれか一つをやるというよりかは、複数の手法を組み合わせていくのがいいと思います。
キャッシュを使う ccacheのように、コンパイル結果をキャッシュしておくソフトを使うことで、
2回目以降のビルドは差分だけをコンパイルし直すため早くなります。
といっても、多分使わない方が珍しいと思いますが。
不要なinclude削除 C++で不要なincludeを減らすのように、
不要なincludeを減らすことでコンパイル時間の短縮化と、キャッシュを最大限活用することができます。
Static Library or Dynamic Library化する 切り出せる部分はライブラリとして切り出し、先にコンパイルしておくことで、
本体のコンパイル時間が短縮されます。
簡単に早くなりますが、ライブラリの切り出し方を工夫しないと効果が無い場合があります。
Static Libraryの場合はリンクが必要なので、リンク時間は減らせませんが、
コンパイル時間は省略できるため大幅に早くなります。
Dynamic Libraryはリンク時間が不要になりますが、その分実際の実行時に時間がかかります。
代わりに、ライブラリの部分だけ入れ換えるといったことができます。
ただし、iOSでは使えません…(´・_・`)　並列コンパイルする makeにはjオプションがあり、指定した数だけ並列実行されます。
多くしすぎると逆効果らしく、一般的にはコア数×2ぐらいを指定すると良いそうです。
分散コンパイルする distcc等を使って分散コンパイルをすることで、劇的にコンパイル時間を短くできます。
ただしその分だけPCが必要なのと、Xcodeは非対応です…(´・_・`)
いいパソコンを使う 当たり前ですが、メモリ、CPU、SSDの性能を上げると早くなります。
ただし、高いPCはコスパが悪いので、他の手法と組み合わせて上手い具合に良いところを見つけてください。</description>
    </item>
    
    <item>
      <title>C&#43;&#43;で不要なincludeを減らす</title>
      <link>http://ota42y.com/blog/2014/09/22/cpp-include/</link>
      <pubDate>Mon, 22 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/22/cpp-include/</guid>
      <description>不要なinclude削除 cppでは、includeは単にそこに書かれているファイルの内容を展開するだけになっています。
そのため、includeする量が増えるほどコンパイラが解析する量も増え、
結果としてコンパイル時間が長くなります。
また、キャッシュを利用している場合、
includeされているファイルのどれか一つでも変更があった場合はビルドし直しになるため、
不要なincludeを消すとよりキャッシュを活用できます。
クラスの前方宣言を活用する クラスのメソッドやプロパティへのアクセスをしない場合、クラスの実態を知る必要はありません。
そのため、メンバ変数にクラスを持つ場合に、ポインタとして持たせることで、
ヘッダファイルにincludeを書く必要が無くなります。
これにより、不要なincludeを減らすことができます。
例えば、以下のようなAクラスがあるとします。
// TestA.h #include “InClass.h&amp;quot; class TestA{ public: TestA(); int getNumber(); private: InClass m_inclass; };  //TestA.cpp #include “TestA.h&amp;quot; TestA::TestA() { }; int TestA::getNumber(){ return m_inclass.getNumber(); };  このとき、TestAクラスをincludeするクラスは、InClassを使わない場合でも、
TestA.hに書かれているために読み込んでしまいます。
そのため、コンパイラが処理する量が増えるのと、
InClass.hに変更があった場合に使っていないファイルまでコンパイルし直しになります。
ここで、以下のようにクラスの前方宣言を使い、
cppファイル側で読み込むことで、includeをヘッダファイルから削除できます。
//TestA.h class InClass; class TestA{ public: TestA(); ~TestA(); int getNumber(); private: InClass* m_inclass; };  //TestA.cpp #include “TestA.h&amp;quot; #include &amp;quot;InClass.h&amp;quot; TestA::TestA() { m_inclass = new InClass(); }; TestA::~TestA() { delete m_inclass; }; int TestA::getNumber(){ return m_inclass-&amp;gt;getNumber(); };  これにより、TestAクラスをincludeしているクラスは、InClass.</description>
    </item>
    
    <item>
      <title>Twitterのoath_callbackは設定していないと上書きできない</title>
      <link>http://ota42y.com/blog/2014/09/19/twitter-callback/</link>
      <pubDate>Fri, 19 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/19/twitter-callback/</guid>
      <description>Twitterはoauth/request_tokenへのアクセス時に、
認証後のコールバック先をoauth_callbackパラメータで上書きできます。
Twitterの開発者画面からは独自スキーマは登録できませんが、
この機能で上書きをすると任意のスキーマをコールバックに設定できます。
ただし、Twitterに何らかのURL(ダミーでも可)を設定してないと上書きできないようです。
たとえば以下のようなコードを書いた場合、Twitterの開発者画面でURLを登録していないと、
401 Authorization Required (OAuth::Unauthorized)が帰ってきますが、
http://example.comのようにダミーURLを登録すると認証画面用のURLが帰ってきます。
require &amp;quot;oauth&amp;quot; consumer_key = &amp;quot;&amp;quot; consumer_secret = &amp;quot;&amp;quot; consumer = OAuth::Consumer.new consumer_key, consumer_secret, site: &amp;quot;https://api.twitter.com&amp;quot; request_token = consumer.get_request_token oauth_callback: &amp;quot;ota42y://test&amp;quot; puts &amp;quot;Please visit here: #{request_token.authorize_url}&amp;quot;  参考: Twitter の AccessToken と AccessTokenSecret を Ruby で取得する</description>
    </item>
    
    <item>
      <title>Travis CI Meetup Tokyoに行ってきた</title>
      <link>http://ota42y.com/blog/2014/09/18/travisci-meetup/</link>
      <pubDate>Thu, 18 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/18/travisci-meetup/</guid>
      <description>前半はTravis CIのデモ、真ん中てCEOのお話、最後にLTと三部構成でした。
こちらに動画があるそうです。
http://www.ustream.tv/recorded/52769967
デモは一通りのTravis CIの使い方をデモしていただきました。
何となくわかってるデフォルトの挙動などにちゃんと説明がはいり、
基礎知識を固めるには凄く良かったです。
また、いくつか知らない機能(sudo false)や、
.travis.ymlの書式をオンラインで確認できるTravis WebLint等が紹介されていました。
なお、デモに使ったリポジトリは以下にあるようです。
https://github.com/BanzaiMan/travis-intro-tokyo
CEOの話では、現在のjob数やVM数、サーバの数などを話していただきました。
それと後半では環境によって良くはまる10個が紹介されていました。
ファイルシステムのケースセンシティブや、GNUとBSDの違いだったり、aptから古いバージョンが消えたり、
タイムゾーンやOSの違いによる環境構築や言語自体のバグ等々…
ここ最近に当たった奴がいくつかあり頭が痛かったです…(´･_･`)
最後のLTではいろんな発表が行われました。
見つけた分だけまとめておきます。
Automated releasing iOS app with Travis CI https://speakerdeck.com/kishikawakatsumi/automated-releasing-ios-app-with-travis-ci
私はJenkinsを使っていますが、似たような構成でやってるので参考になりました。
ただ、ビルド時間が5倍ぐらい長いので、こちらのプロジェクトにそのまま導入は難しそうです…
xcarchiveに書き出すと、申請用ビルドでテストフライトに出せるというのは新たな発見でした。
(追記: 発表者様から指摘がありました)　@ota42y 僕の話し方が悪かったのですが、xcarchiveだから、というわけではなくて、AppStoreとAdHocのプロビジョニングは互換性があるので、xcarchiveからipaにするところでプロビジョニングをAdHocにしています。同じビルドで署名だけ変える方法です。
&amp;mdash; kishikawa katsumi (@k_katsumi) 2014, 9月 18 
Travis CI API LT https://speakerdeck.com/pinzolo/travis-ci-api-lt
Travis CIのAPIと、gemの利用例です。
パッと見た限り、必要最低限は揃ってそうなので、hubotから叩くみたいなことが出来そうです。
QUnit on Travis CI @shigemk2 https://shigemk2.github.io/travisci_jp/#/step-1
jsのライブラリを作った際の利用例でした。
実際に作る際に参考に出来そうでした。
When was the build passing? http://sanemat.github.io/talks/20140917-travis-ci-meetup-tachikoma-io/
依存してるライブラリが更新すると、いつの間にかテストが落ちるようになってるけど、
テストが実行されないとわからないので、活発でないプロジェクトだと大変だよね…って話です。
これに対して、定期的にPRを送るTachikoma.io - Interval Pull Request Appというものを作ったそうです。</description>
    </item>
    
    <item>
      <title>スタックに確保した変数の有効範囲に気をつける</title>
      <link>http://ota42y.com/blog/2014/09/16/stack-heap/</link>
      <pubDate>Tue, 16 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/16/stack-heap/</guid>
      <description>C++にはスタックとヒープという、使い方の違うメモリ領域が存在します。
rubyとかjavaではこのような違いは基本的に意識する必要が無いため、間違った使い方をしてしまう場合があります。
例えば、普通の変数はスタックに積まれるため、スコープを抜けるとたとえ使っていても破棄されます。
以下に、関数の中で文字バッファをスタックに確保し、その参照を戻す関数の間違った例を上げます。
#include &amp;lt;string&amp;gt; #include &amp;lt;stdio.h&amp;gt; char *getTextFilename(const char* basename){ char str[1024] = {0}; strncpy(str, basename, strlen(basename)+1); strncat(str, &amp;quot;.txt&amp;quot;, 5); return str; } int main(void){ char *test_filename = getTextFilename(&amp;quot;test&amp;quot;); printf(&amp;quot;%s\n&amp;quot;, test_filename); char *example_filename = getTextFilename(&amp;quot;example&amp;quot;); printf(&amp;quot;%s\n&amp;quot;, example_filename); printf(&amp;quot;%s\n&amp;quot;, test_filename); return 0; }  環境によりますが、おそらく二回目のtest_filenameの出力がおかしくなると思います。
私の場合は以下のように、test_filenameにexample_filenameの内容が書き込まれていました。
test.txt example.txt example.txt  getTextFilename内のstrはスタックに確保されるので、関数終了時に解放されます。
そのため、戻り値の指し示す文字列は解放済みメモリとなり、勝手に変更される可能性があります。
上の例ではたまたま同じアドレスが再利用されたため、同じ文字列が設定されました。
ですが、間に様々な処理を実行した場合は、謎の値が書き込まれるなどがあり得るため注意が必要です。
対策 このような場合、いくつかの対策があります。
引数で渡す 1つ目が、strncpyやstrncatのように、メモリ利用域を引数として受け取る方法です。
これにより、自分のスコープから外れても値を保持することができます。
先ほどの例ですと、main側でcharの配列を確保して関数の引数でそれを受け取るといった形です。
確保したスコープを抜けるとやはり解放されますが、確実に解放されるためとても楽です。
ヒープに確保する 2つ目はmallocやnewでヒープ領域に確保する方法です。
以下はnewで配列を確保するように書き換えました。
#include &amp;lt;string&amp;gt; #include &amp;lt;stdio.h&amp;gt; char *getTextFilename(const char* basename){ char *str = new char[1024]; strncpy(str, basename, strlen(basename)+1); strncat(str, &amp;quot;.</description>
    </item>
    
    <item>
      <title>第47回情報科学若手の会2014に参加してきた</title>
      <link>http://ota42y.com/blog/2014/09/15/wakate2014/</link>
      <pubDate>Mon, 15 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/15/wakate2014/</guid>
      <description>2014年9月13日(土)〜15日(月)に静岡県伊東市の山喜旅館で開催された、
第47回情報科学若手の会2014に参加してきました。
二年ぶり三回目の参加です。
今回はLTもやりました。
当日の様子はこちらのまとめをどうぞ
第47回情報科学若手の会2014 ツイートまとめ #wakate2014
旅館について 山喜旅館さんのネットワークが超強化されていました。
2年前とは雲泥の差でとても快適で、導入した旅館と幹事さんには頭が下がります…
50人数位が普通に使っても回線部分はびくともしなかったため、 開発合宿には最適ではないでしょうか(宣伝)
発表について 今年は大学の講義をこれにしてくれれば…みたいな、凄くわかりやすい発表が多かったです。
私が把握した範囲で、公開されている資料をまとめました。
招待講演: サイバーセキュリティの世界に飛び込こもう！ 招待講演の資料は公開されていないようですが、紹介されていたnicterとDAEDALUSはとても凄いシステムでした。
従来なら、テキストベースのログとしてしか表現されていなかった悪意のある攻撃を、
わかりやすくかつリアルタイムに(さらにかっこよく)可視化しており、まさに機能美を揃えたシステムでした。
映像を見つけたので是非見ておくことをオススメします。
ネットワーク攻撃可視化・分析技術 - nicter #DigInfo
サイバー攻撃をリアルタイムに可視化、警告を発する「DAEDALUS」 #DigInfo
若手特別講演: 本当は楽しいインターネット http://www.slideshare.net/yuyarin/ss-39061287
インターネットへの愛がある上に、基本的な事柄を凄くわかりやすく説明していただいてます。
特にAS周りの話は名前を聞いたことがある…ぐらいの知識でしたので、
この資料で凄く良く理解することが出来ました。
ないんたんの天気予報と画像処理アルゴリズム（2014年9月13日 情報科学若手の会） https://docs.google.com/presentation/d/1KSIVmHvBR57uzJUkDEIMgi7JpAW51KequMCuws5COdY/edit#slide=id.p
画像処理のオプティカルフローを使って、天気の画像からその先の状態を計算して予報しているらしいです。
割と簡単で、計算量もさくらVPSで1秒ぐらいなのに、
精度の高い結果がでていて凄いなーと思いました。はぴぴーん
シュワルツ超関数としての信号処理理論 http://pel.es.hokudai.ac.jp/~akita/SignalAsDistribution.pdf
実際の発表資料とは違いますが参考資料として…
フーリエ変換系のは種類が色々あって、どこに何を使うのかよくわからなかったのですが、
とてもわかりやすく整理して話していただけました。
また、階段関数の微分はぱっと見意味不明だったのが、
話を聞くとちゃんと理解できるようになっていて、おお！って思いました。
私の発表 http://ota42y.com/blog/2014/09/14/wakate2014-presentation/
その他 交流会 交流会では謎解きゲームが行われました。
旅館の一階を全体的に使って謎探しから始まり、旅館やITに関係していそうな謎が用意されていました。
謎をを説いたチームには景品もあり、かなり白熱していました。
難易度やチーム編成も工夫されており、とても良かったです。
ナイトセッション 例年のごとく、中々濃い話や他では言えない話が繰り広げられていました。
例によって４時や５時ぐらいまで行われていたみたいですが、
さすがに体がついていかなく、２時３時でダウンしていました…
参加者の中に機械式計算機を持ってきてくださった方がいました。
所々の機械式ならではの工夫が見られ、初めて見た私にとっては凄く新鮮でした。
特にオーバーフロー、アンダーフローした時にベルがなるのは素晴らしかったです。

まとめ 帰りにkuro_m88さんと、「HCI系の人いないですよね−」と話していたんですが、
いないなら多分HCI系の話をするのは凄い有用だろうし、
来年はHCI系の話で一般かショート発表に申し込みたいと思います。
その後 帰りは三島の桜屋でうな重を食べました。</description>
    </item>
    
    <item>
      <title>情報科学若手の会2014で発表してきた</title>
      <link>http://ota42y.com/blog/2014/09/14/wakate2014-presentation/</link>
      <pubDate>Sun, 14 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/14/wakate2014-presentation/</guid>
      <description> LT発表してきました。
最近CIとか継続的デリバリーとか流行ってるけど、
ネット上にあふれている事例は、テスト(ビルド)が頑張れば5分程度で出来たり、
ユーザにすぐに提供できるWebサービスの話が多く、
それ以外の事例に適応した話が全然無いなーと思っていました。
特にスマホアプリでは、一つのjobに時間がかかったり、
ユーザに届けるのにもの凄く時間がかかるため、
良くあるやり方をそのまま持って行っても、
全然上手くいかないんですよね。
ただし、やり方をそっくり持ってくるんじゃなくって、
その手法をやると何がうれしいの？を考えていくと、
自ずとその環境に沿ったいい方法、というのが見つかる気がします。
より実践的なTipsになるので削った補足事項等
 コンパイルだけに出来ないのか  ビルドのほとんどがコンパイル時間なのでそれほど効果がありません…(´･_･`)  キャッシュは使っていないのか  キャッシュが悪さをする可能性が考えられるため意図的に切っています。
手元で開発する場合は活用しています。  ビルド自体の速度は速くしないのか  分散コンパイル等で早くできますが、Xcodeは対応していないのでAndroidのみになり、
そんなに効果がありません。  スライドで並列ビルドすればいいじゃん！と書いてますが、それも問題があります  アップロード先が並列で受け付けていないので、デプロイ時に失敗しまくります 排他制御しようとすると、Jenkinsの仕様とJob割り振りがよろしくないので
もの凄く辛いことになります。   </description>
    </item>
    
    <item>
      <title>cronやinit.dでsudoを実行するとエラーになって実行できない</title>
      <link>http://ota42y.com/blog/2014/09/13/sudo-error/</link>
      <pubDate>Sat, 13 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/13/sudo-error/</guid>
      <description>cronやinit.d以下に置いたスクリプトで、
別の以外のユーザで作業しようと思い、sudoを実行したところ、
以下のようなエラーが出て実行できませんでした。
sudo: sorry, you must have a tty to run sudo
どうやら、ttyを使わない場合、sudoは権限に関係なく実行できないようです。
sudoersにある
Defaults requiretty
をコメントアウトすることで解決しました。
参考</description>
    </item>
    
    <item>
      <title>Linux起動時に特定のシェルスクリプトを実行する</title>
      <link>http://ota42y.com/blog/2014/09/12/init-d/</link>
      <pubDate>Fri, 12 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/12/init-d/</guid>
      <description>hubotを自動で実行してほしいなーと思ったので、
OSの起動時にスクリプトを自動実行する方法を調べました。
まず、/etc/init.d/ に実行可能なスクリプトを置きます。
次に、スクリプトの二行目に、起動設定を書きます。
# chkconfig: 345 99 01
一つ目がランレベル、二つ目が起動順番、三つ目が終了順番になります。
起動・終了は小さい数値から行われるため、
前述の例ですと一番最後に起動し、一番最初に終了します。
ランレベルについては Wikipediaの記事 を参考にしてください
最後に、chkconfig --add (init.dに置いたスクリプト名)を実行して登録を行います。
これで、起動時にスクリプトが実行されます。</description>
    </item>
    
    <item>
      <title>strncpyははまりやすい</title>
      <link>http://ota42y.com/blog/2014/09/11/strncpy-bug/</link>
      <pubDate>Thu, 11 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/11/strncpy-bug/</guid>
      <description>strncpyは結構はまりどころがあります。
たとえば以下のコードは正しく動作しません。
char str[100]; const char * c = &amp;quot;test&amp;quot;; strncpy(str, c, strlen(c)); printf(&amp;quot;%s\n&amp;quot;, str);  strncpyはコピー先、コピー元、コピー長を引数で取ります。
この際、strlen等でコピー元の文字長ぴったりを指定すると、
終端文字がコピー先にコピーされません。
そのため、事前に終端文字を設定しておかないと、
未初期化の部分まで文字列扱いになります。
以下のコードでその様子をうかがえます。
#include &amp;lt;string&amp;gt; #include &amp;lt;stdio.h&amp;gt; int main(void){ char str[100] = {0}; str[0] = &#39;z&#39;; str[1] = &#39;z&#39;; str[2] = &#39;z&#39;; str[3] = &#39;z&#39;; str[4] = &#39;z&#39;; const char * c = &amp;quot;test&amp;quot;; strncpy(str, c, strlen(c)); printf(&amp;quot;%s\nlength %lu\noriginal length %lu\n&amp;quot;, str, strlen(str), strlen(c)); strcat(str, &amp;quot;.txt&amp;quot;); printf(&amp;quot;%s\nlength %lu\n&amp;quot;, str, strlen(str)); return 0; }  testz length 5 original length 4 testz.</description>
    </item>
    
    <item>
      <title>go runしても分割したファイルが認識されない</title>
      <link>http://ota42y.com/blog/2014/09/10/golang-file-split/</link>
      <pubDate>Wed, 10 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/10/golang-file-split/</guid>
      <description> Go言語では、ひとつのパッケージのソースファイルを一度にまとめてコンパイルするので、特別な決め事や宣言をすることなく、とあるファイルから別ファイル内の定数、変数、型、関数を参照することができます。
 Goコードの書き方
とのことなので、試しに以下のようにmainパッケージを分割してコンパイルしたところ、上手くいきませんでした(´･_･`)
// main.go package main import ( &amp;quot;fmt&amp;quot; ) func main() { fmt.Println(&amp;quot;main file&amp;quot;) OutputDiv() }  // div.go package main import ( &amp;quot;fmt&amp;quot; ) func OutputDiv() { fmt.Println(&amp;quot;div file&amp;quot;) }  go runの結果、分割したファイルにある関数を見つけられないエラーになります。
go run main.go # command-line-arguments ./main.go:9: undefined: OutputDiv  どうやら、go runした場合は引数のファイルのみがコンパイル対象になるため、
go run *.goか、必要なファイルを全てオプションとして渡す必要があるようです。
go run main.go div.go main file div file  </description>
    </item>
    
    <item>
      <title>golangでcronを使う</title>
      <link>http://ota42y.com/blog/2014/09/09/golang-cron/</link>
      <pubDate>Tue, 09 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/09/golang-cron/</guid>
      <description>cronというライブラリがあるので、それを使うととても簡単です。 なお、終了すると当然ながら実行しないので、 time.Sleep等で処理を止めておく必要があります。
 </description>
    </item>
    
    <item>
      <title>パスワードの文字として避けた方がいい文字</title>
      <link>http://ota42y.com/blog/2014/09/08/password-word/</link>
      <pubDate>Mon, 08 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/08/password-word/</guid>
      <description>ニンテンドー3DSのプリペイドカード入力画面を見て、
見間違えやすい文字は使わないようにしてるんだーと気づいたので、
他にもそういうのが無いかまとめました。
もちろんaやtに代表されるように、フォントによって形は全然違うため、
全ての状況に対して当てはまるとは限りませんが、
目安程度としては使えると思います。
横一列が間違える可能性の高い文字集合です。
||||||| |::| |0|o|O|Q|D| |1|7|i|I|l| |2|z|Z| |5|s|S| |6|b| |8|B| |9|q|g| |a|d| |c|C| |k|K| |u|U|v|V| |w|W| |x|X| |z|Z|
まとめると、
数字だけを扱う場合は
2 3 4 5 6 8 9 0
小文字大文字を区別する場合は
3 4 e f h j m n p r y A E F G H J L M N P R Y
小文字大文字を区別しない場合は
3 4 c e f h j k m n p r t w x w x y z</description>
    </item>
    
    <item>
      <title>gitで現在のブランチ名をクリップボードにコピーする</title>
      <link>http://ota42y.com/blog/2014/09/07/git-copy-current-branch/</link>
      <pubDate>Sun, 07 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/07/git-copy-current-branch/</guid>
      <description># copy current branch ccb = &amp;quot;!f() { echo `git symbolic-ref --short HEAD` | tr -d &#39;\n&#39; ``| pbcopy; pbpaste ; echo &#39;&#39;;}; f&amp;quot;  Jenkinsでブランチ○○をビルドしたいような場合に、
jenkinsに渡すために現在のブランチを調べるのが面倒だったので、
簡単にコピペできるようにしました。
なお、Mac限定です。</description>
    </item>
    
    <item>
      <title>CEDEC2014 9月3日まとめ</title>
      <link>http://ota42y.com/blog/2014/09/06/cedec2014-09-03/</link>
      <pubDate>Sat, 06 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/06/cedec2014-09-03/</guid>
      <description>この日だけ参加したのでまとめ
ウェアラブルコンピューティングの動向とウェアラブルゲームへの展開  ウェアラブルの特徴
 コンピュータを服のように着る 常時ON 生活密着 ハンズフリー Occlusは基本的にウェアラブルではない  ウェアラブルは実は難しい
 装着して利用してみないと解らないことがたくさん ウェアラブルは一日中身につける（≠持ち歩く） モバイル機器とはまた別のノウハウが必要なのか。  汎用vs専用
 汎用 SmartWatch 高性能高機能 まだ実用性低い 専用 アクションカメラとか　 デジカメ、音楽プレーヤー等と同じく、少しづつ汎用の性能が上がっていく  ウェアラブルゲームではリアルの遊びが重要
 ちょっとした時間に暇つぶし ジョギングなど、別のことをしながら ファッション コンピュータはバーチャル空間に伸びたので、それとは違う道   「楽しさ」の設計と評価～我々はどこで失敗し，どこへ向かうのか  楽しさの評価
 完成するよりも前に、それが面白いものなのか、どうすれば面白くなるのか？ 経験則やコンテンツに左右されない、一般的な評価手法を探すのが目的らしい  心理学的手法
 アンケートとか かなりあやふや 過去にやったゲームとか 調整しました！と言われると、変わって無くても変わったと感じる  ３つ以上のパラメータを同時に調整するのは困難
 職人はベターは出せるがベストとは限らない パラメータか困難なものはよりわからない 調整可能な項目をクリアして、調整困難な協会に時間をかけるのがベター 調整可能  ロード時間のストレス ボタン反応の応答時間等  調整困難 ユーザに書ける適切なストレスとか  人間らしいAIの自動獲得
 見ていて機械的な不自然さを感じない 相手に合わせて強さを調整できるAI  人間の生物学的制約、人為的ミス・エラーの導入 揺らぎ、遅れ、使える、技術・知識の不足、錯誤・失念   人間らしさの評価</description>
    </item>
    
    <item>
      <title>CentOS7でAndroid SDKとNDKを使う</title>
      <link>http://ota42y.com/blog/2014/09/04/centos7-android-build/</link>
      <pubDate>Thu, 04 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/04/centos7-android-build/</guid>
      <description>環境設定 sudo yum install java-1.7.0-openjdk-devel java-1.7.0-openjdk ant zip sudo yum install ld-linux.so.2 libstdc++.i686 zlib.i686  他にもいくつかあった気がしますが、
後はエラーとして出たコマンドをインストールするだけなので都度入れてください
Android SDKとNDKのダウンロード http://developer.android.com/sdk/index.html
https://developer.android.com/tools/sdk/ndk/index.html
からそれぞれ対応する物を取ってきて好きなフォルダに解凍します。
その後、sdkとndkのディレクトリ、及びsdkのplatform-toolsディレクトリをPATHに追加します。
SDK Platformのインストール Eclipseが使えないためSDK Managerは使えませんが、
代わりにコマンドラインからインストールすることができます。
android update sdk -u -a -t tools,platform-tools android update sdk -u -a -t android-19 android update sdk -u -a -t extra-google-google_play_services  後は通常のコマンドラインからビルドする方法でビルド可能です。
注意点 なお、MacやWindowsだとファイル名はcase insensitiveですが、
CentOSだと普通はcase sensitiveなので注意です。
特にCのincludeは大文字小文字を区別せずに動くため、
case sensitiveな環境に持って行った場合に特に死にやすいです…(´・_・`)</description>
    </item>
    
    <item>
      <title>mgoのConsistencyについて</title>
      <link>http://ota42y.com/blog/2014/09/03/mgo-consistency/</link>
      <pubDate>Wed, 03 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/03/mgo-consistency/</guid>
      <description>mgoのサンプルにあった
session.SetMode(mgo.Monotonic, true)
の意味がよくわからないので調べてみました。
結論から書きますと、これは複数DBを利用した際の、
データの一貫性をどの程度保証するかの設定です。
一貫性制御 mgoには複数のDB間での一貫性を制御する３種類のモードがあります。
const ( Eventual mode = 0 Monotonic mode = 1 Strong mode = 2 )  SetModeにこれを渡すことで、モードを切り替えられます。
それぞれの内容は以下の通りです。
おそらく一貫性がちゃんとしていくに従って、複雑化&amp;amp;遅くなっていきます。
Eventual Consistency 最終的に辻褄が合えばいいよね設定です。
データに変更が無く十分な時間が過ぎると、最終的に全ての更新が反映されます。
更新済みのノードと、そうでないノードが混在する可能性があるため、
どのノードから読み込むのかが固定されない場合、
新しい値を読み込んだ後に、別のノードから古い値を読み込んでしまう…
といったことが起きる可能性があるはずです。
同じノードから読み取る場合は、後述するMonotonic Consistencyと同じになると思います。
Monotonic Consistency あるプロセスが値を参照したら、以降はその値かそれより新しい値が読み込まれるという設定です。
おそらく、値を参照したタイミングで最新かどうかは保証されないが、
少なくとも古い値が読み込まれることはない、という状態だと思われます。
Strong Consistency 常に必ず最新の値が読み込めるという状態です。
一見すると良さそうですが、最新の値が読めるようになるまで読み込めないため、
注意が必要です。</description>
    </item>
    
    <item>
      <title>golangでmongodbを使う</title>
      <link>http://ota42y.com/blog/2014/09/02/go-mongodb/</link>
      <pubDate>Tue, 02 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/02/go-mongodb/</guid>
      <description>mgoというライブラリが便利そうです。
http://labix.org/mgo
以下はtwitterからツイートを取ってきて、
未登録のツイートをmongodbに保存するスクリプトです。
今のところ、検索結果が存在するかどうかを調べる方法が解らなかったので、
件数を数えてその結果をチェックしています。
 </description>
    </item>
    
    <item>
      <title>MongoDBでインデックスとexplainを使う</title>
      <link>http://ota42y.com/blog/2014/09/01/mongodb-explain/</link>
      <pubDate>Mon, 01 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/09/01/mongodb-explain/</guid>
      <description>事前データ準備  for (var i=0; i&amp;lt;100; i++) { db.test_object.save({name: &amp;quot;test1&amp;quot;, num: i}) } for (var i=0; i&amp;lt;100; i++) { db.test_object.save({name: &amp;quot;test2&amp;quot;, num: i*1000}) } for (var i=0; i&amp;lt;100; i++) { db.test_object.save({name: &amp;quot;test3&amp;quot;, num: i*10000}) }  stringは3種類100個ずつ、numはuniqueなオブジェクトを作成します。
インデックス作成 db.test_object.ensureIndex({&amp;quot;name&amp;quot;: 1})  で、test_objectコレクションのnameに対してインデックスを作成できます。
また、
db.test_object.getIndexes()  でインデックスを確認出来ます。
なお、MongoDBのばあい、インデックスに使用したキーが存在しない場合もあります。 そのような場合は、キーを持っていないものはNULLとして扱われます。
explain 検索したときにindexが使われているかはexplainで確認出来ます。
db.test_object.find({&amp;quot;name&amp;quot;: &amp;quot;test1&amp;quot;, &amp;quot;num&amp;quot;: 10}).explain() { &amp;quot;cursor&amp;quot; : &amp;quot;BtreeCursor name_1&amp;quot;, &amp;quot;isMultiKey&amp;quot; : false, &amp;quot;n&amp;quot; : 1, &amp;quot;nscannedObjects&amp;quot; : 100, &amp;quot;nscanned&amp;quot; : 100, &amp;quot;nscannedObjectsAllPlans&amp;quot; : 100, &amp;quot;nscannedAllPlans&amp;quot; : 100, &amp;quot;scanAndOrder&amp;quot; : false, &amp;quot;indexOnly&amp;quot; : false, &amp;quot;nYields&amp;quot; : 0, &amp;quot;nChunkSkips&amp;quot; : 0, &amp;quot;millis&amp;quot; : 0, &amp;quot;indexBounds&amp;quot; : { &amp;quot;string&amp;quot; : [ [ &amp;quot;test1&amp;quot;, &amp;quot;test1&amp;quot; ] ] }, &amp;quot;server&amp;quot; : &amp;quot;ota42y:27017&amp;quot;, &amp;quot;filterSet&amp;quot; : false }  cursorが BasicCursorではなく、</description>
    </item>
    
    <item>
      <title>golangでtwitter APIを使う</title>
      <link>http://ota42y.com/blog/2014/08/31/go-anaconda/</link>
      <pubDate>Sun, 31 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/08/31/go-anaconda/</guid>
      <description>anacondaが良さそう(ただしストリーミングAPI非対応)
go get github.com/ChimeraCoder/anaconda
 </description>
    </item>
    
    <item>
      <title>sending authentication information</title>
      <link>http://ota42y.com/blog/2014/08/30/sending-authentication-information/</link>
      <pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/08/30/sending-authentication-information/</guid>
      <description>Mac OS X 10.8+MySQL5.6の環境でmysqlコマンドでログインしようとすると、
&#39;sending authentication information&#39;, system error: 32&amp;quot;.
といったエラーが出て、mysqlへのログインすら不可能になってしまいました。
どうやら、MySQL5.6から
innodb_file_per_tableのデフォルト値が変わったのが原因みたいです。
my.cnfに
innodb_file_per_table = OFF
を書くことでで解決します。
参考</description>
    </item>
    
    <item>
      <title>受け取ったPOSTデータをチャットに送信するHubotスクリプトを作った</title>
      <link>http://ota42y.com/blog/2014/08/29/hubot-post-server/</link>
      <pubDate>Fri, 29 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/08/29/hubot-post-server/</guid>
      <description>実はHubotはWebサーバを立ち上げており、チャットからの入力以外にも、
Webサーバへのアクセスに対して反応することができます。
例: scripts/httpd.coffee
今回はその機能を利用し、
/hubot/send_messageにPOSTされたデータをチャットに流すスクリプトを作りました。
Hubot以外のアプリからこのURLを叩くことで、hubotの接続しているチャットに対して簡単にデータを送信できます。
 </description>
    </item>
    
    <item>
      <title>コマンドラインからiOSアプリをビルドする</title>
      <link>http://ota42y.com/blog/2014/08/28/ios-build/</link>
      <pubDate>Thu, 28 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/08/28/ios-build/</guid>
      <description>JenkinsのXCode Pluginが便利なのですが、中で何をやっているか気になったので調べたところ、
普通にコマンドラインからビルドを実行していただけなので、 使っているコマンドをまとめました。
なお、以下のサンプルプロジェクトを使って確認しました。
CustomHTTPProtocol
/usr/bin/agvtool バージョンを設定する /usr/bin/agvtool new-version -all (VERSION_NUMBER)
xcodeのプロジェクト設定の、build部分(CFBundleVersion)を変更できます。
バージョンを確認する /usr/bin/agvtool mvers -terse アプリのバージョン番号を確認出来ます。
プロジェクト設定のVersionの部分ですね。
(mversはmarketing-versionの略です)
使える証明書を確認する /usr/bin/security find-identity -p codesigning -v 1) ABCDEFGHIJKLMNOPQRSTUVWXYZ(識別子) &amp;quot;iPhone Developer: ota42y (XXXXXXXX識別子)&amp;quot; 1 valid identities found  このコンピュータで使えるcodesigningの一覧が取れます。
/usr/bin/xcodebuild 使えるSDKを確認する /usr/bin/xcodebuild -showsdks OS X SDKs: OS X 10.8 -sdk macosx10.8 OS X 10.9 -sdk macosx10.9 iOS SDKs: iOS 7.1 -sdk iphoneos7.1 iOS Simulator SDKs: Simulator - iOS 6.1 -sdk iphonesimulator6.</description>
    </item>
    
    <item>
      <title>ブラウザからArduinoを制御する</title>
      <link>http://ota42y.com/blog/2014/08/27/arduino-browser/</link>
      <pubDate>Wed, 27 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/08/27/arduino-browser/</guid>
      <description>Webブラウザから、接続されているArduinoの値を取りたいのですが、
当然ながらブラウザ本体やJavascriptにはそんな機能はありません。
そこで、Arduinoの制御をやるWebサーバをローカルに立てて、
そこに向けて通信すれば、ブラウザからもArduinoの制御ができるのでは？
と考えたところ、既にそのようなものがありました。
試した結果をまとめます。
noduino http://semu.github.io/noduino/
Node.jsでArduinoを制御できます。 duinoというNode.jsからArduinoに接続するライブラリを利用し、
Webサーバとして扱えるようにしているものみたいです。
かなりいろいろな事ができ、使いやすいように作られていますが、
ブラウザから制御するのはかなり苦労します。 ですが、Node.jsで実行する前提ならば、
簡単でいろいろなことができるため最適だと思います。
Serialport-server http://shokai.github.io/serialport-server/
RubyからArduinoにアクセスし、その結果を返すサーバです。
noduinoに比べるとできることは少ないですが、
サンプルがちゃんと動き、Webブラウザから簡単に値がとれそうです。

というわけで、ブラウザからArduinoを制御する場合は、
Serialport-serverが良さそうです。</description>
    </item>
    
    <item>
      <title>JavascriptでオプションのパースをするOptparse-js</title>
      <link>http://ota42y.com/blog/2014/08/26/optparse/</link>
      <pubDate>Tue, 26 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/08/26/optparse/</guid>
      <description>find /tmp -name core -type f -print のように、渡されたオプションを解析するのはよく行うことのため、
各言語でそれをやってくれるライブラリが作られています。
javascriptでそれにあたるのがOptparse-jsですが、
使い方がちょっと独特です。
インストール https://github.com/jfd/optparse-jsからどうぞ。
npmにも登録されているので、node.jsからも簡単に使えます。
使い方 オプション指定 どのようなオプションを宇受け取るかは配列で定義します。
なお、短縮形も同時に定義できます。
var options = [ [&amp;quot;-n&amp;quot;, &amp;quot;--name FILENAME&amp;quot;, &amp;quot;filename&amp;quot;], [&amp;quot;-t&amp;quot;, &amp;quot;--type TYPE&amp;quot;, &amp;quot;type&amp;quot;] ];  この場合、filenameを受け取る-nもしくは&amp;ndash;nameと、
typeを受け取る-tもしくは&amp;ndash;typeを定義しています。
この配列をOptionParserに渡してパーサー用オブジェクトを受け取ります。
var parser = new optparse.OptionParser(options);  オプション処理 オプションを受け取ったときの処理は、パーサーオブジェクトに関数を渡して登録します。
第一引数にオプション名、第二引数にそのオプションがあったときの処理を渡します。
parser.on(&amp;quot;name&amp;quot;, function(opt, value) { parameter[&amp;quot;name&amp;quot;] = filename; });  パース実行 パーサーオブジェクトのparseメソッドでパースを実行できます。 ただし、スペースを自分で区切ってはくれないため、
先に配列に分けておく必要があります。
parser.parse(command_str.split(&amp;quot; &amp;quot;));  (勝手にオプション足したり、区切り文字を自由に設定できる、こうしていると思われます)
サンプルコード var optparse = require(&#39;optparse&#39;); var command_str = &amp;quot;-name core -type f&amp;quot;; var options = [ [&amp;quot;-n&amp;quot;, &amp;quot;--name FILENAME&amp;quot;, &amp;quot;filename&amp;quot;], [&amp;quot;-t&amp;quot;, &amp;quot;--type TYPE&amp;quot;, &amp;quot;type&amp;quot;] ]; var parser = new optparse.</description>
    </item>
    
    <item>
      <title>スマホアプリ開発でgit-new-workdirがとても便利だった</title>
      <link>http://ota42y.com/blog/2014/08/25/git-new-workdir/</link>
      <pubDate>Mon, 25 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/08/25/git-new-workdir/</guid>
      <description>まとめ  ビルドに時間がかかる環境下では、ビルドのキャッシュが重要 ブランチを切り替えると、ブランチ間の差分だけキャッシュが聞かなくなる 別ブランチでちょっとした修正を行おうとすると、移動と戻りとで二回ビルドし直しが起きる リポジトリを複数cloneすると、管理が面倒 git-new-workdirなら、.gitを共用しつつ、別々のフォルダで作業できる そのため、ビルドのキャッシュを最大限活用できる スマホアプリじゃなくても便利  ビルドのキャッシュ Objective-cやJava、C++でスマホアプリを作成する場合、
どの言語もコンパイルが必要になります。
このコンパイルにかかる時間は、アプリの規模が大きくなるにつれて長くなっていきます。
ただし、多くの場合はキャッシュ機能が有効になっており、
変更したファイルのみがコンパイルされるため、
フルビルドをしなければ気にならないはずです。
gitのチェックアウトによるファイル変更問題 gitの場合、ブランチを切り替えると関連するファイルに対して全部変更が走ります。
これにより、ブランチ間の差分が全てビルド対象になってしまいます。
当然と言えば当然ですが、例えばちょっとした変更を別ブランチで行い多場合、
別ブランチに切り替えて一回、作業後戻ってきて一回と、二回ビルドが必要になります。
ブランチ間の差次第ですが、5分で終わる作業のために30分のフルビルド2回…
なんてこともあり、時間が勿体ないです。
かといって、リポジトリを二つ別々のフォルダにcloneすると、
別々にリポジトリ更新したり、容量圧迫、間違えてコミットしたときにcherry-pickできない等々
いろいろと問題が起きます。
こういった場合、git-new-workdirを使うことで、 だいぶ楽に解決することができます。
git-new-workdirとは これは同じ.gitを使って、複数のディレクトリにリポジトリをcloneできる機能になります。
これにより、二つの別々のフォルダにそれぞれgit cloneしたのと同じ状態を維持しつつ、
片方で作成したコミットや、ローカルブランチ、git stashなどをもう片方で参照することができます。
また、gitのワーキングディレクトリやステージングエリアは共用されないため、
コミットしていない変更が別のディレクトリに影響をあたることはありません。　インストール git/contrib/workdir/git-new-workdirに入っていたので、
export PATH=/usr/local/share/git-core/contrib/workdir/:$PATH でパスを通しました。
使い方 git-new-workdir repository_folder new_workdir branch_name(オプション)です。
後述するように、gitのcloneは行わず、元のディレクトリへのシンボリックリンクを張るだけなので、
ほぼcheckout時間しかかかりません。
新しくできたworkdirは、普通にgit cloneしたときと同じように扱えます。
注意点 同じブランチを同時に変更すると、予期せぬ結果になるので注意が必要です。
なお、片方からもう片方が何をチェックアウトしているかはわからないため、
main/subといった使い方や、develop/releaseといった風に分けたり、
必要な時だけ作るといった方が事故らないと思います。
また、新しいディレクトリの.gitには、HEADとindex、logの3つのみが新たに作られ、
残りは全て元の.gitの該当ディレクトリへのシンボリックリンクになっています。
そのため、高速に新しいworkdirを作れますが、
元のリポジトリを消すとworkdirの方もおかしくなると思われます。</description>
    </item>
    
    <item>
      <title>selenium-webdriverでRuby からブラウザを操作する</title>
      <link>http://ota42y.com/blog/2014/08/24/selenium/</link>
      <pubDate>Sun, 24 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/08/24/selenium/</guid>
      <description>まとめ  selenium-webdriverでプログラムからブラウザを制御できる ページのロードをまったり、ページ内のJSを実行したりできる。 rubyのgemがあり、簡単に導入できる Firefoxならgemを入れるだけで動かせる  Rubyのスクレイピング方法 Rubyでスクレイピングする方法としては、 NokogiriやMechanizeが存在します。
どちらもHTMLを解析してページの要素にアクセスします。
ですが、実際のWebブラウザのエンジンとは違うため、若干の差異があったり、
Javascriptで動的に変化するページなどにちゃんと対応するのはとても大変です。
そこで、Webブラウザを直接操作してスクレイピングするライブラリの一つが、
selenium-webdriverになります。
selenium-webdriverのインストール Rubyではselenium-webdrierのgemをインストールするだけで完了です。
なお、Google Chromeだとgem以外にもう一つインストールするものがありますが、
今回は説明が面倒なので省略し、Firefoxで動かします。
使い方 Selenium::WebDriver.for :firefox で、firefoxのウインドウに対応するWebDriverオブジェクトが取得できます。
このWebDriverに対して、移動先のURLやDOM要素の取得などを行えます。
また、細かい設定などの変更のために、Profileというものが用意されています。
これに対して様々な設定をし、WebDriver作成時に設定することで、
ブラウザの挙動を変更出来ます。
以下は、pdfを開いたときに特定のフォルダに保存する設定がされた状態で、
Googleでpdfを検索し、先頭の一つをdownフォルダにダウンロードするコードになります。
require &amp;quot;selenium-webdriver&amp;quot; profile = Selenium::WebDriver::Firefox::Profile.new profile[&#39;browser.download.dir&#39;] = &amp;quot;#{File.expand_path(File.dirname(__FILE__))}/down&amp;quot; profile[&#39;browser.download.folderList&#39;] = 2 profile[&#39;browser.download.useDownloadDir&#39;] = true profile[&#39;browser.helperApps.neverAsk.saveToDisk&#39;] = &amp;quot;application/pdf&amp;quot; profile[&#39;pdfjs.disabled&#39;] = true driver = Selenium::WebDriver.for :firefox, :profile =&amp;gt; profile driver.navigate.to &amp;quot;https://www.google.com/search?as_q=&amp;amp;as_epq=&amp;amp;as_oq=&amp;amp;as_eq=&amp;amp;as_nlo=&amp;amp;as_nhi=&amp;amp;lr=&amp;amp;cr=&amp;amp;as_qdr=all&amp;amp;as_sitesearch=&amp;amp;as_occt=any&amp;amp;safe=images&amp;amp;as_filetype=pdf&amp;amp;as_rights=&amp;amp;gws_rd=ssl&amp;quot; links = driver.find_element(:class, &#39;r&#39;).find_elements(:tag_name, &#39;a&#39;) if links links.each do |element| if element.</description>
    </item>
    
    <item>
      <title>mocha&#43;chai&#43;sinsonでテストを書く為に必要な最低限の知識</title>
      <link>http://ota42y.com/blog/2014/08/22/mocha-test/</link>
      <pubDate>Fri, 22 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/08/22/mocha-test/</guid>
      <description>まとめ  Mochaではrspecっぽい感じにテストが書ける ただし、done()を呼ぶ必要がある等、細かい部分に差異がある sinonにはいろいろ便利機能がある  Mochaの使い方 coffeescriptを前提にしています。
テストの書き方 Mochaのテストは以下のように、itにテスト内容を書いた関数を渡し、
そのitを呼び出す関数をdescribeに渡すしようです。
describe &amp;quot;test root&amp;quot;, -&amp;gt; it &amp;quot;name&amp;quot;, (done) -&amp;gt; assert.equal getUserName, &amp;quot;user&amp;quot; done()  ただし、it関数では必ずdone()を呼び出す必要があります。
これを呼ばない場合は終了を待ち続け、
一定時間後にタイムアウトしてテストが失敗した扱いになります。
beforeの使い方 rspecのbeforeにあたるものは、beforeEachになります。
なお、変数を他のブロックに渡したい場合、
以下のようにdescribeの中に変数名を書いておいて、
beforeEachのなかで設定する必要があるみたいです。
参考
describe &amp;quot;test&amp;quot;, -&amp;gt; room_name = undefined beforeEach (done) -&amp;gt; room_name = &amp;quot;test_room&amp;quot; done() describe &amp;quot;functions&amp;quot;, -&amp;gt; it &amp;quot;executeNoteShow&amp;quot;, (done) -&amp;gt; assert.equal getRoomName(room_name), room_name done()  pendingテストの作り方 テストの用意はしたけど、とりあえずpendingにしておきたい場合は二通りの方法があります。
describe &amp;quot;functions&amp;quot;, -&amp;gt; it &amp;quot;pending test&amp;quot; // 関数を渡さない場合 // it.</description>
    </item>
    
    <item>
      <title>IntelliJ IDEAで node.jsとmochaを使ってテストする</title>
      <link>http://ota42y.com/blog/2014/08/21/intellij-node/</link>
      <pubDate>Thu, 21 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/08/21/intellij-node/</guid>
      <description> node.js+mocha+coffeescriptな環境で開発をしていたところ、
IntelliJ IDEAの設定方法が調べても無かったのでメモ。
Node.jsプラグインのインストール Node.jsプラグインは公式から提供されています。
そのため、IntelliJ のPreferences からPluginsを選び、
NodeJSプラグインを選択するだけでインストール出来ます。
ビルド設定 以下の画像の通りです。
 Node interpreter
node.jsの実行ファイルの位置を設定します。 working directory
対象のディレクトリ Mocha package
Mochaの実行ファイルの位置 Extra Mocha options
Mochaの設定を指定します 詳しくは後述 Test Directory
Mochaのテストが入っている場所  Mochaのオプション 私の環境では主に次のような設定をしています、
 coffeescriptを利用している spec形式で出力 共通で読み込むファイルがある  これは、以下のオプションを入れることで実現できます。
--compilers coffee:coffee-script/register --reporter spec --require coffee-script --require test/test_helper.coffee --colors  問題点 以上で設定は終わりですが、いくつか問題点があります。
 デバッガが動かない
coffeescriptから変換してるので、
ブレークポイントがうまく動きません エラーになったテストに飛べない
テストがエラーになったとしても、そのテストの位置に飛ぶ機能がありません。
最も、IDEを使わずに開発している時と同じく、
テストメッセージを頼りにテストファイルに移動すればいいので、
大きな問題ではありませんが。  </description>
    </item>
    
    <item>
      <title>Pixivの検索フィルタ作った</title>
      <link>http://ota42y.com/blog/2014/08/20/pixiv-follow-filter/</link>
      <pubDate>Wed, 20 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/08/20/pixiv-follow-filter/</guid>
      <description>まとめ
 Pixivの検索は全ユーザから ○○が好きな新しいユーザを探したい時に既にフォローしている人はノイズになる 検索結果からフォローしている人を非表示にする拡張作った DLはここから  Pixivのイラスト検索でフォロワーを除外できない Pixivでイラスト検索をした場合、全ユーザを対象に検索が行われるため、
検索結果にフォローしてる人とそうでない人が交じります。
私の場合、フォロー新着作品を全てチェックしているため、
検索をする場合は、このイラスト書いてる新しい人を見つけたい！といった目的で行うことがほとんどです。
そのため、フォロワー以外からの検索を行いたいのですが、
残念ながらPixivにそのような機能はありませんでした。
というわけで、Chrome拡張で実現しました。
フィルタの基本機能 DLはここから。
検索結果にはユーザのユニークIDが含まれているため、
自分のフォローしている人と一致していれば非表示にしています。
また、毎回通信するのは無駄が多いため、
事前にフォローしている人のIDをローカルに保存し、そこから読み出しています。
そのため、ポップアップウィンドウから定期的にデータの更新をする必要があります。
ソースコード https://github.com/ota42y/pixiv_follower_filter</description>
    </item>
    
    <item>
      <title>chrome extensionでデータを保存する</title>
      <link>http://ota42y.com/blog/2014/08/17/local-storage/</link>
      <pubDate>Sun, 17 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/08/17/local-storage/</guid>
      <description>Chrome拡張でデータを保存しようとした場合に、
最も簡単に扱えるのが、localStorageです。
manifest.jsonの設定 manifest.jsonに以下のパーミッションを追加してください
&amp;quot;permissions&amp;quot;: [ &amp;quot;storage&amp;quot;, &amp;quot;unlimitedStorage&amp;quot; ],  使い方 localStorageという辞書型の変数が定義されるので、
それに対して読み書きを行うだけで大丈夫です。
localStorage[‘data’] = ‘aaa&#39; var data = localStorage[‘data’]  注意点 保存形式の制約 この方法で保存できるのは文字列だけになります。
true/falseを保存しても文字列として出てきますし、
オブジェクトを入れても正しく保存されません。
このような場合、JSON.stringifyとJSON.parseを使い、
JSONに変換して保存すると解決します。
読み出せる場所の制約 localStorageはページごとに保存するデータがわかれているため、
読み書きは同じ場所で行う必要があります。
基本的にはbackgruond.jsで読み書きを行い、
content_scriptsやpopupからbackground.jsを呼び出すのがいいと思います。
popupからは以下のように、background.jsの関数を簡単に呼び出すことができます。
// background.jsのgetDataを呼び出す var data = chrome.extension.getBackgroundPage().getData();  content_scriptsからは直接アクセスできないため、
メッセージを通して呼び出す必要があり、少々面倒です。
まず、background側にメッセージ受け取りと、コールバックを定義します。
content_scriptsからのメッセージ内容がrequestに入っているので、
それによって処理を分けています。
chrome.runtime.onMessage.addListener(function(request, sender, sendResponse) { if (request.method == &amp;quot;getUser&amp;quot;){ sendResponse({user_id: localStorage[request.user_id]}); }else{ sendResponse({}); } });  content_scriptから呼び出す場合は以下のように、
データとコールバックを渡します。
受け取り側でsendResponse()を読んだときの引数がresponseに入るので、
それを利用してデータを取り出します。
chrome.runtime.sendMessage({method: &amp;quot;getUser&amp;quot;, user_id: user_id}, function(response) { if(response.</description>
    </item>
    
    <item>
      <title>AutomatorでEvernoteのノートリンクを置き換える</title>
      <link>http://ota42y.com/blog/2014/08/16/evernote-url/</link>
      <pubDate>Sat, 16 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/08/16/evernote-url/</guid>
      <description>まとめ  シェルスクリプトをAlfredから直接実行すると無駄にターミナルが立ち上がる Automatorならターミナルを立ち上げずにスクリプトを実行できる  Automatorでスクリプトを実行する 前回(Evernoteのノートリンクが仕様変更で使い物にならないので何とかしようとした) では、
スクリプトの実行方法が微妙たったのでもう少し修正してみました。
結論としては、Macと標準で入っているAutomatorを使うことで、
ターミナルアプリを立ち上げずにシェルスクリプトを実行できます。
Automatorのワークフローを作る 単体のアプリケーションとして実行するため、
Automatorを立ち上げアプリケーションを選択します。
実行ディレクトリの取得 私の環境では、clipboardのインストール先はグローバルではなく、
スクリプトのあるフォルダにbundler専用のディレクトリを作り、
そこにインストールしています。
そのため、スクリプトを実行するためにはそのパスに移動しないといけません。
ですが、Automatorを実行した場合、
カレントパスはユーザのホームディレクトリになり、
実行したファイルのディレクトリではありません。
参考: iNSTANTWiNE or Wine.framework と Automator の連携
そのため、Apple Scriptを先に実行し、
Automatorの実行ファイルがあるパスを求め、
そこに移動するようにしています。
自分自身のパスを求めるApple Script on run {input, parameters} set p to POSIX path of (path to me) return {p} return input end run  AutomatorのAppleScriptを実行を選び、
このスクリプトを後述するシェルスクリプトの前に実行するようにします。
これでシェルスクリプトに引数としてパスを渡せます。
Evernote URLを置換するスクリプト 内容はほぼ前回と同じです。
ただし、各種環境設定は読み込んでくれないため、
sourceで読み込む必要があります。
また、Automator側で引数の引き渡し方法を
引数としてに設定する必要もあります。
source .zshrc pushd $(dirname $1) bundle exec ruby -e &amp;quot; require &#39;clipboard&#39; url = Clipboard.</description>
    </item>
    
    <item>
      <title>sortやfind_ifの条件指定はどういう動きをしているのか</title>
      <link>http://ota42y.com/blog/2014/08/11/find-if/</link>
      <pubDate>Mon, 11 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/08/11/find-if/</guid>
      <description>C++でsortやfind_ifでは、イテレータのbeginとendに加えて、
関数orオブジェクトを渡すことで条件を自由に設定できます。
参考
このとき内部がどういう処理をして渡した関数を呼び出しているのかが気になったので調べました。
特に、第三引数にクラスを渡す場合、何故()オペレータに条件を書く必要があるのかが気になりました。
結論としては、実装を見れば簡単に解決しました。
内部実装 find_ifの場合 template &amp;lt;class _InputIter, class _Predicate&amp;gt; inline _InputIter find_if(_InputIter __first, _InputIter __last, _Predicate __pred, input_iterator_tag) { while (__first != __last &amp;amp;&amp;amp; !__pred(*__first)) ++__first; return __first; }  ソートの場合 ソートは少々複雑なので、該当部分だけ抜き出します。
template &amp;lt;class _RandomAccessIter, class _Tp, class _Compare&amp;gt; void __unguarded_linear_insert(_RandomAccessIter __last, _Tp __val, _Compare __comp) { _RandomAccessIter __next = __last; --__next; while (__comp(__val, *__next)) { *__last = *__next; __last = __next; --__next; } *__last = __val; }  まとめ 動きとしてはとても簡単で、比較が必要なところで引数を実行しているだけです。</description>
    </item>
    
    <item>
      <title>Evernoteのノートリンクが仕様変更で使い物にならないので何とかしようとした</title>
      <link>http://ota42y.com/blog/2014/08/10/evernote-link/</link>
      <pubDate>Sun, 10 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/08/10/evernote-link/</guid>
      <description>まとめ  Evernoteのノートリンクの仕様が変わって使い物にならなくなった 前の仕様でも動くので変換スクリプトを書いた ただし、Macではスクリプトの実行方法で問題あり  Evernoteのノートリンク仕様の変更 Evernoteでは図のように、「ノートリンクをコピー」で特定のノートに一対一対応するURIを取れます。
この機能でコピーされるのは、これまでは evernote:///という独自スキームが使われていました。
そのため、開こうとするとEvernoteアプリが立ち上がり、対応するノートを開いてくれていました。
AndroidやiPhoneでもEvernoteアプリをインストールしてあれば、
同じようにアプリでノートを開いてくれるため、とても便利でした。
しかし、最近の変更により、https://www.evernote.com/ へのリンクに変わってしまい、
ブラウザが起動してノートがEvernoteのサイト上で表示され、
そこからさらにEvernoteのアプリが開くという挙動になりました。
これにより、モバイル端末などではEvernoteへの通信時間が余計にかかるようになったうえに、
オフライン状態ではノートを開くことができなくなってしまいました。
正直使い物にならないレベルになってしまったのですが、
幸いなことにevernote:///スキームは未だ有効です。
また、必要な情報はどちらにも含まれているため、URIの変換スクリプトを書くことで対応可能です。
リンク置換スクリプトの作成 ノートリンクの仕様 以下は予想であり、正確なものではありません
Evernoteのhttpsリンクは以下のようになっています。
https://www.evernote.com/shard/s2/nl/ユーザ固有の文字列/ノート固有の文字列/
対して、evernote:///スキームは以下のようになっています。
evernote:///view/ユーザ固有の文字列/s2/ノート固有の文字列/ノート固有の文字列/
(ノート固有の文字列は二回とも同じ値になります)　どちらもユーザ固有の文字列とノート固有の文字列は含まれているため、相互に変換可能です。
Windowsならクリップボードの置換ソフトはたくさんあるのですが、
Macでは残念ながらまともな物はありませんでした。
一応、ClipMenuは正規表現で置換ができますが、
それを実行するまでにかなりキーストロークが必要なため、見送りました。
理想は自動置換、最低でもスクリプトを簡単に実行できるのが最低条件です。
リンク置換スクリプト 結局、クリップボードのURLをevernote:///に変換するスクリプトを作りました。
事前にclipboard gemをインストールしておく必要があります。
#!/bin/bash pushd $(dirname $0) bundle exec ruby -e &amp;quot; require &#39;clipboard&#39; url = Clipboard.paste r = Regexp.new(\&amp;quot;https://www.evernote.com/shard/s2/nl/([0-9]*)/(.*)/\&amp;quot;) result = r.match url if result new_url = \&amp;quot;evernote:///view/#{result[1]}/s2/#{result[2]}/#{result[2]}/\&amp;quot; p new_url Clipboard.copy new_url end &amp;quot; popd  これをevernote.</description>
    </item>
    
    <item>
      <title>IRCはCL-LFでメッセージを区切る</title>
      <link>http://ota42y.com/blog/2014/08/09/irc/</link>
      <pubDate>Sat, 09 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/08/09/irc/</guid>
      <description>IRCのプロトコルはRFCで決められていて、
一つのメッセージが512文字以下、CR-LFで区切られている前提らしいです。
http://tools.ietf.org/html/rfc1459.html#section-2.3
が、古いやつとかは対応してない場合もあるらしく、
メッセージを受け取る部分は両方に対応できるようにした方がいいとのことです。
http://tools.ietf.org/html/rfc1459.html#section-8
と、ちょうどLFしか送ってこないIRCサーバに、
node-ircで接続したらはまったのでメモ。
修正プルリクは出したけど、メンテ止まってる予感が…
https://github.com/martynsmith/node-irc/pull/246</description>
    </item>
    
    <item>
      <title>MessagePack って何？</title>
      <link>http://ota42y.com/blog/2014/08/05/msgpack/</link>
      <pubDate>Tue, 05 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/08/05/msgpack/</guid>
      <description>最近よく聞くMessagePackとは何かを調べたのでメモ。
MessagePackとは バイナリでデータを保存するフォーマットです。
JSONと比べると、保存した状態の可読性を犠牲にする代わりに、
より早くて小さいフォーマットになっています。
また、汎用的なフォーマットのため、
いろんな言語(nodeとかrubyとかcppとか) で相互にデータを使えます。
他のバイナリシリアライズフォーマットとの差 MessagePackはバッファをうまく使って早かったり、 読み込み途中でもデシリアライズできるとか、
結構いろいろと高速化のための工夫がされています。
(ただし、実装によって若干違いがあるようです)
使い方 rubyならgem install msgpackで、 MacのC++ならbrew install msgpackで使えるようになります。
なお、C++はgccに以下のオプションをつける必要があります。
g++ test.cpp -lmsgpack -o test
また、基本型は全てシリアライズ可能で、保存したいクラスのメンバ変数を、
MSGPACK_DEFINEマクロに入れると、
自動でシリアライズ/デシリアライズできるみたいです。
コード C++ #include &amp;lt;msgpack.hpp&amp;gt; #include &amp;lt;vector&amp;gt; #include &amp;lt;string&amp;gt; #include &amp;lt;fstream&amp;gt; #include &amp;lt;iostream&amp;gt; using namespace std; struct User { User(std::string name, int id, std::vector&amp;lt;int&amp;gt; follower) : name(name) , id(id) , follower_id(follower) {} // 引数なしのコンストラクタは必須 User() : id(0) {} MSGPACK_DEFINE(name, id, follower_id); std::string name; int id; std::vector&amp;lt;int&amp;gt; follower_id; string toString(){ string ret = &amp;quot;name: &amp;quot;; ret += name; ret += &amp;quot; id: &amp;quot;; ret += std::to_string(id); ret += &amp;quot; follower: &amp;quot;; for(int i=0; i&amp;lt;follower_id.</description>
    </item>
    
    <item>
      <title>Jenkinsで複雑な処理をするときのjob構成について</title>
      <link>http://ota42y.com/blog/2014/07/21/jenkins-architecture/</link>
      <pubDate>Mon, 21 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/07/21/jenkins-architecture/</guid>
      <description>変数やビルド後の通知設定等が違うために、
ほぼ同じ内容のjobを15個ぐらい使っていたら、
だんだんと運用が死んできたのでメモ。
まとめ Jenkinsはプラグインの挙動を変えるのが難しいため、
ほぼ同じだけれど若干違う手順を行う必要がある場合、
似たようなjobが大量に並び、
手順の変更時などに全てのjobを変えきるのが辛くなります。
そこで、可能な限りビルド手順はスクリプトにするのと、
jobを種類別に細かく分け、
最上位のjobにはどの下流jobを実行するかだけを管理させることで、
複雑なjobでも変更に強くすることができるようになります。
前提条件  設定が全部で7種類ぐらいある 1ビルドはCPUをフルパワーで使って30分程度 一部の設定は定期実行やマージ毎でもビルドしたい 設定ごとに用途や使う頻度が違う為、定期実行タイミングは個別に設定したい ビルド後のデプロイ先や通知先、通知条件がいくつかある ビルド手順はバージョン/デプロイ先によって変化する  問題点 大きな問題として、
Jenkins pulginで提供されている機能を切り替えるのが難しいという問題があります。
たとえば、Jenkinsの拡張メールプラグインでは、
そのjobの結果によって通知先や通知内容等を変更することができます。
ですが、その定義自体を切り替えることが難しく、
この場合は成功時にメールしないけど、
この場合は成功時にメールするといった切り替えができませんでした。
また、定期的に変更チェックするブランチや、チェックする間隔が設定毎に異なりますが、
JenkinsのGit プラグインではそのあたりをうまく設定することが難しいようです。
他にもdeploy先によって使うプラグインが微妙に違うなどの問題があり、
ほぼ同じだけれど若干違うjobを15個並べるといった運用をしていました。
こんな感じに、ほぼ同じだけれど若干違うビルドが並びます。
結果として、ビルド手順の変更時などに全てのビルドを変更しきることが難しかったり、
古い手順のバージョンのマイクロアップデートと、
新しい手順のメジャーアップデートが重なったときに、
ビルド前にJenkinsの設定を必ず変更する必要がある等、
運用がかなり辛くなってきました。
これに対する対策を考えましたが、あまり良いのが無い感じですが、
とりあえずまとまったのでメモをしておきます。
解決案 1.ビルド手順や通知を全てをシェルスクリプトにし、Jenkinsはシェルを叩くだけにする Jenkinsのプラグインをほぼ使用せず、全てスクリプトで解決する手法です。
ビルド手順や通知がSCMに保存されるため、ビルド手順とバージョンが完全に紐付き、
常に正しい手順で実行できるという利点があります。
だだし、Jenkinsプラグインの恩恵を得られないため、
今プラグインでやっている処理を全て置き換える必要があります。
また、新たな手順の追加が大変と言った問題もあります。
2.ワークスペース共有を使ってうまくやる チェックアウトとビルドジョブを切り離し、
SCMのポーリング&amp;amp;チェックアウトだけを行うjobから、
ビルドジョブを下流ビルドとして呼び出す方式です。
こんな感じですね。
ワークスペース共有を使い、
下流ビルドは上流のワークスペース上でビルド作業を行うようにすることで、
ビルドjobの数を大幅に減らすことができます。
だだし、下流が実行中は上流ビルドが動かない事を保証しなければなりません。
(ビルド途中に次のキューによってワークスペースが書き換えられる可能性がある) Parameterize pluginで下流が終わるまで終了を待つ事ができますが、
複数のビルドが同時に走った場合にデッドロックに陥る可能性があります。 - A-Cとビルドするjobと、B-Cとビルドするjobがあり、jenkinsの同時実行が2の場合、
2つのビルドを同時に実行するとCが2つともキューに積まれて進まない場合がある
3.jobを疎結合にして成果物を使ってうまくやる 2のデッドロックを回避するために、疎結合にした版になります。</description>
    </item>
    
    <item>
      <title>2進数の引き算で補数を使う</title>
      <link>http://ota42y.com/blog/2014/07/18/2jin-shu-falseyin-kisuan-debu-shu-woshi-u/</link>
      <pubDate>Fri, 18 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/07/18/2jin-shu-falseyin-kisuan-debu-shu-woshi-u/</guid>
      <description>2進数の引き算で補数表現を使うと足し算にできることの説明が見あたら無かったのでやってみる
ここでは2の補数表現の求め方がすでに解っている前提で進める、
(要するに1000-101=010+1=011は相互に変換可能ということ)
結果がプラスになる場合
x = 5 - 2 (10進数) x = 101 - 010 (2進数に変換) x + 1000 = 101 - 010 + 1000 (両辺に+1000) x + 1000 = 101 + (1000 - 010) x + 1000 = 101 + 110 (2の補数表現) x + 1000 = 1011 x = 1011 - 1000 x = 11 x = 3 (10進数に変換)  結果がマイナスになる場合
x = 2 - 5 (10進数) x = 010 - 101 (2進数に変換) x + 1000 = 010 - 101 + 1000 (両辺に+1000) x + 1000 = 010 + (1000 - 101) x + 1000 = 010 + 011 (2の補数表現) x + 1000 = 101 x = 101 - 1000 x = - (1000 - 101) x = - (11) (2の補数表現) x = - (3) (10進数に変換) x = - 3  </description>
    </item>
    
    <item>
      <title>GitHub Kaigiに行ってきた</title>
      <link>http://ota42y.com/blog/2014/06/03/github-kaigi/</link>
      <pubDate>Tue, 03 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/06/03/github-kaigi/</guid>
      <description>スライドまとめ  GitHub実践入門は活用するためのガイドブック
 はてなブログチームの開発フローとGitHub
 OSS と GitHub
 How GitHub Works (GitHub Kaigi, Tokyo, 2014)
 GitHubで雑誌・書籍を作る
 Atom, the Programmable Text Editor
 入門書には載ってない Git &amp;amp; GitHub Tips
  LTは用事があって聞けませんでした…(´･_･`)
感想 全体的な感想 githubのハックや事例はもちろんですが、
どちらかというとgithubを利用してのワークフローとか、
エンジニアを含む環境がどう変化したか的なお話が多かったです。
一番はじめの、GitHub実践入門や、Gitのティップスに関しては、
すぐに導入したいような内容ばかりでした。
特に前者は凄くいい本っぽかったので(載せられてる)、会場で注文しましたw
Atomの発表が凄かった 他にもいろいろ発表がありましたが、特に私にヒットしたのは、Atomのプレゼンでした。
AtomはChromiumベースで、エディタ部分をHTML5/CSSで作っているので、
Developer Toolsを使ってWebサイトのように、Atomのデザインを変えられます。
プレゼン中に変更している様子を見せてもらいましたが、
本当に何でもリアルタイムで変更できて凄く未来を感じました。
また、DOM要素を直接いじることで、APIが提供されていなくても、
画面上の全ての要素(Macのメニューバーに出てる部分ですら！）を
自由に変更することが可能で、javascriptをコンソールに書いて、
新しいメニューがついたのは本当に度肝を抜かれました。
スライドの資料には映像がないため、
後日映像がアップロードされるのを待つしか無いのが凄く残念ですが、
とにかく未来を感じるエディタでした。
もの凄くおもしろかったので、第二回開催はまだですかね。</description>
    </item>
    
    <item>
      <title>Webで何かを作る人誰もが読むべき本だった - ハイパフォーマンス ブラウザネットワーキング</title>
      <link>http://ota42y.com/blog/2014/05/13/browser-networking/</link>
      <pubDate>Tue, 13 May 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/05/13/browser-networking/</guid>
      <description>丸一日かけて読んだので書評っぽいのを。
Webで何かしようとする人はどのレイヤの人も必読だと思います。
インフラはもちろんのこと、アプリを作っている人も知らないでは済まされません。

ハイパフォーマンス ブラウザネットワーキング ―ネットワークアプリケーションのためのパフォーマンス最適化
#概要
目次としてはこんな感じで、TCP/UDPから無線ネットワークの特性から最新のプロトコルまで、
今のインターネットに触れる場合に触りそうな部分を、だいたいカバーしてます。
特にTCPとHTTP、WebRTCについてはとてもしっかりと書かれていました。
また具体的な最適化や、目次に乗っていない関係する技術に関してはかなりばっさり切り落としており、
取り扱っている内容の割にはすんなり読むことができました。
1章　レイテンシ・帯域幅入門 2章　TCPの構成要素 3章　UDPの構成要素 4章　TLS II部　ワイヤレスネットワークのパフォーマンス 5章　ワイヤレスネットワーク入門 6章　WiFi 7章　モバイルネットワーク 8章　モバイルネットワークの最適化 III部　HTTP 9章　HTTPの歴史 10章　Webパフォーマンス入門 11章　HTTP 1.x 12章　HTTP 2.0 13章　アプリケーション配信最適化 IV部　ブラウザAPIとプロトコル 14章　ブラウザネットワーク入門 15章　XMLHttpRequest 16章　Server-Sent Events 17章　WebSocket 18章　WebRTC  本のレベルとしては、3ウェイ・ハンドシェイクの基礎から教えてくれたりと、
かなり詳細に説明してくれるため、TCPやHTTPの詳しい知識は必要ありません。
だだし、ネットワークとは何かとかサーバとは何かとかの説明はないので、
最低限のネットワークの知識は必須です。
#感想
この本で特にいいと思ったところは、定義通りの何もしていない状態から、
順序立てて一つ一つ効率良くしていく様子を示している所です。
これはキープアライブからHTTPパイプラインの説明のところが顕著ですが、
まずはじめに何も考えずに愚直にやって、どれだけ無駄が多いかを説明し、
次にキープアライブを有効にすることでこれだけ無駄が省ける、
さらにHTTPパイプラインでこんなに無駄が省ける。</description>
    </item>
    
    <item>
      <title>Atomのプラグイン作の作り方その１</title>
      <link>http://ota42y.com/blog/2014/04/08/atom-plugin1/</link>
      <pubDate>Tue, 08 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2014/04/08/atom-plugin1/</guid>
      <description>Github製のエディタAtomの招待が来ました。
簡単なプラグインの作り方が公開されていましたので、 自分でも作ってみました。
[Create Your First Package]https://atom.io/docs/latest/your-first-package)
#テンプレート作成 Atomはプラグインのテンプレートを作る仕組みが入っています。 コマンドパレットから
Generate Package  を選択して実行します。
するとパッケージ名を聞かれます。
適当に入力すると(ここではデフォルトのmy-package)、 ひな形がエディタで開きます。
#コマンドを作る コマンドパレットに表示されて実行可能なコマンドを作ります。
Atomの処理はCoffeeScriptで書きます。
メイン部分はlib/my-package.coffeeなので、 このファイルを編集します。
基本的に、module.exports =以降を書き換えればいいみたいです。
この中にはテンプレートを作った段階でいろいろ書いてありますが、
基本的に全部削除して大丈夫です。
module.exports = activate: -&amp;gt; # コマンドと実行する関数を登録する atom.workspaceView.command &amp;quot;my-package:hello&amp;quot;, =&amp;gt; @hello() hello: -&amp;gt; # 今いるパネルを得る editor = atom.workspace.activePaneItem # 文字の挿入 editor.insertText(&#39;Hello, World!&#39;)  package jsonに &amp;ldquo;activationEvents&amp;rdquo;が存在するので、
その値をさっき登録したコマンドを入れた配列 [&amp;quot;my-package:hello&amp;quot;] に変更します。
 チュートリアルにはこれをしないと、
コマンドパレットに出ないと書いてありますが、
atom.workspaceView.command &amp;quot;test-pkg:test&amp;quot;, =&amp;gt; @test()
を実行するだけで出てくるので、必ずしも必要ではなさそうです
(ただし、書いてあった方が安心できそう)
 変更を反映するために、コマンドパレットから
Window:Reload を実行します。
そうすると、コマンドパレットにmy-package:helloが出てくるので、
実行すると、今のカーソルの部分にHello, World!が挿入されます。</description>
    </item>
    
    <item>
      <title>Railsはアクセスをどう処理しているのか(1)</title>
      <link>http://ota42y.com/blog/2013/11/10/rails-trace-1/</link>
      <pubDate>Sun, 10 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2013/11/10/rails-trace-1/</guid>
      <description>ふと、Railsのコントローラーに書いたコードがが実行されるまでに、 何が起きているのか気になったので、全部追ってみようと思います。
まだ全部追い切れてないですが、思った以上に長くなったのでとりあえずメモとして出 してみます。
一部Rails力やRuby力が足りなくて追い切れない部分がありますが(´･_･`)
##準備
rails g controller Trace index create app/controllers/trace_controller.rb route get &amp;quot;trace/index&amp;quot; invoke erb create app/views/trace create app/views/trace/index.html.erb invoke test_unit create test/controllers/trace_controller_test.rb invoke helper create app/helpers/trace_helper.rb invoke test_unit create test/helpers/trace_helper_test.rb invoke assets invoke coffee create app/assets/javascripts/trace.js.coffee invoke scss create app/assets/stylesheets/trace.css.scss  というコントローラーを作り、
class TraceController &amp;lt; ApplicationController def index caller().each{ |line| p line} end end  というtraceを用意し、ここにアクセスしてみました。
出力されたログは以下のようになりました。(見にくかったので、GEMまでのパスはGEM_FILE_PATHとしてます)
Started GET &amp;quot;/trace/index&amp;quot; for 127.0.0.1 at 2013-11-02 20:22:17 +0900 Processing by TraceController#index as HTML &amp;quot;/GEMLIFE_PATH/actionpack-4.</description>
    </item>
    
    <item>
      <title>WordpressからOctopressへ</title>
      <link>http://ota42y.com/blog/2013/09/25/octopress/</link>
      <pubDate>Wed, 25 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/blog/2013/09/25/octopress/</guid>
      <description>前までブログシステムにはWordpressを使っていたんですが、
個人でやるには無駄に高性能すぎて、ほとんどの機能を使っていませんでした。
そこで、ちょっと話題になってる(らしい)Octopressを導入してみました。
#Octopressって何？ jekyllを使って、
github pages上で簡単にブログを作れるシステムらしいです。
テキストファイルにmarkdown形式で書いてそれをコンパイル、ローカル上で確認することができ、
githubのへデプロイすることでweb上に公開できるシステムになります。
そのため、gitを使ってブログのデプロイやバックアップを作れるという、
まさにプログラマー向けなブログシステムです。
公式サイトにも、A blogging framework for hackersって書いてありますしね。
インストールの仕方はこっち
#運用方法 デプロイするgithubのリポジトリには、このブログのファイルが全部入ってます。
ですが、実際にはこれはoctopressの_deployフォルダの中身だけで、
このブログファイルをコンパイルするOctopressや、markdownで書かれた生原稿などは含まれていません。
そこで、公開用のリポジトリとは別に、
Octopressや原稿そのものもbutbucketのプライベートリポジトリで管理しています。
こうすると、まだ書きかけの記事とかもgitで管理できるのでとても便利になります。
今のところ、これでいけそうな感じなのでしばらくこれを使ってみようと思います。</description>
    </item>
    
    <item>
      <title>about</title>
      <link>http://ota42y.com/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://ota42y.com/about/</guid>
      <description>おおた
Twitterのプロフィール欄とGithubの最近のアクティビティを見た方が早いです。
Twitter Github
C/C++/C#/Java/Node/Ruby/Python/Go/Ob-Cとかいろいろ
iOS/Android/WindowsApiとかも</description>
    </item>
    
  </channel>
</rss>